[
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFor one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products.\nAs a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).\nThe impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.\nThus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems.\nAnd it worked!\nWhat is make\nMake is a build automation tool created in 1976, designed to solve dependency problems of the build process.\nIt was originally used to get compiled files from source code, for instance for C language.\nIn website development, an equivalent could be the installation of a project and its dependencies from the source code.\nLet me introduce a few concepts about make that we will need after.\nMake reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.\nThe architecture of the Makefile is the following:\n\r\ntarget: [prerequisite]\r\n    command1\r\n    [command2]\r\n\nAnd you run make target in your terminal to execute the target\u00a0commands. Simple, right?\nUse it for project installation\nWhat is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.\nWhat if all these commands were executed by running make install?\nYou would have your project working with one command, and no documentation to maintain anymore.\nI will only describe a simple way to build dependencies from your composer.json file\n\r\nvendor: composer.json\r\n    composer install\r\n\nThis snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.\nNote that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.\nThere is much more you can do, and I won\u2019t talk about it here.\nBut if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018.\nUse it for the commonly used commands you need on your project\nAfter the project installation, a complexity for the developer is to find the commands needed to develop features locally.\nWe decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis.\nWhat are the advantages of this:\n\nThe commands are committed and versioned\nAll developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something\nIt is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026\nIt\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options\nYou can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!\n\nAuto-generate a help target\nBut after a long time, we started having a lot of commands.\nIt was painful to find the one you wanted in the file, and even to know the one that existed\nSo we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.\nThe initial snippet we used is:\n\r\n.DEFAULT_GOAL := help\r\nhelp:\r\n    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'\r\n\nIf you add the following target and comments in your Makefile:\n\r\n## Example section\r\nexample_target: ## Description for example target\r\n        @does something\r\n\nIt would give this help message:\n\nThis generic, reusable snippet has another advantage: the documentation it generates is always up to date!\nAnd you can customize it to your need, for instance to display options associated to your commands.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMartin Guillier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few weeks ago, Nicolas and I launched https://jamstack.paris, a JAMstack website powered by GatsbyJS and hosted on Netlify.\nJAMstack applications\u00a0deliver static websites to end-users with the benefit of high performance and CDN ease and still allow dynamism by sourcing content or data during build time.\n\nOn https://jamstack.paris for example, we collect the number of attendees for our events using Gatsby Source Meetup plugin and this happens on build time.\nThe drawback of this dynamism-on-build approach is that our website may be not synced with real count of attendees until we build the site again.\n\nSo, we wanted a quick way to build the site on Netlify with a simple button on our smartphones, and\u00a0you can have the same setup in less than five minutes, let\u2019s go \ud83d\ude80.\nClaim your build hook on Netlify\nHead on Settings > Build & Deploy > Build hooks\u00a0section on your Netlify dashboard and hit the \u201cAdd build hook\u201d button.\n\nGive an explicit name to your hook, for example\u00a0Smartphone Deploy and copy the curl request Netlify provides to you.\n\nSend\u00a0the build request from your phone\nNow we want to sent this build request from our phone.\nYou are on android \ud83d\udc7e\nHTTP Request Shortcuts\u00a0application is doing exactly that\u00a0and requires no permissions on device, which is good in terms of privacy.\n\n\nYou are on iOS \ud83c\udf4f\nThe native Shortcuts application does the job. Head to the app and create a new shortcut. You will need two actions:\n\nURL\nGet Contents of URL\n\nPaste the URL from Netlify (without curl) in the field of the first action.\nThe second action is also really easy to setup. Just open the Advanced options and change the Method to POST.\nHurray! Now we can deploy from our\u00a0smartphone\u2019s homescreen \ud83d\ude80\nIf you have any suggestions or questions, feel free to add them in the comments. And if you want to speak about JAMstack in Paris, we\u2019d love to get in touch with you. First meetup gathering in Paris on December 18th\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago I was a young developer starting his journey on React. My client came to see my team and told us: \u201cWe need to make reusable components\u201d, I asked him: \u201cWhat is a reusable component?\u201d and the answer was \u201cThis project is a pilot on the subject\u201d.\n2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable\u00a0\ud83d\ude31\nAt the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?\nThe answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !\n Storybook, what is that?\nIt is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.\nThis allows you to visually describe your components : anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!\nA picture is worth a thousand words,\u00a0so just check the best example I know, the open Storybook of Airbnb.\nOne interesting thing to\u00a0note is that it\u2019s working with Vue, Angular and React!\nUsage example\nLet\u2019s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.\nThen I added Storybook to the project, I won\u2019t detail this part as the\u00a0Storybook doc\u00a0is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup your asset builder.\nNow I\u2019ll focus on the component FilteredList that display the todos, first it looked like this:\n\r\nimport React from 'react';\r\nimport styled from 'styled-components';\r\nimport TodoItem from './TodoItem';\r\n\r\nconst StyledUl = styled.ul`\r\n  list-style: none;\r\n`;\r\n\r\nconst StyledP = styled.p`\r\n  margin: 10px 0;\r\n  padding: 10px;\r\n  border-radius: 0;\r\n  background: #f2f2f2;\r\n  border: 1px solid rgba(229, 229, 229, 0.5);\r\n  color: #888;\r\n`;\r\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus} = props;\r\n\r\n    if (items.length === 0) {\r\n        return (\r\n            <StyledP>There are no items.</StyledP>\r\n        );\r\n    }\r\n\r\n    return (\r\n        <StyledUl>\r\n            {items.map(item => (\r\n                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>\r\n            ))}\r\n        </StyledUl>\r\n    );\r\n}\r\n\n(It is not exactly the same as the one on the repo, I\u00a0used styled-component instead of plain css)\nTodoItem is the component that displays an element of the list.\nHere we can see there are two different branches in the render: the nominal case and the empty state.\nLet\u2019s write some stories, I created a file FilteredList.stories.js\u00a0and added this inside:\n\r\nimport React from 'react';\r\nimport { storiesOf } from '@storybook/react';\r\nimport FilteredList from \"./FilteredList\";\r\n\r\nconst data = [{\r\n    id: 1,\r\n    completed: true,\r\n    text: 'Jean-Claude Van Damme'\r\n}];\r\n\r\nstoriesOf('FilteredList')\r\n    .add('Nominal usage', () => (\r\n        <FilteredList items={data} changeMode={() => void 0}/>\r\n    ))\r\n    .add('Empty state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0}/>\r\n    ));\r\n\nSo what did I do here?\nI defined placeholder data in a variable for the component props.\nWe use the function storiesOf\u00a0from storybook, this function takes the name we want to give to the group of stories as entry param.\nThen we add some stories with .add. It\u00a0works pretty much\u00a0like jest or mocha\u2019s\u00a0describe or\u00a0it\u00a0in tests, it takes the name of the story and a function that returns the component to render.\nHere\u2019s what it looks like:\n\n\nIt\u2019s rather simple but it\u2019s working, we see the two different cases.\nWhat if we add other branches? Let\u2019s say the parent component of FilteredList\u00a0is calling an API to get the list and so we have to add a loading and error state.\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus, isLoading, error} = props;\r\n\r\n    if (isLoading) {\r\n        return (\r\n            <StyledP>Loading...</StyledP>\r\n        )\r\n    }\r\n\r\n    if (error) {\r\n        return (\r\n            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>\r\n        )\r\n    }\r\n    \r\n    //...\r\n}\r\n\nNow we need to add the corresponding stories.\n\r\n.add('Loading state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0} isLoading />\r\n))\r\n.add('Error state', () => (\r\n    <FilteredList items={[]} changeMode={() => void 0} error=\"Internal server error\"/>\r\n));\r\n\nAnd now our Storybook looks like:\n\n\nThis example is rather simple but it shows pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.\nAin\u2019t nobody got time for that?\nIn fact it takes time when you are coding but I feel like it is more like an investment.\nWhen you develop your app\u00a0aren\u2019t you trying to make it easy to use? Then why not make it easy for developers to use your code?\nAnd that\u2019s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.\nThis is very important because you are not the only user of your code, you might be working with a team or someone else will\u00a0take over\u00a0your project afterwards.\nWe all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)\nFinal thoughts\nIn a way we are all trying to do reusable components \u2013 whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It\u2019s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that\u2019s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tL\u00e9o Anesi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLate in 2018 AWS released Lambda Layers and custom runtime support. This means that to run unsupported runtimes you no longer need to \u2018hack\u2019 around with VMs, docker or using node.js to `exec` your binary.\nRecently I needed to\u00a0setup\u00a0a 100% serverless PHP infrastructure for a client. PHP is one option, but similar steps can allow you to run any language.\nLambda layers provide shared code between different lambda functions. For instance, if you wanted to share your vendor code between lambdas (e.g. node_modules for node.js).\nWe will create a Lambda layer to provide the PHP binary for our custom runtime API. You could also create a second to provide the vendor folder for composer dependencies.\n# Step 1: Compiling the\u00a0PHP\u00a0binary\nWe will need a `/bin` directory containing the PHP binary. Because we are compiling a binary this step needs to happen on the same OS and architecture that our Lambda will use.\n\nThis page\u00a0lists the AWS Execution environment, but last time I checked their version was out of date.\nTo find the correct AMI I used the latest version for the region I was deploying in, found here\u00a0by a quick regex for AMIs containing `amzn-ami-hvm-.*-gp2`.\n\nOnce you have the correct AMI, spin up a large EC2 instance and `ssh` in.\nRun the following commands as listed in AWS\u2019s docs.\n\r\nsudo yum update -y\r\nsudo yum install autoconf bison gcc gcc-c++ libcurl-devel libxml2-devel -y\r\n\r\ncurl -sL http://www.openssl.org/source/openssl-1.0.1k.tar.gz | tar -xvz\r\ncd openssl-1.0.1k\r\n./config && make && sudo make install\r\ncd ~\r\n\r\nmkdir ~/php-7-bin\r\ncurl -sL https://github.com/php/php-src/archive/php-7.3.0.tar.gz | tar -xvz\r\ncd php-src-php-7.3.0\r\n\r\n./buildconf --force\r\n./configure --prefix=/home/ec2-user/php-7-bin/ --with-openssl=/usr/local/ssl --with-curl --with-zlib\r\nmake install\r\n\nCheckpoint:\u00a0The following should give\u00a0you the version number of PHP you wanted\n /home/ec2-user/php-7-bin/bin/php -v \nMove this into a `/bin` directory.\n`mkdir './bin && mv php ./bin\nNow we can zip up the code for our Lambda layer that will provide our custom runtime API.\nzip -r runtime.zip bin bootstrap\n# Vendor files\nOn the same EC2, we need to use composer to get our vendor code.\ncurl -sS https://getcomposer.org/installer | ./bin/php`\nAdd some vendor code:\n./bin/php composer.phar require guzzlehttp/guzzle\n\u00a0\nzip -r vendor.zip vendor/\n\u00a0# Bring down to local\nNow use `scp` to copy the PHP binary down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/runtime.zip .\nNow use `scp` to copy the vendor zip down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/vendor.zip .\nDon\u2019t forget to terminate your large EC2 instance\n# Creating a custom runtime API\nTo use a custom runtime AWS requires you specify a bootstrap file which will provide the interface for lambda events. As we will be writing PHP support we can write it in PHP (very meta).\nCreate a `bootstrap` executable:\ntouch ./bootstrap && chmod +x ./bootstrap\nExample adapted from AWS docs\n\r\n#!/opt/bin/php\r\n<?php\r\n\r\n// This invokes Composer's autoloader so that we'll be able to use Guzzle and any other 3rd party libraries we need.\r\nrequire __DIR__ . '/vendor/autoload.php';\r\n\r\n// amzn-ami-hvm-2017.03.1.20170812-x86_64-gp2\r\n\r\nfunction getNextRequest()\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $response = $client->get('http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/next');\r\n\r\n return [\r\n 'invocationId' => $response->getHeader('Lambda-Runtime-Aws-Request-Id')[0],\r\n 'payload' => json_decode((string) $response->;getBody(), true)\r\n ];\r\n}\r\n\r\nfunction sendResponse($invocationId, $response)\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $client->post(\r\n 'http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/' . $invocationId . '/response',\r\n ['body' => $response]\r\n );\r\n}\r\n\r\n// This is the request processing loop. Barring unrecoverable failure, this loop runs until the environment shuts down.\r\ndo {\r\n // Ask the runtime API for a request to handle.\r\n $request = getNextRequest();\r\n\r\n // Obtain the function name from the _HANDLER environment variable and ensure the function's code is available.\r\n $handlerFunction = array_slice(explode('.', $_ENV['_HANDLER']), -1)[0];\r\n require_once $_ENV['LAMBDA_TASK_ROOT'] . '/src/' . $handlerFunction . '.php';\r\n\r\n // Execute the desired function and obtain the response.\r\n $response = $handlerFunction($request['payload']);\r\n\r\n // Submit the response back to the runtime API.\r\n sendResponse($request['invocationId'], $response);\r\n} while (true);\r\n\r\n?>\r\n\n\u2013 Note: #!/opt/bin/php links to our /bin/php created earlier.\nManual Deployment\n\nGo to AWS lambda page.\nCreate a function selecting to use a custom runtime.\nCreate a layer called `php` and upload `runtime.zip`.\nCreate a layer called `vendor` and upload `vendor.zip`.\nApply the layers to the function you created in the merge order: 1) runtime, 2) vendor\n\nWriting a handler function\n\r\n\r\nmkdir src\r\n\r\ntouch src/hello.php\r\n\r\n\nAdd some basic function called `hello`:\n\r\n<?php\r\n\r\nfunction hello($data)\r\n{\r\n return \"Hello, {$data['name']}!\";\r\n}\r\n\r\n?>\r\n\nThen zip this up to be uploaded:.\nzip hello.zip src/hello.php\nUpload the function handler zip to the function and change the handler name to the name of the php file without the extension. e.g. hello.php => hello\n\u00a0\n# Automation\nMany of the steps here can be automated once you have compiled your binary. Either by using the AWS API, cloudformation or the `serverless`\u00a0library which supports layers.\n# Other languages\nThese steps should allow any language that can compile on the AWS AMI used by Lambda to be used as your runtime, e.g. Rust.\n\u00a0\nResources:\n\nhttps://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\nhttps://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUsually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.\nThe problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page!\nLooking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.\nWhy SSR (Server-Side Rendering)?\nThis is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:\n\nImprove your SEO\nSpeed up your first page load\nAvoid blank page flicker before rendering\n\nIf you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.\nBut let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here.\nWhat\u2019s the plan?\nFor this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings.\nThis article is split in 3 sections matching 3 server-side-rendering strategies:\n\nHow tomanually upgrade your React app to get\u00a0SSR\nHow to start with Next.js from scratch\nMigrate your existing React app to server-side with Next.js\n\nI won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon).\n1) Look how twisted manual SSR is\u2026\n\nIn this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:\n\npackage.json\u00a0for dependencies\nWebpack configuration included\nApp.js\u00a0\u2013 loads React and renders the Hello component\nindex.js\u00a0\u2013 puts all together into a root component\n\nChecking rendering type\nI just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window:\nconst isClientOrServer = () => {\r\n  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';\r\n};\r\n\nso that we display on the page what is rendering the application: server or client.\nTest it by yourself\n\nclone\u00a0this repository\ncheckout the initial commit\ninstall the dependencies with\u00a0yarn\nlaunch the dev server with\u00a0yarn start\nbrowse to\u00a0http://localhost:3000\u00a0to view the app\n\nI am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:\n\nImplementing\u00a0Server-side Rendering\nLet\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps.\nWe first need a node server using Express:\u00a0yarn add express.\nIn our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer.\n// use port 3001 because 3000 is used to serve our React app build\r\nconst PORT = 3001; const path = require('path');\r\n\r\n// initialize the application and create the routes\r\nconst app = express();\r\nconst router = express.Router();\r\n\r\n// root (/) should always serve our server rendered page\r\nrouter.use('^/$', serverRenderer);\r\n\nTo render our html, we use a server renderer that is replacing the root component with the built html:\n// index.html file created by create-react-app build tool\r\nconst filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');\r\n\r\nfs.readFile(filePath, 'utf8', (err, htmlData) => {\r\n  // render the app as a string\r\n  const html = ReactDOMServer.renderToString(<App />);\r\n\r\n  // inject the rendered app into our html\r\n  return res.send(\r\n    htmlData.replace(\r\n      '<div id=\"root\"></div>',\r\n      `<div id=\"root\">${html}</div>`\r\n    )\r\n  );\r\n}\r\n\nThis is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string.\nWe finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.\nrequire('babel-register')({\r\n  ignore: [ /(node_modules)/ ],\r\n  presets: ['es2015', 'react-app']\r\n});\r\n\nTest it by yourself\n\ncheckout last changes on master branch\ninstall the dependencies with\u00a0yarn\nbuild the application with\u00a0yarn build\ndeclare babel environment in your terminal:\u00a0export BABEL_ENV=development\nlaunch your node server with\u00a0node server/bootstrap.js\nbrowse to\u00a0http://localhost:3001\u00a0to view the app\n\nStill simulating the \u20183G network\u2019 in Chrome, here is the result:\n\nDo not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client.\nWe proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?!\nWe\u2019re missing many features\nOur example is a good proof of concept but very limited. We would like to see more features like:\n\nimport images in js files (logo problem)\nseveral routes usage or route management (check\u00a0this article)\ndeal with the\u00a0</head>\u00a0and the metatags (for SEO improvements)\ncode splitting (here is\u00a0an article\u00a0solving the problem)\nmanage the state of our app or use Redux (check this\u00a0great article\n\nand performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website.\nIt is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations.\nSeriously\u2026 Next.js can bring you all these features!\n2) Next.js helps you building server rendered React.js Application\n\nWhat is Next.js?\nNext.js is a minimalistic framework for server-rendered React applications with:\n\na very simple page based routing\nWebpack hot reloading\nautomatic transpilation (with babel)\ndeployment facilities\nautomatic code splitting (loads page faster)\nbuilt in css support\nability to run server-side actions\nsimple integration with Redux using next-redux-wrapper.\n\nGet started in 1 minute\nIn this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.\nFirst, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this:\n\"scripts\": {\r\n  \"dev\": \"next\",\r\n  \"build\": \"next build\",\r\n  \"start\": \"next start\"\r\n}\r\n\nCreate a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function):\nconst Index = ({ title = 'Hello from Next.js' }) => (\r\n  <div>\r\n    <h1>{title}</h1>\r\n    <p className=\"App-intro\">\r\n      Is my application rendered by server or client?\r\n    </p>\r\n    <h2><code>{isClientOrServer()}</code></h2>\r\n  </div>\r\n);\r\n\r\nexport default Index;\r\n\nNo need to import any library at the top of our index.js file, Next.js already knows that we are using React.\nNow enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!\n\nRepeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.\nYou\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository.\nUse create-next-app\nYou want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app:\nnpm install -g create-next-app\r\n\r\ncreate-next-app my-app\r\ncd my-app/\r\nnpm run dev\r\n\nThis is all you need to do to create a React app with server-side rendering thanks to Next.js.\nFinally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.\nVue.js and Nuxt\nYou\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:\n vue init nuxt-community/starter-template <project-name> \nWhat\u2019s Next? \nHope you liked this article which was mainly written in order to introduce server-side rendering with Next.\nIf you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBaptiste Jan\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @Theodo. I like Vue.js and all the ecosystem growing around.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you already know what scraping is, you can directly jump to how I did it\nWhat is scraping?\nScraping is the process of data mining. Also known as web data extraction, web harvesting, spying.. It is software that simulates human interaction with a web page to retrieve any wanted information (eg images, text, videos). This is done by a scraper.\nThis scraper involves making a GET request to a website and parsing the html response. The scraper then searches for the data required within the html and repeats the process until we have collected all the data we want.\nIt is useful for quickly accessing and analysing large amounts of data, which can be stored in a CSV file, or in a database depending on our needs!\nThere are many reasons to do web scraping such as lead generation and market analysis. However, when scraping websites, you must always be careful not to violate the terms and conditions of websites you are scraping, or to violate anyone\u2019s privacy. This is why it is thought to be a little controversial but can save many hours of searching through sites, and logging the data manually. All of those hours saved mean a great deal of money saved.\nThere are many different libraries that can be used for web scraping, e.g. selenium, phantomjs.\u00a0In Ruby you can also use the nokogiri gem to write your own ruby based scraper. Another popular library is is beautiful soup which is popular among python devs.\nAt Theodo, we needed to use a web scraping tool with the ability to follow links and as python developers the solution we opted for was using theDjango framework with an open source web scraping framework called\u00a0Scrapy.\nScrapy and Django\nScrapy allows us to define data structures, write data extractors, and comes with built in CSS and xpath selectors that we can use to extract the data, the scrapy shell, and built in JSON, CSV, and XML output. There is also a built in FormRequest class which allows you to mock login and is easy to use out of the box.\nWebsites tend to have countermeasures to prevent excessive requests, so Scrapy randomises the time between each request by default which can help avoid getting banned from them. Scrapy can also be used for automated testing and monitoring.\nDjango has an integrated admin which makes it easy to access the db. That along with the ease of filtering and sorting data and import/export library to allow us to export data.\nScrapy also used to have a built in class called DjangoItem which is now an easy to use external library. The DjangoItem library provides us with a class of item that uses the field defined in a Django model just by specifying which model it is related to. The class also provides a method to create and populate the Django model instance with the item data from the pipeline. This library allows us to integrate Scrapy and Django easily and means we can also have access to all the data directly in the admin!\nSo what happens?\n\u00a0\nSpiders\nLet\u2019s start from the spider. Spiders are the core of the scraper. It makes the request to our defined URLs, parses the responses, and extracts information from them to be processed in the items.\nScrapy has a start_requests method which generates a request with the URL. When Scrapy fetches a website according to the request, it will parse the response to a callback method specified in the request object. The callback method can generate an item from the response data or generate another request.\nWhat happens behind the scenes? Everytime we start a Scrapy task, we start a crawler to do it. The Spider defines how to perform the crawl (ie following links). The crawler has an engine to drive it\u2019s flow. When a crawler starts, it will get the spider from its queue, which means the crawler can have more than one spider. The next spider will then be started by the crawler and scheduled to crawl the webpage by the engine. The engine middlewares drive the flow of the crawler. The middlewares are organised in chains to process requests and responses.\nSelectors\nSelectors can be use to parse a web page to generate an item. They select parts of the html document specified either by xpath or css expressions. Xpath selects nodes in XML docs (that can also be used in HTML docs) and CSS is a language for applying styles to HTML documents. CSS selectors use the HTML classes and id tag names to select the data within the tags. Scrapy in the background using the cssselect library transforms these CSS selectors into xpath selectors.\nCSS vs Xpath\n\r\n        data = response.css(\"div.st-about-employee-pop-up\") \r\n        data = response.xpath(\"//div[@class='team-popup-wrap st-about-employee-pop-up']\")\r\n\nShort but sweet: when dealing with classes, ids and tag names, use CSS selectors. If you have no class name and just know the content of the tag use xpath selectors. Either way chrome dev tools can help: copy selector for the element\u2019s unique css selector or you can copy its xpath selector. This is to give a basis, may have to tweak it! Two more helper tools are XPath helper and this cheatsheet. Selectors are also chainable.\nItems and Pipeline\nItems produce the output. They are used to structure the data parsed by the spider. The Item Pipeline is where the data is processed once the items have been extracted from the spiders. Here we can run tasks such as validation and storing items in a database.\nHow I did it\nHere\u2019s an example of how we can integrate Scrapy and Django. Let\u2019s scrape the data off the Theodo UK Team Page and integrate it into a Django Admin Panel:\n\nGenerate Django project with integrated admin + db\nCreate a django project, with admin and database\nCreate app and add to installed apps\nDefine the data structure, so the item, so our django model.\n## models.py\r\n      from django.db import model\r\n\r\n      class TheodoTeam(models.Model):\r\n        name = models.CharField(max_length=150)\r\n        image = models.CharField(max_length=150)\r\n        fun_fact = models.TextField(blank=True)\r\n\r\n        class Meta:\r\n            verbose_name = \"theodo UK team\"\r\n      \n\nInstall Scrapy\nRun\nscrapy startproject scraper\n\nConnect using DjangoItem\n    ## items.py\r\n      from scrapy_djangoitem import DjangoItem\r\n      from theodo_team.models import TheodoTeam\r\n\r\n      class TheodoTeamItem(DjangoItem):\r\n        django_model = TheodoTeam\r\n    \n\nThe Spider \u2013 Spiders have a starturls class which takes a list of URLs. The URLs will then be used by the startrequests method to create the initial requests for your spider. Then using the response and selectors, select the data required.\nimport scrapy\r\n    from scraper.items import TheodoTeamItem\r\n\r\n    class TheodoSpider(scrapy.Spider):\r\n      name = \"theodo\"\r\n      start_urls = [\"https://www.theodo.co.uk/team\"]\r\n\r\n      # this is what start_urls does\r\n      # def start_requests(self):\r\n      #     urls = ['https://www.theodo.co.uk/team',]\r\n      #     for url in urls:\r\n      #       yield scrapy.Request(url=url, callback=self.parse)\r\n\r\n      def parse(self, response):\r\n          data = response.css(\"div.st-about-employee-pop-up\")\r\n\r\n          for line in data:\r\n              item = TheodoTeamItem()\r\n              item[\"name\"] = line.css(\"div.h3 h3::text\").extract_first()\r\n              item[\"image\"] = line.css(\"img.img-team-popup::attr(src)\").extract_first()\r\n              item[\"fun_fact\"] = line.css(\"div.p-small p::text\").extract().pop()\r\n              yield item\r\n    \n\nPipeline \u2013 use it to save the items to the database\n## pipelines.py\r\n    class TheodoTeamPipeline(object):\r\n      def process_item(self, item, spider):\r\n          item.save()\r\n          return item\r\n    \n\nCreate a Django command to run Scrapy crawl \u2013 This initialises django in the scraper and is needed to be able to access django in the spider.\n## commands/crawl.py\r\n\r\n    from django.core.management.base import BaseCommand\r\n    from scraper.spiders import TheodoSpider\r\n    from scrapy.crawler import CrawlerProcess\r\n    from scrapy.utils.project import get_project_settings\r\n\r\n    class Command(BaseCommand):\r\n      help = \"Release the spiders\"\r\n\r\n      def handle(self, *args, **options):\r\n          process = CrawlerProcess(get_project_settings())\r\n\r\n          process.crawl(TheodoSpider)\r\n          process.start()\r\n    \n\nRun manage.py crawl to save the items to the database\n\nProject Structure:\n scraper\r\n     management\r\n         commands\r\n             crawl.py\r\n     spiders\r\n         theodo_team_spider.py\r\n         apps.py\r\n         items.py\r\n         middlewares.py\r\n         pipelines.py\r\n         settings.py\r\n theodo_team\r\n     admin\r\n     migrations\r\n     models\r\n\nChallenges and problems encountered:\nSelectors!! Selectors are not one size fits all. Different selectors are needed for every website and if there is constant layout changes, they require upkeep. It can also be difficult to find all the data required without manipulating it. This occurs when tags may not have a class name or if data is not consistently stored in the same tag.\nAn example of how complicated selectors can get:\nsegments = response.css(\"tr td[rowspan]\")\r\nrowspan = int(segment.css(\"::attr(rowspan)\").extract_first())\r\n           all_td_after_segment = segment.xpath(\"./../following-sibling::tr[position()<={}]/td\".format(rowspan- 1))\r\n\r\nline = all_td_after_segment.extract_first()\r\ndata = line.xpath(\"./descendant-or-self::a/text()\")\r\nmore_data = line.xpath(\"substring-after(substring-before(./strong/text()[preceding-sibling::a], '%'), '\\xa0')\").extract_first()\r\n\nAs you can see, setting up the scraper is not the hard part! I think integrating Scrapy and Django is a desirable, efficient and speedy solution to be able to store data from a website into a database.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHenriette Brand\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tKotlin is an object-oriented programming language for making Android apps that uses Java-like syntax with functional programming features. It was created by Jetbrains, the makers of hugely popular IDEs like IntelliJ and PyCharm, and is used by big companies such as Pinterest, Uber, and Atlassian.\nAt Theodo, React Native is the language of choice for building apps. And for good reason. It has a rich ecosystem of libraries to use, hot reloading out of the box, is very similar to React which is a hugely popular web framework, and vitally it allows you to compile your apps to both Android and iOS from just one codebase. But this doesn\u2019t mean that we should necessarily use React Native in every case. I will talk through some examples of why Kotlin might be a better choice in some cases.\nWhy use Kotlin/Java over React Native?\nThere are many reasons why one may choose to write native code for an app rather than React Native. One of the most common I have seen is API support. Many services that have APIs for apps cannot be used fully within React Native. This leads to developers having to write native code to get the functionality they need. This can be tricky to developers who may not have ever written native code. Kotlin, on the other hand, is strongly supported within the Android ecosystem and is interoperable with Java so it has easier access to these APIs. Any API that supports Java 8 can be easily used with Kotlin.\nAnother big reason to prefer native code over React Native is performance. For advanced functionality or heavy computation, the overhead of using React Native can considerably slow down an app. Kotlin, on the other hand, appears and acts totally natively without lag.\nMany of us have suffered through the pain of having to update React or React Native. It can be a long and arduous task. However, with writing Kotlin, each new version is backwards compatible with the last. That means that even legacy code written by previous teams doesn\u2019t have to be rewritten when updating to a new version of Kotlin.\nA big problem with large javascript projects is the errors that can come up due to the lack of typing. This is why many developers choose to integrate Flow or TypeScript to help with this. However, this can be a difficult task, especially if the codebase is already large before you start. Kotlin however is statically typed meaning that there is no need for tools like these to check for type errors.\nAnother benefit of using Kotlin over React Native is that it is fully compatible with Android Studio, and Intellij. This means you can make use of loads of cool features like advanced code completion, the built in debugger, and gradle integration, all while benefitting from a UI that was purpose built for making Android apps.\nWhy use Kotlin instead of Java?\nKotlin was created by JetBrains as an attempt to be a more concise and feature-rich version of Java, whilst being designed primarily for Android app development. I understand that writing native code can feel intimidating to many people who have only written Javascript or Python before, but I see Kotlin as a good stepping stone; it has the benefits of writing native code but it will feel more familiar in its functional aspects such as spread, deconstruction, and closures.\nKotlin can also be used to write iOS applications. This is a great benefit when writing applications for both iOS and Android as the application logic can be shared between the two. However, the rendering code is platform specific and thus must be rewritten for each.\nA great thing about Kotlin is that it\u2019s fully compatible with Java. This means that Java code can be inserted into it, or it can be inserted into a Java codebase. This can be helpful if you have a large codebase of legacy code written in Java which you need to add new features to.\nKotlin has all the features of Java 8, plus more. This means you get lambda functions, static functions, streams and nested classes from Java 8, on top of immutable variables, method references, methods without classes, extension methods to classes, closures, spread and deconstruction. As well as all this, built in @Nullable and @NonNull mean that there are no Null-Pointer Exceptions which comes as a great relief to those who are apprehensive to get started writing native code.\nWriting Kotlin, you can use any library that is compatible with Java 8. As well as this, Kotlin has a lot of libraries written for it such as kotlinx-coroutines-android and rxkotlin for writing asynchronous and event-based programs, just to name a few. Whilst still perhaps not as many libraries as React Native might have, there are still a huge number of libraries to take advantage of when writing Kotlin.\nDrawbacks of using Kotlin/Java over React Native\nSadly no tool is perfect and there are many reasons why you may choose to go for React Native over native code. The main reason is iOS compatibility. Yes, Kotlin code can be compiled to objective C, and is bi-directionally interoperable with Objective-C and Swift. But you still have to write separate render code for iOS and Android. This could take a long time and is the main reason why one would choose React Native.\nAnother big reason is that writing React allows hot reloading straight out of the box. While Kotlin does allow for hot reloading, it can be a pain to set up and work unreliably. Hot reloading can really speed up development and if you\u2019re used to having it you may sorely miss it if you can\u2019t get it working for Kotlin development.\nConclusion\nIf you want to get the best of both worlds, it is possible to use React Native to implement the frontend code and Kotlin to implement the backend of your application. This means that you get the speed improvements and Java-compatible API integration of Kotlin, but only have to write one codebase for both Android and iOS. As well as this, you can make use of the front-end libraries available to React Native.\nJetBrains have created a library called kotlin-wrappers which contains wrappers to use Kotlin with React, Redux, styled-components, and React Router DOM, amongst others. There are also a few other libraries online which do similar things.\nThis is a very new field and there are not many people using it just yet, so there may be some teething issues. One possible problem is that passing the store via props to the component won\u2019t work because the state will become immutable. There may be libraries to help with this, and if not then I\u2019m sure there soon will be, but this could be a big pain point.\nThis emerging combination of languages looks like a really new, interesting addition to our use of React Native here at Theodo. I hope I will on the next applicable project to test out this exciting new tech.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAbbie Howell\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\ttl:dr\nTo add a pre-commit git hook with Husky:\n\nInstall Husky with npm install husky --save-dev\nSet the pre-commit command in your package.json:\n\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nWhat are git hooks?\nGit hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created.\nThe scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.\n\nWhy do I need to install the Husky\u00a0package then?\nThe problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.\nHusky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:\n\nTheses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nTo add husky to your project, simply run npm install husky --save-dev.\nFor more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\".\nEnhancing your git flow\nGit hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.\n\nCheck for linting errors\n\nIf you have tools to check the code quality or formatting, you can run it on a pre-commit hook:\n\"scripts\": {\r\n    \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"\r\n},\r\n\nI advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.\n\nProtect important branches\n\nIn some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:\n\"scripts\": {\r\n    \"prepush\": \"./scripts/pre-push-check.sh\"\r\n},\r\n\n\n#!/bin/bash\r\nset -e\r\n\r\nbranch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')\r\nif [ \"$branch\" == \"master\" ]\r\nthen\r\n    npm test\r\nfi\r\n\n\nIf your tests fail, the code won\u2019t be pushed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy?\nAdding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.\nSetup Symfony and AWS\nFirst you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.\nAmazon AWS\nCreating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.\nFollow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.\nSymfony\nNow you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:\n\nVichUploader (a bundle that will ease files upload)\nKNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)\nAWS-SDK (A SDK provided by Amazon to communicate with AWS API)\n\nInstall the two bundles and the SDK with composer:\n\r\ncomposer require vich/uploader-bundle\r\ncomposer require aws/aws-sdk-php\r\ncomposer require knplabs/knp-gaufrette-bundle\r\n\r\n\nThen register the bundles in AppKernel.php\n\r\npublic function registerBundles()\r\n    {\r\n     return [\r\n            \tnew Vich\\UploaderBundle\\VichUploaderBundle(),\r\n            \tnew Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),\r\n            ];\r\n    }\r\n\r\n\nBucket parameters\nIt is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment.\nYou will need to define four parameters to get access to your AWS bucket:\n\nAWS_BUCKET_NAME\nAWS_BASE_URL\nAWS_KEY (only for and private buckets)\nAWS_SECRET_KEY (only for and private buckets)\n\nYou can find the values of these parameters in your AWS console.\nConfiguration\nYou will have to define a service extending Amazon AWS client and using your AWS credentials.\nAdd this service in services.yml:\n\r\nct_file_store.s3:\r\n        class: Aws\\S3\\S3Client\r\n        factory: [Aws\\S3\\S3Client, 'factory']\r\n        arguments:\r\n            -\r\n                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)\r\n                region: YOUR_AWS_S3_REGION\r\n                credentials:\r\n                    key: '%env(AWS_KEY)%'\r\n                    secret: '%env(AWS_SECRET_KEY)%'\r\n\r\n\nNow you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.\nHere is a basic example:\n\r\nknp_gaufrette:\r\n    stream_wrapper: ~\r\n    adapters:\r\n        document_adapter:\r\n            aws_s3:\r\n                service_id: ct_file_store.s3\r\n                bucket_name: '%env(AWS_BUCKET_NAME)%'\r\n                detect_content_type: true\r\n                options:\r\n                    create: true\r\n                    directory: document\r\n    filesystems:\r\n        document_fs:\r\n            adapter:    document_adapter\r\n\r\nvich_uploader:\r\n    db_driver: orm\r\n    storage: gaufrette\r\n    mappings:\r\n        document:\r\n            inject_on_load: true\r\n            uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET_NAME)%/document\"\r\n            upload_destination: document_fs\r\n            delete_on_update:   false\r\n            delete_on_remove:   false \r\n\r\n\nUpload files\nFirst step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).\nThe attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().\n\r\nnamespace MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\nuse Symfony\\Component\\HttpFoundation\\File\\File;\r\nuse Vich\\UploaderBundle\\Mapping\\Annotation as Vich;\r\n\r\n/**\r\n * Class Document\r\n *\r\n * @ORM\\Table(name=\"document\")\r\n * @ORM\\Entity()\r\n * @Vich\\Uploadable()\r\n */\r\nclass Document\r\n{\r\n    /**\r\n     * @var int\r\n     *\r\n     * @ORM\\Column(type=\"integer\", name=\"id\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(type=\"string\", length=255, nullable=true)\r\n     */\r\n    private $documentFileName;\r\n\r\n    /**\r\n     * @var File\r\n     * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")\r\n     */\r\n    private $documentFile;\r\n\r\n    /**\r\n     * @var \\DateTime\r\n     *\r\n     * @ORM\\Column(type=\"datetime\")\r\n     */\r\n    private $updatedAt;\r\n}\r\n\r\n\nThen you can add an uploaded document to any of your entities:\n\r\n     /**\r\n     * @var Document\r\n     *\r\n     * @ORM\\OneToOne(\r\n     *     targetEntity=\"\\MyBundle\\Entity\\Document\",\r\n     *     orphanRemoval=true,\r\n     *     cascade={\"persist\", \"remove\"},\r\n     * )\r\n     * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")\r\n     */\r\n    private $myDocument;\r\n\r\n\nCreate a form type to be able to upload a document:\n\r\nclass UploadDocumentType extends AbstractType\r\n{\r\n    public function buildForm(FormBuilderInterface $builder, array $options)\r\n    {\r\n        add('myDocument', VichFileType::class, [\r\n                'label'         => false,\r\n                'required'      => false,\r\n                'allow_delete'  => false,\r\n                'download_link' => true,\r\n            ]);\r\n    }\r\n...\r\n}\r\n\r\n\nUse this form type in your controller and pass the form to the twig:\n\r\n...\r\n$myEntity = new MyEntity();\r\n$form = $this->createForm(UploadDocumentType::class, $myEntity);\r\n...\r\nreturn [ 'form' => $form->createView()];\r\n\r\n\nFinally, add this form field in your twig and you should see an upload field in your form:\n\r\n<div class=\"row\">\r\n    <div class=\"col-xs-4\">\r\n        {{ form_label(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8\">\r\n        {{ form_widget(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8 col-xs-offset-4\">\r\n        {{ form_errors(form.myDocument) }}\r\n    </div>\r\n</div>\r\n\r\n\nNavigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.\nUsers are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.\nDisplay or download documents stored in private buckets\nIn most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.\nGet your document from private bucket\nYou will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.\ns3Client is the service (ct_file_store.s3) we defined previously extending AWS S3 client with credentials for private bucket. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.\n\r\n     /**\r\n     * @param string $documentName\r\n     *\r\n     * @return \\Aws\\Result|bool\r\n     */\r\n    public function getDocumentFromPrivateBucket($documentName)\r\n    {\r\n        try {\r\n            return $this->s3Client->getObject(\r\n                [\r\n                    'Bucket' => $this->privateBucketName,\r\n                    'Key'    => 'my-documents/'.$documentName,\r\n                ]\r\n            );\r\n        } catch (S3Exception $e) {\r\n            // Handle your exception here\r\n        }\r\n    }\r\n\r\n\nDefine an action with a custom route:\nYou will need to use the method previously defined to download the file from AWS and expose it on a custom route.\n\r\n     /**\r\n     * @param Document $document\r\n     * @Route(\"/{id}/download-document\", name=\"download_document\")\r\n     * @return RedirectResponse|Response\r\n     */\r\n    public function downloadDocumentAction(Document $document)\r\n    {\r\n        $awsS3Uploader  = $this->get('app.service.s3_uploader');\r\n\r\n        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());\r\n\r\n        if ($result) {\r\n            // Display the object in the browser\r\n            header(\"Content-Type: {$result['ContentType']}\");\r\n            echo $result['Body'];\r\n\r\n            return new Response();\r\n        }\r\n\r\n        return new Response('', 404);\r\n    }\r\n\r\n\nDownload document\nEventually add a download button to access a document stored in a private bucket directly in your Symfony application.\n\r\n<a href=\"{{ path('/download-document', {'id': document.id}) }}\" \r\n                   target=\"_blank\">\r\n   <i class=\"fa fa-print\">\r\n   {{ 'label_document_download'|trans }}\r\n</a>\r\n\r\n\nPublic assets\nYou may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.\n\r\n<img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />\r\n\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlan Rauzier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tZeplin\u00a0vs\u00a0InVision: I work for a service company as a lead developer and we have been using\u00a0these tools\u00a0on different projects for mockup integration. I teamed up with France Wang, lead designer at BAM, to list the pros and cons and give you our combined point of view on these\u00a0design handoff tools.\n\nTo clarify the need we have on our projects and what we use these platforms for, here is our development workflow:\n\nDesigners make the mockups on\u00a0Sketch\nThey\u00a0import the mockup on Zeplin or InVision along with the assets\n(They add comments for developers to\u00a0explain\u00a0specific behaviours)\nDevs use the platform to inspect mockups and integrate them\n\nUsually there are back-and forth between devs and designers during the mockup integration\u00a0phase (4). Choosing the best tool helps limiting these wastes and frustrations. We focus on these goals for the benchmark. We\u2019ll also share tips on how we use these tools for better\u00a0collaboration.\nDisclaimer: this content is not sponsored by either mentioned parties\nAnd the best tool for mockup integration is\u2026\n\nAfter benchmarking the two solutions, our recommendation would be to use Zeplin,\u00a0as it is more advanced and more convenient for design handoff.\nIf you have the budget (17$-26$/month) do not hesitate! The time you\u2019ll save is worth it. The platform eases knowledge sharing between designers and developer. Consequently the production workflow will be faster and less painful. Happier collaboration yay \\o/! #DevUx\nIf you need prototyping and do user testing, use InVision in addition. Designers can use the same Sketch file for both platforms. Separate tools for separate purposes!\nDetailed Benchmark of Zeplin vs Invision\nHere is a recap of the comparison we made:\n\n\n\n\n\n\n\n\nZeplin\u2019s Pros:\n\nKiller\u00a0features:\n\nAutomatically-generated styleguide linked to the mockups\nCommenting tool\nPixel perfect comparison\n\n\nMost\u00a0complete tool for mockup integration\n\n\n\nInVision\u2019s Pros:\n\nInteresting free plan (as many collaborators as you want)\nMore complete offer if you need prototyping\nWill get the job done\n\n\n\n\n\nZeplin\u2019s Cons:\n\nPlans are more expensive and free plan less interesting if your want several collaborators\n\n\n\nInVision\u2019s Cons\n\nOverall\u00a0less efficient because the platform is multipurpose and the UX is not focused on\u00a0mockup integration.\n\n\n\n\n\nKiller feature: Semi-automatically generated styleguide\nWhy should you use a styleguide?\nUsing a styleguide helps us save time and limit rework. To\u00a0both designers and developers, the styleguide defines a design reference\u00a0(for both UI and UX).\nFor the designers, it helps with\n\nkeeping the product consistent\nsummarising\u00a0all colors, font styles, components used throughout the app\u00a0along as their states (idle, disabled, active, loading, success, error\u2026) and variations\u00a0(primary, secondary, icon only etc\u2026)\nexplaining to the devs each\u00a0component behaviour without having to repeat it on each mock up\n\nFor developers, it serves as\n\na reference of standard color codes, font sizes, font weights to use in the code\na library of components\u00a0with the variations\u00a0and states they can have\na compilation of\u00a0components behaviour so as\u00a0not to forget any cases\n\nStyleguides on Zeplin vs Invision: why Zeplin wins\nZeplin integrates an interface to create a styleguide from imported mockups. The platform detects font properties and colors so the designers can add them to the styleguide easily.\nDesigners can add colors to the styleguide in one click from the mockup and define a name for each\nDesigners can also export individual Sketch Symbols, which will then appear in the styleguide Components section.\nThese components are reused by the designer on several mockups. You can see how the navigation looks like with 2 and 3 tabs.\nComponents and mockup are linked: on the styleguide, the developer can see which mockups use which components. Reciprocally, the developer can also see on\u00a0each mockup which components are used and can click the link to see different component states.\nA link to the component styleguide from a mockup on Zeplin\nDesigners can also share their styleguide publicly to get visibility and reactions.\nOn InVision, \u00a0if you want a styleguide you will need to create one from scratch in Sketch.\u00a0Consequently, there are no links between the styleguide and the mockups, so it is less maintainable (or takes too much time to maintain) and is less visible for the team.\nCommenting the mockup to clarify integration\nComments are an essential feature to hand over extra informations not visible on the mockup or the inspector. They also allow to cover edge cases. Is it scrollable? Vertically centered? Is the size proportional? These pieces of information\u00a0should be left by the designers for the developer to use on integration.\nComments on Zeplin are visible when the developer inspects elements\nWhat makes it best on Zeplin is that comments are visible by default on the inspector\u2019s page. On InVision they are on a separate page, and the users need\u00a0to switch between modes. So they\u00a0can forget they exist. What a pity \nComments on InVision are on a separate page as the Inspector\nSmall plus, Zeplin\u00a0lets you to categorise your comments. However\u00a0the links you attach are not always clickable (is it a bug?)\nAnother\u00a0killer feature for pixel perfect integration\n(Only on Mac) With Zeplin\u00a0desktop\u00a0app, you can generate a transparent overlay of the design to compare to the actual development. Of course if you don\u2019t need this level of accuracy it\u2019s just a wow feature \ud83d\ude09\nThis helps the dev checking that their integration matches exactly the mockup, for the desired screen sizes.\nPop out the mockup in a transparent window and move it over your app to check for differences\nCss properties inspection\nThis is a basic feature that\u00a0both platforms allow, but here is what makes Zeplin\u00a0slightly better:\n\nThe inspector\u2019s panel is more condensed\u00a0because null properties are hidden. So you can check properties more easily without having to scroll down\nCSS has syntaxic coloration\n\n\n\n\n\nZeplin inspector\n\nInVision inspector: opacity and left alignment are not useful to have here as these are default values\n\n\n\nPadding/margin inspect\nIn this battle of Zeplin vs Invision, this is the only\u00a0round where\u00a0Zeplin looses!\nDesigners create groups of elements in Sketch to have blocks containing a label and its value for instance, or an input with its submit button \u2013 just like developers might build their app. Designers use these groups to place blocks on the mockup and build a screen.\nWe tried a little experiment on both platforms. The designer made a table with header and values on Sketch.\nHere is\u00a0what you can see on InVision\u2019s inspector tab\nWe uploaded the Sketch file on InVision. Above is what we could see in the inspector: we were able to select the \u2018Group 5\u2032 containing the invoice number and its value\u00a0\u201901_000001\u2019. So by hovering the adjacent block, we could see the margin in between (48px).\nZeplin\u2019s\u00a0mockup inspection panel\nThen we uploaded the same sketch file on Zeplin: the\u00a0Sketch groups are not replicated on the inspector platform: you cannot select the\u00a0container blocks, only the text elements. Therefore, devs cannot see spacing between blocks easily. They loose the designer\u2019s previous reflection on block cutting and spacing. It\u2019s too bad to have two people do the same work twice!\nThe downside of using groups on the other hand is that it makes small details inspection harder. By experience, InVision\u2019s inspector cursor is less accurate because you hover the groups before the element. You need to zoom in for more accuracy.\nManaging Assets\nManaging assets is equivalent on both platforms. Once the designer has set the export options in Sketch, the developer can download\u00a0it from the inspect page when inspecting an element.\nZeplin vs Invision: on both, developers can pick the file format they want from the available list\nYou can find the list of downloadable assets in a specific section.\nOn InVision, you can see the downloadable assets in the group panel.\nIcon inspection on Invision: on the left panel you can see that the Icon/User is downloadable.\nPrototyping\nWhy many choose InVision is because you can make clickable mockups\u00a0to navigate from a screen to another. You can test the\u00a0prototype with\u00a0the targeted users. Very handy to get feedback before development starts!\nThe downside of using InVision both for prototyping and integration comes when several version of the same mockup conflict. This can occur when mockups include\u00a0features under testing phase. It gets confusing for the developers.\nPricing (updated 27/11/18)\nZeplin\nFree tier: 1 project, no collaboration\nIf you are a service company with several clients,\u00a0your clients cannot create an account and invite you on their project, you\u2019ll need to disconnect and connect to\u00a0the same account.\nStarter: 17$/mo 3 projects \u2013 unlimited collaborators\nGrowing business: 26$/mo \u2013 17 projects \u2013 unlimited collaborators\nOrganization: from 122$/mo with only 16 collaborators, +7$/mo per extra collaborator\nInVision\nFree tier: 1 project \u2013 unlimited collaborators\nStarter: 15$/mo \u2013 3 prototypes\nGrowing business: 25$/mo \u2013 unlimited prototypes\nTeam: 99$/mo unlimited prototypes but only 5 members\nCustom: on demand\nTo go further into benchmarking\nSome other tools like Avocode would also need our attention, notably because of their powerful assets export feature. Framer\u2019s new FramerX tool is also an important player we should pay attention too. Their Beta version is not collaborative yet like Invision or Zeplin, but their prototyping tool based on React components is promising.\nWe at Theodo are building our custom Sketch plugin in order to make it even faster to integrate a component.\nWith Overlay, we can export components from Sketch and get prod ready React/VueJs code.\nGenerate React.js/Vue.js components with full design from Sketch files with Overlay plugin\nConclusion\nAll in all,\u00a0these tools will increase your mockup integration process. Nonetheless, the difference lies in\u00a0subtle details and better user experience.\u00a0That\u2019s what gives\u00a0Zeplin\u00a0the edge over\u00a0InVision.\nHaving developers and designers working together is not easy. Indeed, each profile\u00a0has their own stakes and think differently. We have a lot to learn from each other to deliver the best products. Integrating\u00a0the right tool in our process will make the collaboration much smoother.\nFrance and I have been working together to spread the DevUx culture (yes, DevOps is not the only one). This goes from understanding each other, defining mutual expectations and design collaboratively. More articles are coming!\nDon\u2019t hesitate to share your experience and tips using these tools and more generally on your mockup integration processes \nCredits and Resources\nCover pic\u00a0(more sumo battles there, thank you\u00a0Tomoshi Shiiba\u00a0for your art work)\nZeplin/Sketch gif\u00a0(from an article on how\u00a0The Create Labs\u00a0implemented Sketch & Zeplin in their workflow)\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYu Ling Cheng\r\n  \t\t\t\r\n  \t\t\t\tLead Developer at Theodo\r\nhttps://www.linkedin.com/in/yulingcheng  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy\nWhat is a Virtual DOM ?\nThe virtual DOM (VDOM) is a programming concept where an ideal, or \u201cvirtual\u201d, representation of a UI is kept in memory.\nThen, it is\u00a0synced with the \u201creal\u201d DOM by a library such as ReactDOM. This process is called\u00a0reconciliation.\nPerformance\u00a0and\u00a0windowing\nYou might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.\nSometimes you might want to display a lot of html elements, like for grids,\u00a0lists, calendars, dropdowns etc and the user will often complain about performance.\n\nHence, a good way\u00a0to display a lot of information is to \u2018window\u2019\u00a0it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it \u2018renders\u2019 all the pages, the Kindle only display what the user can see.\nReact-Virtualized\nThat is how Brian Vaughn came up with the idea of creating React-Virtualized.\nIt is an open-source library which provides you many components in order to window some of your application List, Grid etc\nAs a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.\nFurthermore, it offers lots of functionalities and customization that you would not even think about.\nWe will discuss about it later, but before, let\u2019s see when to use React-virtualized.\nWhen\nWhen thinking about performance, lots of actions can be taken, but\u00a0React official website\u00a0already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).\nHow\nGetting into it\nOk, now that you\u2019re convinced, let\u2019s go throught the real part.\n\nYou can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github.\u00a0However,\u00a0I\u2019m going to show you a complex example so you can use React-Virtualized in an advanced way.\nReact-Virtualized 101\nTo render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.\nFirstly, you use the List component from the library, then, the few important props are the next one:\n\nwidth: the width of the List\nheight: the height of the List\nrowCount: the number of elements you will display\nrowHeight: the height of each row you will display\nrowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.\n\nThe example\n\r\nimport React from 'react';\r\nimport { List } from 'react-virtualized';\r\n\r\n// List data as an array of strings\r\nconst list = [\r\n 'Easy windowing1',\r\n 'Easy windowing2',\r\n // And so on...\r\n];\r\n\r\nfunction rowRenderer({\r\n key, // Unique key within array of rows\r\n index, // Index of row within collection\r\n isScrolling, // Used for performance\r\n isVisible, // Used for performancee\r\n style, // Style object to be applied to row (to position it)\r\n}) {\r\n return (\r\n\r\n<div key={key} style={style}>\r\n   {list[index]}\r\n </div>\r\n\r\n );\r\n}\r\n\r\n// Render your list\r\nconst ListExample = () => (\r\n <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />\r\n);\r\n\nClick here to see a demo\nA more complex virtualized list:\nDisplay a virtualized list might be easy, but you might have a complicated behaviour to implement.\n\nIn this advanced example, we will:\n\nUse the AutoSizer HOC to automatically calculate the size the List will fill\nBe able to display row with a dynamic height using the CellMeasurer\nBe able to use the CellMeasurer even if the data are not static\n\nThis advanced example goes through 4 steps:\n\nInstantiate the AutoSizer and List component\nSee how the CellMeasurer and the CellMeasurerCache work\nLearn how we use them in the rowRenderer\nGo further with using these on a list that does not contain a stable number of elements\n\nThe example\nLet\u2019s look first at how we render the list:\n\u00a0\n\r\nimport {\r\n AutoSizer,\r\n List,\r\n CellMeasurer,\r\n CellMeasurerCache,\r\n} from 'react-virtualized';\r\n...\r\n<AutoSizer>\r\n {({ height, width }) => (\r\n  <List\r\n    width={width}\r\n    height={height}\r\n    rowGetter={({ index }) => rows[index]}\r\n    rowCount={1000}\r\n    rowHeight={40}\r\n    rowRenderer={this.rowRenderer}\r\n    headerHeight={20}\r\n  />\r\n )}\r\n</AutoSizer>\r\n\nIt is very simple:\n\nWe wrap the list with the AutoSizer HOC\nIt uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.\n\nHow it works :\nFirst, you instantiate a new CellMeasurerCache that will contain all the calculated heights :\n\r\nconstructor(props) {\r\n super(props);\r\n this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best\r\n defaultHeight: 80,\r\n minHeight: 50,\r\n fixedWidth: true,\r\n });\r\n}\r\n\nThen, you use the CellMeasurer in the rowRenderer method:\n\r\nrowRenderer = ({\r\n   key, // Unique key within array of rows\r\n   index, // Index of row within collection\r\n   parent,\r\n   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment\r\n   isVisible, // This row is visible within the List (eg it is not an overscanned row)\r\n   style, // Style object to be applied to row (to position it)\r\n}) => (\r\n   <CellMeasurer\r\n     cache={this.cache}\r\n     columnIndex={0}\r\n     key={key}\r\n     parent={parent}\r\n     rowIndex={index}\r\n   >\r\n   <div\r\n     className=\"Row\"\r\n     key={key}\r\n     style={{\r\n       ...style,\r\n       display: 'flex',\r\n     }}\r\n   >\r\n     <span style={{ width: 400 }}>{rows[index].name}</span>\r\n     <span style={{ width: 100 }}>{rows[index].age}</span>\r\n   </div>\r\n   </CellMeasurer>\r\n);\r\n\n\u00a0\nPitfall:\nFinally, we obtain a nice windowed list, ready to be deployed and used\u2026\nUnless your application contain filters or some data added dynamically.\nActually, when I implemented this, after using some filters, some blank spaces were staying in the list.\nIt is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).\nConsequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:\n\r\ncomponentWillReceiveProps() { //Really important !!\r\n this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no \"blank spaces\" (some row are erased)\r\n this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights\r\n}\r\n\nA big thanks to\u00a0Brian Vaughn\u00a0for this amazing library\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyril Gaunet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. \nThe principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.\nBasically, here is an example of my git history before and after I used this technique.\n\nStay focus, rebase and merge are no joke! \nWhat is the goal of a rebase or a merge ?\nRebase and merge both aim at integrating changes that happened on another branch into your branch.\nWhat happens during a merge ?\nFirst of all there are two types of merge:\n\nFast-forward merge\n3-way merge\n\nFast-forward merge\nA fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.\nThe following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nAs you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.\nNotes:\n\ngit checkout A, git rebase B you would have had the exact same result!\ngit checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.\n\n3-way merge\nA 3-way merge happens when both branches have had new commits since the last shared commit.\nThe following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nDuring a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:\n\nAll the modifications brought by the three commits from B (in purple)\nThe possible conflict resolutions\n\nGit will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.\nThe default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.\nWhat happens during a rebase?\nA rebase differ from a merge in the way in which it integrates the modifications.\nThe following drawings show what happens during a rebase and how it is shown in a graphical git software.\nA: the branch that you are rebasing\nB: the branch from which you get the new commits\n\ngit checkout A\ngit rebase B\n\n\n\nWhen you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.\nFor each commit to apply, if there are conflicts, they will be resolved inside of the commit.\nAfter a rebase, the new commits from A (in blue) are not exactly the same as they were:\n\nIf there were conflicts, those conflicts are integrated in each commit\nThey have a new hash\n\nBut they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.\nWhat is the best solution to integrate a new feature into a shared branch and keep your git tree clean?\nLet say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.\nYou have two main solutions: \nFirst solution: \n\ngit checkout feature\ngit rebase master\ngit checkout master\ngit merge feature\n\n\nNote : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only\nSecond solution:\n\ngit checkout master\ngit merge feature\n\n\nAs you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened.\nIn this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!\nUnfortunately, the first solution has a few drawbacks:\nHistory rewriting\nWhen you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.\nThis can potentially erase changes another developer made, or introduce conflicts resolution for him.\nTo avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.\nHowever you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.\nThe obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all.\nConflicts resolution\nWhen you merge or rebase, you might have to resolve conflicts.\nWhat I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch.\nThe only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.\nConclusion\nTo conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9mie Marniquet Fabre\r\n  \t\t\t\r\n  \t\t\t\tI am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don\u2019t need any huge tech background in order to follow this tutorial.\n1 \u2013 Your very first AR example\nFirst we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.\nLibrary import\nAs I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.\nFirst we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.\nYou can import A-Frame library using the folowing line :\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n\nThen we need to import ARjs, the web-framework for augmented reality we are going to use.\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"></script>\r\n\nInitialize the scene\nA-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :\n<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>\r\n  <!-- All our components goes here -->\r\n</a-scene>\r\n\nNote the 2 elements we have in our a-scene tag :\n\nstats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.\narjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.\n\nShape\nThen we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.\nIn this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :\n<a-box position=\"0 0 0\" rotation=\"0 0 0\"></a-box>\r\n\na-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :\n\nposition : the three coordinates that will be used to position our components\nrotation : that color of the shape\n\nMarker\nWe are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.\n\nDeployment\nAs announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.\nhttp-server\nThere are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.\nFirst you need to install the module by running the following command\nnpm install -g http-server\r\n\nThen to launch your server, you can do it with the command line :\nhttp-server\r\n\nYou can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:\npython -m SimpleHTTPServer 8000\r\n\nngrok\nNow that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.\nThen you use ngrok with the following command :\nngrok http 8080\r\n\nIt will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.\nSo with our index.html containing the following :\n<!doctype HTML>\r\n<html>\r\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\r\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\r\n  <body style='margin : 0px; overflow: hidden;'>\r\n    <a-scene stats embedded arjs='trackingMethod: best;'>\r\n      <a-marker preset=\"hiro\">\r\n      <a-box position='0 1 0' material='color: blue;'>\r\n      </a-box>\r\n      </a-marker>\r\n      <a-entity camera></a-entity>\r\n    </a-scene>\r\n  </body>\r\n</html>\r\n\nOr if you prefer on codepen.\nAnd the result should look like :\n\n2 \u2013 Animation\nNow that we were able to display our first A-frame component, we want to make it move.\nA-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :\n  <a-entity>\r\n    <a-animation></a-animation>\r\n  </a-entity>\r\n\na-animation can be used on various attributes of our entity such as position, rotation, scale or even color.\nWe will see in the next part what can be done with these attributes.\nBasics\nFirst we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.\n\ndur : duration of the animation\nfrom : start position or state of the animation\nto : end position or state of the animation\nrepeat : if and how the animation should be repeated\n\nPosition\nWe will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.\nOur code should look like this :\n<a-animation attribute=\"position\"\r\n    dur=\"1000\"\r\n    from=\"1 0 0\"\r\n    to=\"0 0 1\">\r\n</a-animation>\r\n\nRotation\nRotations work the same, except that instead of a vector you have to indicate three angles :\n<a-animation attribute=\"rotation\"\r\n    dur=\"2000\"\r\n    from=\"0 0 0\"\r\n    to=\"360 0 0\"\r\n    repeat=\"indefinite\">\r\n</a-animation>\r\n\nYou can either make your entity rotate on itself:\n\nYou can access the full code here.\nOr make it rotates around an object:\n\nAnd the full code is here.\nOthers properties\nThis animation pattern works on a lot of others properties, for example :\n\nScale :\n\n<a-animation\r\n    attribute=\"scale\"\r\n    to=\"2 2 3\">\r\n</a-animation>\r\n\n\nColor :\n\n<a-animation\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nTrigger\na-animation has a trigger : begin. That can either be used with a delay or an event. For example :\n<a-animation\r\n    begin=\"3000\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nThis animation will start in 3000ms.\nOtherwise with an event you can use :\n<a-entity id=\"entity\">\r\n<a-animation\r\n    begin=\"colorChange\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nWith the event emission :\ndocument.querySelector('#entity').emit('colorChange');\r\n\nCombining\nYou can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.\nFirst case is very simple you only have to add multiple entities with an animation for each.\nSecond one is more difficult because you must add an entity inside an entity, like in this example :\n<a-entity>\r\n    <a-animation attribute=\"rotation\"\r\n      dur=\"2000\"\r\n      easing=\"linear\"\r\n      from=\"0 0 0\"\r\n      to=\"0 360 0\"\r\n      repeat=\"indefinite\"></a-animation>\r\n          <a-entity rotation=\"0 0 25\">\r\n              <a-sphere position=\"2 0 2\"></a-sphere>\r\n          </a-entity>\r\n</a-entity>\r\n\nThe first entity is used as the axis for our rotating animation.\n3 \u2013 Model loading\nType of models\nYou can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.\nYou can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/\nYou can also use an online library to download models, some of them even allow you to directly use their models.\nLoading a model from the internet\nDuring the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models\nWe will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.\nTo load your 3d model, we will use the following piece of code :\n<a-assets>\r\n      <a-asset-item id=\"smiley\" src=\"https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf\"></a-asset-item>\r\n</a-assets>\r\n\nYou can of course load your very own model the same way.\na-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.\nThe full example look like this.\nDisplay your model\nNow that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :\n<a-entity gltf-model=\"#smiley\" rotation= \"180 0 0\">\r\n</a-entity>\r\n\nRemember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.\nAnd here is a demo :\n\n4 \u2013 Markers\nFor now we only have used Hiro marker, now we will see how we can use other kind of marker.\nBarcode\nIn ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :\n<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>\r\n\nAs you can guess, our value will be found in a 5\u00d75 matrix. And now the a-marker component will look like :\n<a-marker type='barcode' value='5'></a-marker>\r\n\nCustom marker\nYou can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.\nYou can then download the .patt file and you can use it in your application, like this:\n<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>\r\n\nMultiple markers\n<script src=\"https://aframe.io/releases/0.6.0/aframe.min.js\"></script>\r\n<script src=\"https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js\"></script>\r\n<body style='margin : 0px; overflow: hidden;'>\r\n  <a-scene embedded arjs='sourceType: webcam;'>\r\n\r\n    <a-marker type='pattern' url='path/to/pattern-marker.patt'>\r\n      <a-box position='0 0.5 0' material='color: red;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker preset='hiro'>\r\n      <a-box position='0 0.5 0' material='color: green;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker type='barcode' value='5'>\r\n      <a-box position='0 0.5 0' material='color: blue;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-entity camera></a-entity>\r\n  </a-scene>\r\n</body>\r\n\nWhat\u2019s next\nYou now have everything you need to start and deploy a basic AR web application using ARjs.\nIf you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBastien Teissier\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tStop wasting your time on tasks your CI could do for you.\nFind 4 tips on how to better use your CI in order to focus on what matters \u2013 and what you love: code. Let\u2019s face it: as a developer, a huge part of the value you create is your code.\nNote: Some of these tips use the GitHub / CircleCI combo. Don\u2019t leave yet if you use BitBucket or Jenkins! I use GitHub and CircleCi on my personal and work-related projects, so they are the tools I know best. But most of those tips could be set up with every CI on the market.\nTip 1: Automatic Changelogs\nI used to work on a library of React reusable components, like Material UI. Several teams were using components from our library, and with our regular updates, we were wasting a lot of time writing changelogs. We decided to use Conventional Commits. Conventional Commits is a fancy name for commits with a standardized name:\nexample of Conventional Commits\nThe standard format is \u201cTYPE(SCOPE): DESCRIPTION OF THE CHANGES\u201d. \nTYPE can be\n\nfeat: a new feature on your project\nfix: a bugfix\ndocs: update documentation / Readme\nrefactor: a code change that neither fixes a bug nor adds a feature\nor others\u2026\n\nSCOPE (optional parameter) describes what part of your codebase is changed within the commit.\nDESCRIPTION OF THE CHANGES is pretty much what you would write in a \u201ctraditional\u201d commit message. However, you can use keywords in your commit message to add more information. For instance:\nfix(SomeButton): disable by default to fix IE7 behaviour\r\nBREAKING CHANGE: prop `isDisabled` is now mandatory\nWhy is this useful? Three main reasons:\n\nAllow scripts to parse the commit names, and generate changelogs with them\nHelp developers thinking about the impact of their changes (Does my feature add a Breaking Change?)\nAllow scripts to choose the correct version bump for your project, depending on \u201chow big\u201d the changes in a commit are (bugfix: x.y.Z, feature: x.Y.z, breaking change: X.y.z)\n\nThis standard version bump calculation is called Semantic Versioning. Depending of the version bump, you can anticipate the impact on your app and the amount of work needed.\nBe careful though! Not everyone follows this standard, and even those who do can miss a breaking change! You should never update your dependencies without testing everything is fine \ud83d\ude09\nHow to set up Conventional Commits\n\nInstall Commitizen\nInstall Semantic Releases\nAdd GITHUB_TOKEN and NPM_TOKEN to the environment variables of your CI\nAdd `npx semantic-release` after the bundle & tests steps on your CI master/production build\nUse `git cz` instead of `git commit` to get used to the commit message standard\nSquash & merge your feature branch on master/production branch\n\nWhen you get used to the commit message standard, you can go back to `git commit`, but remember the format! (e.g: `git commit -m \u201cfeat: add an awesome feature\u201d`)\nNow, every developer working on your codebase will create changelogs without even noticing it. Plus, if your project is used by others, they only need a glance at your package version/changelog to know what changes you\u2019ve made, and if they are Breaking.\nTip 2a: Run parallel tasks on your CI\nWhy do I say task instead of tests? Because a CI can do a lot more than run tests! You can:\n\nGenerate automatic changelogs \ud83d\ude09 and version your project\nBuild and push the bundle on a release branch\nDeploy your app\nDeploy your documentation site\n\nThere are several ways to use parallelism to run your tasks.\nThe blunt approach\nThis simply consists of using the built-in parallelism of your tasks, combined with a multi-thread CI container.\nWith Jest, you can choose the number of workers (threads) to use for your test with the `\u2013max-workers` flag.\nWith Pytest, try xdist and the `-n` flag to split your tests on multiple CPUs.\nAnother way of parallelizing tests is by splitting the test files between your CI containers, as React tries to do it. However, I won\u2019t write about this approach in this article since the correct way of doing it is nicely explained in the CircleCi docs.\n\u00a0\nTip 2b: CircleCI Workflows\nWith Workflows, we reduced our CI Build time by 25% on feature branches (from 11\u2033 to 8\u203330) and by 30% on our master branch (from 16\u203330 to 11\u203330). With an average of 7 features merged on master a day, this is 1 hour and 30 minutes less waiting every day for our team.\nWorkflow is a feature of CircleCI. Group your tasks in Jobs, then order your Jobs how it suits your project best. Let\u2019s imagine you are building a library of re-usable React Components (huh, I think I\u2019ve already read that somewhere\u2026). Your CI:\n\nSets up your project (maybe spawn a docker, install your dependencies, build your app)\nRuns unit/integration tests\nRuns E2E tests\nDeploys your Storybook\nPublishes your library\n\nEach of those bullet points can be a Job: it may have several tasks in it, but all serve the same purpose. But do you need to wait for your unit tests to pass before launching your E2E tests? Those two jobs are independent and could be running on two different machines.\nOur CircleCI workflow\nExtract of our config.yml\nAs you can see, it is pretty straight-forward to re-order or add dependencies between steps. For more info on how to setup Workflows, check out the documentation.\nThis is also useful for cross-platform testing (you can take a look at Yarn\u2019s workflows).\nNote: Having trouble setting up a workflow? You can SSH on the machine during the build.\n\u00a0\nParallelization drawbacks\nBut be careful with the parallelism: resources are not unlimited; if you share your CI plan with other teams in your organization, make sure using more resources for parallelism will not be counter-productive at a larger scale. You can easily understand why using 2 machines for 10 minutes can be worse than using 1 machine for 15 minutes:\n\u00a0\nProject #2 is queued on CI because there is no machine free when the build was triggered\n\u00a0\nPlus, sharing the Workspace (the current state) of one machine to others (e.g: after running `yarn`, to make your dependencies installed for every job) costs time (both when saving the state on the first machine and loading it on the other).\nSo, when should I parallelize my CI tasks?\nA good rule of thumb is always keeping jobDuration > (nb_containers * workspaceSharingDuration).\nWorkspace sharing can take up to a minute for a large codebase. You should try several workflow configurations to find what\u2019s best for you.\n\u00a0\nTip 3: Set up cron(tab)s\nCrontabs help make your CI more reliable without making builds longer.\n\nWant to run in-depth performance tests that need to send requests to your app? Schedule it for night time with a cron!\nWant to publish a new version of your app every week? Cron.\nWant to train your ML model but it takes hours? Your CI could trigger the training every night.\n\nSome of you may wonder: what is a cron/crontab? Cron(tab) is an abbreviation of ChronoTable, a job scheduler. A cron is a program that executes a series of instructions at a given time. It can be once an hour, once a day, once a year\u2026\nI worked on a project in finance linking several sources of data and API\u2019s. Regression was the biggest fear of our client. If you give a user outdated or incorrect info, global financial regulators could issue you a huge fine. Therefore, I built a tool to generate requests with randomized parameters (country, user profile\u2026), play them, and check for regressions. The whole process can take an hour. We run it via our CI, daily, at night, and it saved the client a lot of trouble.\nYou can easily set up crons on CircleCi if you\u2019ve already tried Jobs/Workflows. Check out the documentation.\nNote: Crons use the POSIX date-time notation, which can be a bit tricky at first. Check out this neat Crontab Tester tool to get used to it!\n\u00a0\nMisc tips:\n\nLearn Shell! All Continuous Integration / Continuous Delivery systems can run Shell scripts. Don\u2019t be afraid to sparkle some scripts in your build! Add quick checks between/during tasks to make debugging easier, or make your build fail faster: you don\u2019t want to wait for the full 10 minutes when you can check at 2\u201930 that your lockfile is not up-to-date!\nUse cache on your project dependencies!\nAdd extra short task to your CI to connect useful tools like Codecov.io or Danger\n\n\u00a0\nIf you have any other tip you would like to share, don\u2019t hesitate!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAur\u00e9lien Le Masson\r\n  \t\t\t\r\n  \t\t\t\tDeveloper @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a quick guide on how to set up the debugger in VS code server-side for use with Node.js in a Docker container. I recently worked on a project which uses the Koa.js framework as the API. Whilst trying to set up the debugger with VS code, a google search led to several articles that had conflicting information about how to set it up and the port number to expose, or was overly verbose and complicated.\nTo keep things simple, I have split this into 3 steps.\n1) Check version of Node.js on\u00a0server\nTo do this with docker-compose set up, use the following, replace [api] with the name of your\u00a0docker container.\ndocker-compose exec api\u00a0node --version\nInspector\u00a0Protocol\u00a0(Node V7+, since Oct 2016)\nRecent versions of Node.js now uses the inspector protocol. This is easier to set up and is the default setting for new Node.js applications, as most documentation will refer to this protocol. This means that:\n\nThe --inspect flag is required when starting the node process.\nBy default, the port 9229 is exposed, and is equivalent to --inspect:9229\nThe port can be changed, eg. --inspect-brk:1234 . Here, the \u2018-brk\u2019 flag adds a breakpoint on start.\n\nLegacy\u00a0Protocol (Node V6 and earlier)\nOlder versions of Node.js (prior to V7) uses the \u2018Legacy Debugger\u2019. The version of Node.js used on my project was 6.14. This means that:\n\nThe\u00a0--debug\u00a0flag is required when starting the node process.\nBy default, the port 5858 is exposed, and is equivalent to\u00a0--debug:5858\nThis port cannot be changed.\n\nFor more information goto:\nhttps://code.visualstudio.com/docs/nodejs/nodejs-debugging\nhttps://nodejs.org/en/docs/guides/debugging-getting-started/\n2) Expose port in Node and Docker\nIn \u2018package.json\u2019, add\u00a0--debug:5858\u00a0 (or\u00a0--inspect:9229\u00a0depending on Node version) when starting Node, so:\n\"dev\": \"nodemon index.js\",\u00a0becomes\n\"debug\": \"nodemon --debug:5858 index.js\",\nIn \u2018docker-compose.yml\u2019, run the debug node command\u00a0and expose the port. In my case:\napi:\nbuild: ./api\ncommand: yarn dev\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\nbecomes:\napi:\nbuild: ./api\ncommand: yarn debug\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\n- \"5858:5858\"\n3)\u00a0Set launch configuration of Debugger\nIn \u2018/.vscode/launch.json\u2019, my launch configuration is:\n{\n\"type\": \"node\",\n\"request\": \"attach\",\n\"name\": \"Docker: Attach to Node\",\n\"port\": 5858,\n\"address\": \"localhost\",\n\"localRoot\": \"${workspaceFolder}/api/\",\n \"remoteRoot\": \"/code/\",\n\"protocol\": \"legacy\"\n}\nThe port and protocol needs to correspond to the version of Node used as determine above. For newer versions of Node:\u00a0\"port\": \"9229\" and \"protocol\": \"inspector\" should be used.\n\u201clocalRoot\u201d and \u201cremoteRoot\u201d should be set to the folder corresponding to the entry point (eg. index.js) of your Node application in the local repository and the docker folder respectively.\n4) Attach debugger and go!\nIn VS code, set your breakpoints and press F5 or click the green triangle button to start debugging! By default VS code comes with a debug panel to the left and debug console to the bottom, and a moveable debug toolbar. Mousing over a variable shows its values if it has any.\n\n\u00a0\nI hope this article has been useful, and thanks for reading my tech blog!\u00a0 \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHo-Wan To\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHow to recode Big Brother in 15 min on your couch\nFace Recognition Explained\nIn this article, we will step by step implement a smart surveillance system, able to recognise people in a video stream and tell you who they are. \nMore seriously, we\u2019ll see how we can recognise in real-time known faces that appear in front of a camera, by having a database of portraits containing those faces.\nFirst, we\u2019ll start by identifying the different essential features that we\u2019ll need to implement. To do that,\u00a0we\u2019ll analyse the way we would to that, as human beings (to all the robots that are reading those words, I hope I don\u2019t hurt your feelings too much and I truly apologize for the methodology of this article).\nAsk yourself : if someone passes just in front of you, how would you recognise him ?\n\nYou\u2019ll need first to see the person\nYou then need to focus on the face itself\nThen there are actually two possibilities.\n\nEither I know this individual and I can recognise him by comparing his face with every face I know.\nOr I don\u2019t know him\n\n\n\nLet\u2019s see now how to the algo will do those different steps.\nFirst step of the process : seeing the person\nThis is quite a simple step. We\u2019ll simply need a computer and a webcam, to capture the video stream. \nWe\u2019ll use openCV Python. With a few lines of code, we\u2019ll be able to capture the video stream, and dispose of the frame one by one.\nimport cv2\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   frame = video_capture.read()\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nHow to detect a face in a picture ?\nTo be able to find a face in the picture, let\u2019s ask ourselves, what is a face and how can we discriminate a face from Gmail\u2019s logo for example ?\n\n\nWe actually do it all the time without even thinking about it. But how can we know that easily that all these pictures are faces ?\n\nWhen we look at those different pictures, photographs and drawings, we see that a face is actually made of certain common elements : \n\nA nose\nTwo eyes\nA mouth \nEars\n\u2026\n\nBut not only are the presence of these elements essential, but their positions is also paramount. \nIndeed, in the two pictures here, you\u2019ll find all the elements that you\u2019ll find in a face. Except one is a face, and one is not.\n\nSo, now that we\u2019ve seen that a face is characterised by certain criterias, we\u2019ll turn them into simple yes-no questions, which will be very useful to find a face in a square image.\nAs a matter of fact, the question \u201cIs there a face in a picture ?\u201d is very complex. However, we\u2019ll be able to approximate it quite well by asking one after the other a series of simple question : \u201cis there a nose ?\u201d ; \u201cIs there an eye ? If yes, is their two eyes ?\u201d ; \u201cAre there ears ?\u201d ; \u201cIs there some form of symmetry ?\u201d. \nAll these questions are both far more simple than the complex question \u201cIs there a face in the picture ?\u201d, while providing us with information to know if part of the image is or is not a face. \nFor each one of these questions, a no answer is very strong and will tell us that there is definitely no face in the picture. \nOn the contrary, a yes answer will not allow us to be sure that there is a human face, but it will slightly increase the probability of the presence of a face.\u00a0If the image is not a face, but it is tree, the answer to the first question \u201cis there a nose ?\u201d will certainly be negative. No need then to ask if there are eyes, or if there is some form of symmetry.\nHowever, if indeed there is a nose, we can go forward and ask \u201care there ears?\u201d. If the answer is still yes, this won\u2019t mean that there is a face, but will slightly increase the likeliness of this possibility, and we will keep digging until being sufficiently confident to say that there is a face indeed.\nThe interest is that the simplicity of the questions will reduce drastically the cost of face detection, and allow to do real-time face detection on a video stream. \nThis is the principle of a detection method called \u201cthe cascade of weak classifier\u201d. Every classifier is very weak considering that it gives only a very little degree of certitude. But if we do the checks one by one, and a region of the picture has them all, we\u2019ll be at the end almost sure that a face is present here. \nThat\u2019s why it is called a cascade classifier, because like a series of waterfalls, the algorithm will simply do a very simple and quick check, one by one, and will only move forward with another check if the first one is positive. \nTo do face detection on the whole picture, and considering that we don\u2019t know in advance the size of the face, we\u2019ll simply apply the cascade algorithm on a moving window for every frame.\n\nWhat we\u2019ve explained here is the principle of the algorithm. Lots of research has been made about how to use cascade for object detection. OpenCV has a built-in way to do face detection with a cascade classifier, by using a set of 6,000 weak classifiers especially developed to do face detection.\nimport cv2\r\n\r\nopencv_path = 'm33/lib/python3.7/site-packages/cv2/data/'\r\nface_cascade = cv2.CascadeClassifier(opencv_path + 'haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    # Capture frame-by-frame\r\n    ret, frame = video_capture.read()\r\n\r\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n \r\n    # Draw a rectangle around the faces\r\n    for (x, y, w, h) in faces:\r\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n    # Display the resulting frame\r\n    cv2.imshow('Video', frame)\r\n\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\nNow that we can detect the face, we need to recognise it among other known faces. Here is what we got :\u00a0\nFace recognition\nNow that we have a system able to detect a face, we need to make sense out of it and recognise the face.\nBy applying the same methodology as before, we\u2019ll find the criterias to recognise a face among others.\nTo do that, let\u2019s look at how we differentiate between two faces : Harry Potter on one side, and Aragorn on the second.\n\nLet\u2019s make a list of the things that can differentiate them : \n\nForm of their nose\nForm of their eyes\nTheir hair\nColor of their eyes\nDistance between the eyes\nSkin color\nBeard\nHeight of the face\nWidth of the face\nRatio of height to width\n\nOf course, this list is not exhaustive. However, are all those criterias good for face recognition ? \nSkin color is a bad one for example. We\u2019ve all seen Harry Potter or Aragorn after hard battles covered with dirt or mud, and we\u2019re still able to recognise them easily.\nSame goes for height and width of the face. Indeed, these measures change a lot with the distance of the person to the camera. Despite that we can easily recognise the faces even when their size changes. \nSo we can keep some good criterias that will really help recognise a face : \n\nForm of their nose\nForm of their eyes\nDistance between the eyes\nRatio of height to width \nPosition of the nose relative to the whole face\nForm of eyebrows\n\u2026\n\nLet\u2019s now measure all these elements. By doing this, we\u2019ll have a set of values that describe the face of an individual. These measures are a discrete description of what the face looks like.\n\nActually, what we have done, is that we reduced the face to a limited number of \u201cfeatures\u201d that will give us valuable and comparable information of the given face.\n\nMathematically speaking, we have simply created a vector space projection, that allowed us to reduce the number of dimensions in our problem. From a million-dimensions vector space problems (if the picture is 1MPixel RGB image, the vector space is of 1M * 3 dimensions) to a an approximately a-hundred-dimension vector space. The problem becomes far more simple ! \nNo need to consider all the pixels in the picture at all, we only need to extract from the image a limited set of features. These extracted features can be considered as vectors that we can then compare the way we do it with any vector by computing euclidean distances for example.\n\nAnd just like that, comparing faces becomes mathematically as simple as computing the distance between two points on a grid, with only a few more dimensions ! To be simple, it\u2019s as though, every portrait can then be described as a point in space. The closer points are, the more likely they describe the same face ! And that\u2019s all !\nWhen we find a face in a frame, we find its position in the feature-space and we look for the nearer known point. If the distance between the two is close, we\u2019ll consider that they\u2019re both linked to the same face. Otherwise, if the point representing the new face is too far from all the faces known, it means we don\u2019t know this face.\n\nTo implement that, we\u2019ll use the face_recognition Python library that allows us to use a deep learning algorithm that extracts from a face a 128-dimension vector of features. \nWe\u2019ll do it in two steps.\nWe first turn our portrait database into a set of computed feature-vectors (reference points like the Frodo point in the example above). \nimport face_recognition\r\nimport os\r\nimport pandas as pd\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n\t// happens when no face is detected\r\n       Continue\r\n\r\nfeatures_df = pd.DataFrame(face_features, names)\r\nfeatures_df.to_csv('database.csv')\r\n\r\n\nThen, we load the database and launch the real-time face recognition:\nimport cv2\r\nimport pandas as pd\r\nfrom helpers import load_database\r\nimport PIL\r\nimport numpy as np\r\nimport face_recognition\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n       # happens when no face is detected\r\n       Continue\r\n\r\nface_cascade= cv2.CascadeClassifier('m33/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   ret, frame = video_capture.read()\r\n   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n   faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n  \r\n   # Draw a rectangle around the faces\r\n   for (x, y, w, h) in faces:\r\n       cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n       pil_im = PIL.Image.fromarray(frame[y:y+h, x:x+w])\r\n       face = np.array(pil_im.convert('RGB'))\r\n       try:\r\n           face_descriptor = face_recognition.face_encodings(face)[0]\r\n       except Exception:\r\n           continue\r\n       distances = np.linalg.norm(face_descriptors - face_descriptor, axis=1)\r\n       if(np.min(distances) < 0.7): found_name = names[np.argmin(distances)] print(found_name) print(found_name) #y = top - 15 if top - 15 > 15 else top + 15\r\n       cv2.putText(frame, found_name, (y, y-15), cv2.FONT_HERSHEY_SIMPLEX,\r\n                   0.75, (0, 255, 0), 2)\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nAnd here it comes ! \n\nHere is a github repo with the code working : https://github.com/oussj/big_brother_for_dummies\nExternal links I used : \n\nhttps://github.com/ageitgey/face_recognition\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78\nhttps://realpython.com/face-recognition-with-python/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tOussamah Jaber\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from MINES ParisTech, I joined Theodo as an agile web developer to use cutting-edge technology and build awesome products !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I started web development, the developer tools were so new to me I thought I would save time not using them in the first place. I quickly realized how wrong I was as I started using them. No more console.log required, debugging became a piece of cake in a lot of cases!\nThe Network tab is used to track all the interactions between your front end and your back end, thus the network.\nIn this article, I will show usages of the developer tools Network tab on Google Chrome web browser.\nLet\u2019s start exploring the network tab!\nRecord the network logs\nThe first thing to do is to record the network logs by making sure the record button is activated before going on the page you want to record: \n\nCheck the response of a request sent to your server\nYou can keep only the requests sent to your server by clicking on the filter icon and then selecting \u201cXHR\u201d:\n\nIn that section, you can see some information about your requests:\n\nHTTP status code\nType of request\nSize of the request\nEtc.\n\nTo get more details about a request, you can simply click on it.\nLet\u2019s look at the my-shortcuts endpoint that retrieves the shortcuts of an user connected on the application I am using. You can look at the formatted response by clicking on the \u201cPreview\u201d tab: \n\nIf the response of an XHR is an image, you will be able to preview the image instead.\nOn this tab, it becomes easy to determine if the format of the response corresponds to what your front end expected.\nGreat! Now you know how to check the response of a request sent to your server without writing any console.log in your code!\nTest your requests with various Internet connection speeds\nIf the users of the application you are developing have a lower Internet speed than yours, it can be interesting to test your application with custom Internet speed.\nIt is possible to do so by using bandwidth throttling by clicking on the following dropdown menu: \u00a0\n\n\nReplay a request\nReplaying a request can be useful if you want to see how the front end interacts with the response of a request again or if you need to test your request with different parameters. It can be long and painful to reload the page and reproduce exactly the same actions over and over again. Here are some better ways to replay a request:\n\n When right-clicking on a request, you can copy the cURL format of your request and paste it in the terminal of your computer to send the request to your back end:\n\n\n\n When right-clicking on a request, you can copy the request headers and paste them in your favorite API development environment (e.g. Postman):\n\n\nIn Postman, click on \u201cHeaders tab\u201d > \u201cBulk Edit\u201d to edit the headers:\n\nNow all you need to do is paste your headers. Don\u2019t forget to comment the path of the request which is not a header: \u00a0\n\n\n\n If you are using \u201cXHR\u201d requests, you can simply right-click on the request you want to replay and click on \u201cReplay XHR: \n\n\nI hope that I could help you debug your network interactions with this article!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJordan Lao\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"}
][
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few weeks ago, Nicolas and I launched https://jamstack.paris, a JAMstack website powered by GatsbyJS and hosted on Netlify.\nJAMstack applications\u00a0deliver static websites to end-users with the benefit of high performance and CDN ease and still allow dynamism by sourcing content or data during build time.\n\nOn https://jamstack.paris for example, we collect the number of attendees for our events using Gatsby Source Meetup plugin and this happens on build time.\nThe drawback of this dynamism-on-build approach is that our website may be not synced with real count of attendees until we build the site again.\n\nSo, we wanted a quick way to build the site on Netlify with a simple button on our smartphones, and\u00a0you can have the same setup in less than five minutes, let\u2019s go \ud83d\ude80.\nClaim your build hook on Netlify\nHead on Settings > Build & Deploy > Build hooks\u00a0section on your Netlify dashboard and hit the \u201cAdd build hook\u201d button.\n\nGive an explicit name to your hook, for example\u00a0Smartphone Deploy and copy the curl request Netlify provides to you.\n\nSend\u00a0the build request from your phone\nNow we want to sent this build request from our phone.\nYou are on android \ud83d\udc7e\nHTTP Request Shortcuts\u00a0application is doing exactly that\u00a0and requires no permissions on device, which is good in terms of privacy.\n\n\nYou are on iOS \ud83c\udf4f\nThe native Shortcuts application does the job. Head to the app and create a new shortcut. You will need two actions:\n\nURL\nGet Contents of URL\n\nPaste the URL from Netlify (without curl) in the field of the first action.\nThe second action is also really easy to setup. Just open the Advanced options and change the Method to POST.\nHurray! Now we can deploy from our\u00a0smartphone\u2019s homescreen \ud83d\ude80\nIf you have any suggestions or questions, feel free to add them in the comments. And if you want to speak about JAMstack in Paris, we\u2019d love to get in touch with you. First meetup gathering in Paris on December 18th\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tKotlin is an object-oriented programming language for making Android apps that uses Java-like syntax with functional programming features. It was created by Jetbrains, the makers of hugely popular IDEs like IntelliJ and PyCharm, and is used by big companies such as Pinterest, Uber, and Atlassian.\nAt Theodo, React Native is the language of choice for building apps. And for good reason. It has a rich ecosystem of libraries to use, hot reloading out of the box, is very similar to React which is a hugely popular web framework, and vitally it allows you to compile your apps to both Android and iOS from just one codebase. But this doesn\u2019t mean that we should necessarily use React Native in every case. I will talk through some examples of why Kotlin might be a better choice in some cases.\nWhy use Kotlin/Java over React Native?\nThere are many reasons why one may choose to write native code for an app rather than React Native. One of the most common I have seen is API support. Many services that have APIs for apps cannot be used fully within React Native. This leads to developers having to write native code to get the functionality they need. This can be tricky to developers who may not have ever written native code. Kotlin, on the other hand, is strongly supported within the Android ecosystem and is interoperable with Java so it has easier access to these APIs. Any API that supports Java 8 can be easily used with Kotlin.\nAnother big reason to prefer native code over React Native is performance. For advanced functionality or heavy computation, the overhead of using React Native can considerably slow down an app. Kotlin, on the other hand, appears and acts totally natively without lag.\nMany of us have suffered through the pain of having to update React or React Native. It can be a long and arduous task. However, with writing Kotlin, each new version is backwards compatible with the last. That means that even legacy code written by previous teams doesn\u2019t have to be rewritten when updating to a new version of Kotlin.\nA big problem with large javascript projects is the errors that can come up due to the lack of typing. This is why many developers choose to integrate Flow or TypeScript to help with this. However, this can be a difficult task, especially if the codebase is already large before you start. Kotlin however is statically typed meaning that there is no need for tools like these to check for type errors.\nAnother benefit of using Kotlin over React Native is that it is fully compatible with Android Studio, and Intellij. This means you can make use of loads of cool features like advanced code completion, the built in debugger, and gradle integration, all while benefitting from a UI that was purpose built for making Android apps.\nWhy use Kotlin instead of Java?\nKotlin was created by JetBrains as an attempt to be a more concise and feature-rich version of Java, whilst being designed primarily for Android app development. I understand that writing native code can feel intimidating to many people who have only written Javascript or Python before, but I see Kotlin as a good stepping stone; it has the benefits of writing native code but it will feel more familiar in its functional aspects such as spread, deconstruction, and closures.\nKotlin can also be used to write iOS applications. This is a great benefit when writing applications for both iOS and Android as the application logic can be shared between the two. However, the rendering code is platform specific and thus must be rewritten for each.\nA great thing about Kotlin is that it\u2019s fully compatible with Java. This means that Java code can be inserted into it, or it can be inserted into a Java codebase. This can be helpful if you have a large codebase of legacy code written in Java which you need to add new features to.\nKotlin has all the features of Java 8, plus more. This means you get lambda functions, static functions, streams and nested classes from Java 8, on top of immutable variables, method references, methods without classes, extension methods to classes, closures, spread and deconstruction. As well as all this, built in @Nullable and @NonNull mean that there are no Null-Pointer Exceptions which comes as a great relief to those who are apprehensive to get started writing native code.\nWriting Kotlin, you can use any library that is compatible with Java 8. As well as this, Kotlin has a lot of libraries written for it such as kotlinx-coroutines-android and rxkotlin for writing asynchronous and event-based programs, just to name a few. Whilst still perhaps not as many libraries as React Native might have, there are still a huge number of libraries to take advantage of when writing Kotlin.\nDrawbacks of using Kotlin/Java over React Native\nSadly no tool is perfect and there are many reasons why you may choose to go for React Native over native code. The main reason is iOS compatibility. Yes, Kotlin code can be compiled to objective C, and is bi-directionally interoperable with Objective-C and Swift. But you still have to write separate render code for iOS and Android. This could take a long time and is the main reason why one would choose React Native.\nAnother big reason is that writing React allows hot reloading straight out of the box. While Kotlin does allow for hot reloading, it can be a pain to set up and work unreliably. Hot reloading can really speed up development and if you\u2019re used to having it you may sorely miss it if you can\u2019t get it working for Kotlin development.\nConclusion\nIf you want to get the best of both worlds, it is possible to use React Native to implement the frontend code and Kotlin to implement the backend of your application. This means that you get the speed improvements and Java-compatible API integration of Kotlin, but only have to write one codebase for both Android and iOS. As well as this, you can make use of the front-end libraries available to React Native.\nJetBrains have created a library called kotlin-wrappers which contains wrappers to use Kotlin with React, Redux, styled-components, and React Router DOM, amongst others. There are also a few other libraries online which do similar things.\nThis is a very new field and there are not many people using it just yet, so there may be some teething issues. One possible problem is that passing the store via props to the component won\u2019t work because the state will become immutable. There may be libraries to help with this, and if not then I\u2019m sure there soon will be, but this could be a big pain point.\nThis emerging combination of languages looks like a really new, interesting addition to our use of React Native here at Theodo. I hope I will on the next applicable project to test out this exciting new tech.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAbbie Howell\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFor one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products.\nAs a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).\nThe impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.\nThus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems.\nAnd it worked!\nWhat is make\nMake is a build automation tool created in 1976, designed to solve dependency problems of the build process.\nIt was originally used to get compiled files from source code, for instance for C language.\nIn website development, an equivalent could be the installation of a project and its dependencies from the source code.\nLet me introduce a few concepts about make that we will need after.\nMake reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.\nThe architecture of the Makefile is the following:\n\r\ntarget: [prerequisite]\r\n    command1\r\n    [command2]\r\n\nAnd you run make target in your terminal to execute the target\u00a0commands. Simple, right?\nUse it for project installation\nWhat is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.\nWhat if all these commands were executed by running make install?\nYou would have your project working with one command, and no documentation to maintain anymore.\nI will only describe a simple way to build dependencies from your composer.json file\n\r\nvendor: composer.json\r\n    composer install\r\n\nThis snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.\nNote that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.\nThere is much more you can do, and I won\u2019t talk about it here.\nBut if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018.\nUse it for the commonly used commands you need on your project\nAfter the project installation, a complexity for the developer is to find the commands needed to develop features locally.\nWe decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis.\nWhat are the advantages of this:\n\nThe commands are committed and versioned\nAll developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something\nIt is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026\nIt\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options\nYou can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!\n\nAuto-generate a help target\nBut after a long time, we started having a lot of commands.\nIt was painful to find the one you wanted in the file, and even to know the one that existed\nSo we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.\nThe initial snippet we used is:\n\r\n.DEFAULT_GOAL := help\r\nhelp:\r\n    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'\r\n\nIf you add the following target and comments in your Makefile:\n\r\n## Example section\r\nexample_target: ## Description for example target\r\n        @does something\r\n\nIt would give this help message:\n\nThis generic, reusable snippet has another advantage: the documentation it generates is always up to date!\nAnd you can customize it to your need, for instance to display options associated to your commands.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMartin Guillier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago I was a young developer starting his journey on React. My client came to see my team and told us: \u201cWe need to make reusable components\u201d, I asked him: \u201cWhat is a reusable component?\u201d and the answer was \u201cThis project is a pilot on the subject\u201d.\n2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable\u00a0\ud83d\ude31\nAt the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?\nThe answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !\n Storybook, what is that?\nIt is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.\nThis allows you to visually describe your components : anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!\nA picture is worth a thousand words,\u00a0so just check the best example I know, the open Storybook of Airbnb.\nOne interesting thing to\u00a0note is that it\u2019s working with Vue, Angular and React!\nUsage example\nLet\u2019s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.\nThen I added Storybook to the project, I won\u2019t detail this part as the\u00a0Storybook doc\u00a0is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup your asset builder.\nNow I\u2019ll focus on the component FilteredList that display the todos, first it looked like this:\n\r\nimport React from 'react';\r\nimport styled from 'styled-components';\r\nimport TodoItem from './TodoItem';\r\n\r\nconst StyledUl = styled.ul`\r\n  list-style: none;\r\n`;\r\n\r\nconst StyledP = styled.p`\r\n  margin: 10px 0;\r\n  padding: 10px;\r\n  border-radius: 0;\r\n  background: #f2f2f2;\r\n  border: 1px solid rgba(229, 229, 229, 0.5);\r\n  color: #888;\r\n`;\r\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus} = props;\r\n\r\n    if (items.length === 0) {\r\n        return (\r\n            <StyledP>There are no items.</StyledP>\r\n        );\r\n    }\r\n\r\n    return (\r\n        <StyledUl>\r\n            {items.map(item => (\r\n                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>\r\n            ))}\r\n        </StyledUl>\r\n    );\r\n}\r\n\n(It is not exactly the same as the one on the repo, I\u00a0used styled-component instead of plain css)\nTodoItem is the component that displays an element of the list.\nHere we can see there are two different branches in the render: the nominal case and the empty state.\nLet\u2019s write some stories, I created a file FilteredList.stories.js\u00a0and added this inside:\n\r\nimport React from 'react';\r\nimport { storiesOf } from '@storybook/react';\r\nimport FilteredList from \"./FilteredList\";\r\n\r\nconst data = [{\r\n    id: 1,\r\n    completed: true,\r\n    text: 'Jean-Claude Van Damme'\r\n}];\r\n\r\nstoriesOf('FilteredList')\r\n    .add('Nominal usage', () => (\r\n        <FilteredList items={data} changeMode={() => void 0}/>\r\n    ))\r\n    .add('Empty state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0}/>\r\n    ));\r\n\nSo what did I do here?\nI defined placeholder data in a variable for the component props.\nWe use the function storiesOf\u00a0from storybook, this function takes the name we want to give to the group of stories as entry param.\nThen we add some stories with .add. It\u00a0works pretty much\u00a0like jest or mocha\u2019s\u00a0describe or\u00a0it\u00a0in tests, it takes the name of the story and a function that returns the component to render.\nHere\u2019s what it looks like:\n\n\nIt\u2019s rather simple but it\u2019s working, we see the two different cases.\nWhat if we add other branches? Let\u2019s say the parent component of FilteredList\u00a0is calling an API to get the list and so we have to add a loading and error state.\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus, isLoading, error} = props;\r\n\r\n    if (isLoading) {\r\n        return (\r\n            <StyledP>Loading...</StyledP>\r\n        )\r\n    }\r\n\r\n    if (error) {\r\n        return (\r\n            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>\r\n        )\r\n    }\r\n    \r\n    //...\r\n}\r\n\nNow we need to add the corresponding stories.\n\r\n.add('Loading state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0} isLoading />\r\n))\r\n.add('Error state', () => (\r\n    <FilteredList items={[]} changeMode={() => void 0} error=\"Internal server error\"/>\r\n));\r\n\nAnd now our Storybook looks like:\n\n\nThis example is rather simple but it shows pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.\nAin\u2019t nobody got time for that?\nIn fact it takes time when you are coding but I feel like it is more like an investment.\nWhen you develop your app\u00a0aren\u2019t you trying to make it easy to use? Then why not make it easy for developers to use your code?\nAnd that\u2019s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.\nThis is very important because you are not the only user of your code, you might be working with a team or someone else will\u00a0take over\u00a0your project afterwards.\nWe all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)\nFinal thoughts\nIn a way we are all trying to do reusable components \u2013 whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It\u2019s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that\u2019s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tL\u00e9o Anesi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLate in 2018 AWS released Lambda Layers and custom runtime support. This means that to run unsupported runtimes you no longer need to \u2018hack\u2019 around with VMs, docker or using node.js to `exec` your binary.\nRecently I needed to\u00a0setup\u00a0a 100% serverless PHP infrastructure for a client. PHP is one option, but similar steps can allow you to run any language.\nLambda layers provide shared code between different lambda functions. For instance, if you wanted to share your vendor code between lambdas (e.g. node_modules for node.js).\nWe will create a Lambda layer to provide the PHP binary for our custom runtime API. You could also create a second to provide the vendor folder for composer dependencies.\n# Step 1: Compiling the\u00a0PHP\u00a0binary\nWe will need a `/bin` directory containing the PHP binary. Because we are compiling a binary this step needs to happen on the same OS and architecture that our Lambda will use.\n\nThis page\u00a0lists the AWS Execution environment, but last time I checked their version was out of date.\nTo find the correct AMI I used the latest version for the region I was deploying in, found here\u00a0by a quick regex for AMIs containing `amzn-ami-hvm-.*-gp2`.\n\nOnce you have the correct AMI, spin up a large EC2 instance and `ssh` in.\nRun the following commands as listed in AWS\u2019s docs.\n\r\nsudo yum update -y\r\nsudo yum install autoconf bison gcc gcc-c++ libcurl-devel libxml2-devel -y\r\n\r\ncurl -sL http://www.openssl.org/source/openssl-1.0.1k.tar.gz | tar -xvz\r\ncd openssl-1.0.1k\r\n./config && make && sudo make install\r\ncd ~\r\n\r\nmkdir ~/php-7-bin\r\ncurl -sL https://github.com/php/php-src/archive/php-7.3.0.tar.gz | tar -xvz\r\ncd php-src-php-7.3.0\r\n\r\n./buildconf --force\r\n./configure --prefix=/home/ec2-user/php-7-bin/ --with-openssl=/usr/local/ssl --with-curl --with-zlib\r\nmake install\r\n\nCheckpoint:\u00a0The following should give\u00a0you the version number of PHP you wanted\n /home/ec2-user/php-7-bin/bin/php -v \nMove this into a `/bin` directory.\n`mkdir './bin && mv php ./bin\nNow we can zip up the code for our Lambda layer that will provide our custom runtime API.\nzip -r runtime.zip bin bootstrap\n# Vendor files\nOn the same EC2, we need to use composer to get our vendor code.\ncurl -sS https://getcomposer.org/installer | ./bin/php`\nAdd some vendor code:\n./bin/php composer.phar require guzzlehttp/guzzle\n\u00a0\nzip -r vendor.zip vendor/\n\u00a0# Bring down to local\nNow use `scp` to copy the PHP binary down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/runtime.zip .\nNow use `scp` to copy the vendor zip down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/vendor.zip .\nDon\u2019t forget to terminate your large EC2 instance\n# Creating a custom runtime API\nTo use a custom runtime AWS requires you specify a bootstrap file which will provide the interface for lambda events. As we will be writing PHP support we can write it in PHP (very meta).\nCreate a `bootstrap` executable:\ntouch ./bootstrap && chmod +x ./bootstrap\nExample adapted from AWS docs\n\r\n#!/opt/bin/php\r\n<?php\r\n\r\n// This invokes Composer's autoloader so that we'll be able to use Guzzle and any other 3rd party libraries we need.\r\nrequire __DIR__ . '/vendor/autoload.php';\r\n\r\n// amzn-ami-hvm-2017.03.1.20170812-x86_64-gp2\r\n\r\nfunction getNextRequest()\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $response = $client->get('http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/next');\r\n\r\n return [\r\n 'invocationId' => $response->getHeader('Lambda-Runtime-Aws-Request-Id')[0],\r\n 'payload' => json_decode((string) $response->;getBody(), true)\r\n ];\r\n}\r\n\r\nfunction sendResponse($invocationId, $response)\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $client->post(\r\n 'http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/' . $invocationId . '/response',\r\n ['body' => $response]\r\n );\r\n}\r\n\r\n// This is the request processing loop. Barring unrecoverable failure, this loop runs until the environment shuts down.\r\ndo {\r\n // Ask the runtime API for a request to handle.\r\n $request = getNextRequest();\r\n\r\n // Obtain the function name from the _HANDLER environment variable and ensure the function's code is available.\r\n $handlerFunction = array_slice(explode('.', $_ENV['_HANDLER']), -1)[0];\r\n require_once $_ENV['LAMBDA_TASK_ROOT'] . '/src/' . $handlerFunction . '.php';\r\n\r\n // Execute the desired function and obtain the response.\r\n $response = $handlerFunction($request['payload']);\r\n\r\n // Submit the response back to the runtime API.\r\n sendResponse($request['invocationId'], $response);\r\n} while (true);\r\n\r\n?>\r\n\n\u2013 Note: #!/opt/bin/php links to our /bin/php created earlier.\nManual Deployment\n\nGo to AWS lambda page.\nCreate a function selecting to use a custom runtime.\nCreate a layer called `php` and upload `runtime.zip`.\nCreate a layer called `vendor` and upload `vendor.zip`.\nApply the layers to the function you created in the merge order: 1) runtime, 2) vendor\n\nWriting a handler function\n\r\n\r\nmkdir src\r\n\r\ntouch src/hello.php\r\n\r\n\nAdd some basic function called `hello`:\n\r\n<?php\r\n\r\nfunction hello($data)\r\n{\r\n return \"Hello, {$data['name']}!\";\r\n}\r\n\r\n?>\r\n\nThen zip this up to be uploaded:.\nzip hello.zip src/hello.php\nUpload the function handler zip to the function and change the handler name to the name of the php file without the extension. e.g. hello.php => hello\n\u00a0\n# Automation\nMany of the steps here can be automated once you have compiled your binary. Either by using the AWS API, cloudformation or the `serverless`\u00a0library which supports layers.\n# Other languages\nThese steps should allow any language that can compile on the AWS AMI used by Lambda to be used as your runtime, e.g. Rust.\n\u00a0\nResources:\n\nhttps://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\nhttps://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you already know what scraping is, you can directly jump to how I did it\nWhat is scraping?\nScraping is the process of data mining. Also known as web data extraction, web harvesting, spying.. It is software that simulates human interaction with a web page to retrieve any wanted information (eg images, text, videos). This is done by a scraper.\nThis scraper involves making a GET request to a website and parsing the html response. The scraper then searches for the data required within the html and repeats the process until we have collected all the data we want.\nIt is useful for quickly accessing and analysing large amounts of data, which can be stored in a CSV file, or in a database depending on our needs!\nThere are many reasons to do web scraping such as lead generation and market analysis. However, when scraping websites, you must always be careful not to violate the terms and conditions of websites you are scraping, or to violate anyone\u2019s privacy. This is why it is thought to be a little controversial but can save many hours of searching through sites, and logging the data manually. All of those hours saved mean a great deal of money saved.\nThere are many different libraries that can be used for web scraping, e.g. selenium, phantomjs.\u00a0In Ruby you can also use the nokogiri gem to write your own ruby based scraper. Another popular library is is beautiful soup which is popular among python devs.\nAt Theodo, we needed to use a web scraping tool with the ability to follow links and as python developers the solution we opted for was using theDjango framework with an open source web scraping framework called\u00a0Scrapy.\nScrapy and Django\nScrapy allows us to define data structures, write data extractors, and comes with built in CSS and xpath selectors that we can use to extract the data, the scrapy shell, and built in JSON, CSV, and XML output. There is also a built in FormRequest class which allows you to mock login and is easy to use out of the box.\nWebsites tend to have countermeasures to prevent excessive requests, so Scrapy randomises the time between each request by default which can help avoid getting banned from them. Scrapy can also be used for automated testing and monitoring.\nDjango has an integrated admin which makes it easy to access the db. That along with the ease of filtering and sorting data and import/export library to allow us to export data.\nScrapy also used to have a built in class called DjangoItem which is now an easy to use external library. The DjangoItem library provides us with a class of item that uses the field defined in a Django model just by specifying which model it is related to. The class also provides a method to create and populate the Django model instance with the item data from the pipeline. This library allows us to integrate Scrapy and Django easily and means we can also have access to all the data directly in the admin!\nSo what happens?\n\u00a0\nSpiders\nLet\u2019s start from the spider. Spiders are the core of the scraper. It makes the request to our defined URLs, parses the responses, and extracts information from them to be processed in the items.\nScrapy has a start_requests method which generates a request with the URL. When Scrapy fetches a website according to the request, it will parse the response to a callback method specified in the request object. The callback method can generate an item from the response data or generate another request.\nWhat happens behind the scenes? Everytime we start a Scrapy task, we start a crawler to do it. The Spider defines how to perform the crawl (ie following links). The crawler has an engine to drive it\u2019s flow. When a crawler starts, it will get the spider from its queue, which means the crawler can have more than one spider. The next spider will then be started by the crawler and scheduled to crawl the webpage by the engine. The engine middlewares drive the flow of the crawler. The middlewares are organised in chains to process requests and responses.\nSelectors\nSelectors can be use to parse a web page to generate an item. They select parts of the html document specified either by xpath or css expressions. Xpath selects nodes in XML docs (that can also be used in HTML docs) and CSS is a language for applying styles to HTML documents. CSS selectors use the HTML classes and id tag names to select the data within the tags. Scrapy in the background using the cssselect library transforms these CSS selectors into xpath selectors.\nCSS vs Xpath\n\r\n        data = response.css(\"div.st-about-employee-pop-up\") \r\n        data = response.xpath(\"//div[@class='team-popup-wrap st-about-employee-pop-up']\")\r\n\nShort but sweet: when dealing with classes, ids and tag names, use CSS selectors. If you have no class name and just know the content of the tag use xpath selectors. Either way chrome dev tools can help: copy selector for the element\u2019s unique css selector or you can copy its xpath selector. This is to give a basis, may have to tweak it! Two more helper tools are XPath helper and this cheatsheet. Selectors are also chainable.\nItems and Pipeline\nItems produce the output. They are used to structure the data parsed by the spider. The Item Pipeline is where the data is processed once the items have been extracted from the spiders. Here we can run tasks such as validation and storing items in a database.\nHow I did it\nHere\u2019s an example of how we can integrate Scrapy and Django. Let\u2019s scrape the data off the Theodo UK Team Page and integrate it into a Django Admin Panel:\n\nGenerate Django project with integrated admin + db\nCreate a django project, with admin and database\nCreate app and add to installed apps\nDefine the data structure, so the item, so our django model.\n## models.py\r\n      from django.db import model\r\n\r\n      class TheodoTeam(models.Model):\r\n        name = models.CharField(max_length=150)\r\n        image = models.CharField(max_length=150)\r\n        fun_fact = models.TextField(blank=True)\r\n\r\n        class Meta:\r\n            verbose_name = \"theodo UK team\"\r\n      \n\nInstall Scrapy\nRun\nscrapy startproject scraper\n\nConnect using DjangoItem\n    ## items.py\r\n      from scrapy_djangoitem import DjangoItem\r\n      from theodo_team.models import TheodoTeam\r\n\r\n      class TheodoTeamItem(DjangoItem):\r\n        django_model = TheodoTeam\r\n    \n\nThe Spider \u2013 Spiders have a starturls class which takes a list of URLs. The URLs will then be used by the startrequests method to create the initial requests for your spider. Then using the response and selectors, select the data required.\nimport scrapy\r\n    from scraper.items import TheodoTeamItem\r\n\r\n    class TheodoSpider(scrapy.Spider):\r\n      name = \"theodo\"\r\n      start_urls = [\"https://www.theodo.co.uk/team\"]\r\n\r\n      # this is what start_urls does\r\n      # def start_requests(self):\r\n      #     urls = ['https://www.theodo.co.uk/team',]\r\n      #     for url in urls:\r\n      #       yield scrapy.Request(url=url, callback=self.parse)\r\n\r\n      def parse(self, response):\r\n          data = response.css(\"div.st-about-employee-pop-up\")\r\n\r\n          for line in data:\r\n              item = TheodoTeamItem()\r\n              item[\"name\"] = line.css(\"div.h3 h3::text\").extract_first()\r\n              item[\"image\"] = line.css(\"img.img-team-popup::attr(src)\").extract_first()\r\n              item[\"fun_fact\"] = line.css(\"div.p-small p::text\").extract().pop()\r\n              yield item\r\n    \n\nPipeline \u2013 use it to save the items to the database\n## pipelines.py\r\n    class TheodoTeamPipeline(object):\r\n      def process_item(self, item, spider):\r\n          item.save()\r\n          return item\r\n    \n\nCreate a Django command to run Scrapy crawl \u2013 This initialises django in the scraper and is needed to be able to access django in the spider.\n## commands/crawl.py\r\n\r\n    from django.core.management.base import BaseCommand\r\n    from scraper.spiders import TheodoSpider\r\n    from scrapy.crawler import CrawlerProcess\r\n    from scrapy.utils.project import get_project_settings\r\n\r\n    class Command(BaseCommand):\r\n      help = \"Release the spiders\"\r\n\r\n      def handle(self, *args, **options):\r\n          process = CrawlerProcess(get_project_settings())\r\n\r\n          process.crawl(TheodoSpider)\r\n          process.start()\r\n    \n\nRun manage.py crawl to save the items to the database\n\nProject Structure:\n scraper\r\n     management\r\n         commands\r\n             crawl.py\r\n     spiders\r\n         theodo_team_spider.py\r\n         apps.py\r\n         items.py\r\n         middlewares.py\r\n         pipelines.py\r\n         settings.py\r\n theodo_team\r\n     admin\r\n     migrations\r\n     models\r\n\nChallenges and problems encountered:\nSelectors!! Selectors are not one size fits all. Different selectors are needed for every website and if there is constant layout changes, they require upkeep. It can also be difficult to find all the data required without manipulating it. This occurs when tags may not have a class name or if data is not consistently stored in the same tag.\nAn example of how complicated selectors can get:\nsegments = response.css(\"tr td[rowspan]\")\r\nrowspan = int(segment.css(\"::attr(rowspan)\").extract_first())\r\n           all_td_after_segment = segment.xpath(\"./../following-sibling::tr[position()<={}]/td\".format(rowspan- 1))\r\n\r\nline = all_td_after_segment.extract_first()\r\ndata = line.xpath(\"./descendant-or-self::a/text()\")\r\nmore_data = line.xpath(\"substring-after(substring-before(./strong/text()[preceding-sibling::a], '%'), '\\xa0')\").extract_first()\r\n\nAs you can see, setting up the scraper is not the hard part! I think integrating Scrapy and Django is a desirable, efficient and speedy solution to be able to store data from a website into a database.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHenriette Brand\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tZeplin\u00a0vs\u00a0InVision: I work for a service company as a lead developer and we have been using\u00a0these tools\u00a0on different projects for mockup integration. I teamed up with France Wang, lead designer at BAM, to list the pros and cons and give you our combined point of view on these\u00a0design handoff tools.\n\nTo clarify the need we have on our projects and what we use these platforms for, here is our development workflow:\n\nDesigners make the mockups on\u00a0Sketch\nThey\u00a0import the mockup on Zeplin or InVision along with the assets\n(They add comments for developers to\u00a0explain\u00a0specific behaviours)\nDevs use the platform to inspect mockups and integrate them\n\nUsually there are back-and forth between devs and designers during the mockup integration\u00a0phase (4). Choosing the best tool helps limiting these wastes and frustrations. We focus on these goals for the benchmark. We\u2019ll also share tips on how we use these tools for better\u00a0collaboration.\nDisclaimer: this content is not sponsored by either mentioned parties\nAnd the best tool for mockup integration is\u2026\n\nAfter benchmarking the two solutions, our recommendation would be to use Zeplin,\u00a0as it is more advanced and more convenient for design handoff.\nIf you have the budget (17$-26$/month) do not hesitate! The time you\u2019ll save is worth it. The platform eases knowledge sharing between designers and developer. Consequently the production workflow will be faster and less painful. Happier collaboration yay \\o/! #DevUx\nIf you need prototyping and do user testing, use InVision in addition. Designers can use the same Sketch file for both platforms. Separate tools for separate purposes!\nDetailed Benchmark of Zeplin vs Invision\nHere is a recap of the comparison we made:\n\n\n\n\n\n\n\n\nZeplin\u2019s Pros:\n\nKiller\u00a0features:\n\nAutomatically-generated styleguide linked to the mockups\nCommenting tool\nPixel perfect comparison\n\n\nMost\u00a0complete tool for mockup integration\n\n\n\nInVision\u2019s Pros:\n\nInteresting free plan (as many collaborators as you want)\nMore complete offer if you need prototyping\nWill get the job done\n\n\n\n\n\nZeplin\u2019s Cons:\n\nPlans are more expensive and free plan less interesting if your want several collaborators\n\n\n\nInVision\u2019s Cons\n\nOverall\u00a0less efficient because the platform is multipurpose and the UX is not focused on\u00a0mockup integration.\n\n\n\n\n\nKiller feature: Semi-automatically generated styleguide\nWhy should you use a styleguide?\nUsing a styleguide helps us save time and limit rework. To\u00a0both designers and developers, the styleguide defines a design reference\u00a0(for both UI and UX).\nFor the designers, it helps with\n\nkeeping the product consistent\nsummarising\u00a0all colors, font styles, components used throughout the app\u00a0along as their states (idle, disabled, active, loading, success, error\u2026) and variations\u00a0(primary, secondary, icon only etc\u2026)\nexplaining to the devs each\u00a0component behaviour without having to repeat it on each mock up\n\nFor developers, it serves as\n\na reference of standard color codes, font sizes, font weights to use in the code\na library of components\u00a0with the variations\u00a0and states they can have\na compilation of\u00a0components behaviour so as\u00a0not to forget any cases\n\nStyleguides on Zeplin vs Invision: why Zeplin wins\nZeplin integrates an interface to create a styleguide from imported mockups. The platform detects font properties and colors so the designers can add them to the styleguide easily.\nDesigners can add colors to the styleguide in one click from the mockup and define a name for each\nDesigners can also export individual Sketch Symbols, which will then appear in the styleguide Components section.\nThese components are reused by the designer on several mockups. You can see how the navigation looks like with 2 and 3 tabs.\nComponents and mockup are linked: on the styleguide, the developer can see which mockups use which components. Reciprocally, the developer can also see on\u00a0each mockup which components are used and can click the link to see different component states.\nA link to the component styleguide from a mockup on Zeplin\nDesigners can also share their styleguide publicly to get visibility and reactions.\nOn InVision, \u00a0if you want a styleguide you will need to create one from scratch in Sketch.\u00a0Consequently, there are no links between the styleguide and the mockups, so it is less maintainable (or takes too much time to maintain) and is less visible for the team.\nCommenting the mockup to clarify integration\nComments are an essential feature to hand over extra informations not visible on the mockup or the inspector. They also allow to cover edge cases. Is it scrollable? Vertically centered? Is the size proportional? These pieces of information\u00a0should be left by the designers for the developer to use on integration.\nComments on Zeplin are visible when the developer inspects elements\nWhat makes it best on Zeplin is that comments are visible by default on the inspector\u2019s page. On InVision they are on a separate page, and the users need\u00a0to switch between modes. So they\u00a0can forget they exist. What a pity \nComments on InVision are on a separate page as the Inspector\nSmall plus, Zeplin\u00a0lets you to categorise your comments. However\u00a0the links you attach are not always clickable (is it a bug?)\nAnother\u00a0killer feature for pixel perfect integration\n(Only on Mac) With Zeplin\u00a0desktop\u00a0app, you can generate a transparent overlay of the design to compare to the actual development. Of course if you don\u2019t need this level of accuracy it\u2019s just a wow feature \ud83d\ude09\nThis helps the dev checking that their integration matches exactly the mockup, for the desired screen sizes.\nPop out the mockup in a transparent window and move it over your app to check for differences\nCss properties inspection\nThis is a basic feature that\u00a0both platforms allow, but here is what makes Zeplin\u00a0slightly better:\n\nThe inspector\u2019s panel is more condensed\u00a0because null properties are hidden. So you can check properties more easily without having to scroll down\nCSS has syntaxic coloration\n\n\n\n\n\nZeplin inspector\n\nInVision inspector: opacity and left alignment are not useful to have here as these are default values\n\n\n\nPadding/margin inspect\nIn this battle of Zeplin vs Invision, this is the only\u00a0round where\u00a0Zeplin looses!\nDesigners create groups of elements in Sketch to have blocks containing a label and its value for instance, or an input with its submit button \u2013 just like developers might build their app. Designers use these groups to place blocks on the mockup and build a screen.\nWe tried a little experiment on both platforms. The designer made a table with header and values on Sketch.\nHere is\u00a0what you can see on InVision\u2019s inspector tab\nWe uploaded the Sketch file on InVision. Above is what we could see in the inspector: we were able to select the \u2018Group 5\u2032 containing the invoice number and its value\u00a0\u201901_000001\u2019. So by hovering the adjacent block, we could see the margin in between (48px).\nZeplin\u2019s\u00a0mockup inspection panel\nThen we uploaded the same sketch file on Zeplin: the\u00a0Sketch groups are not replicated on the inspector platform: you cannot select the\u00a0container blocks, only the text elements. Therefore, devs cannot see spacing between blocks easily. They loose the designer\u2019s previous reflection on block cutting and spacing. It\u2019s too bad to have two people do the same work twice!\nThe downside of using groups on the other hand is that it makes small details inspection harder. By experience, InVision\u2019s inspector cursor is less accurate because you hover the groups before the element. You need to zoom in for more accuracy.\nManaging Assets\nManaging assets is equivalent on both platforms. Once the designer has set the export options in Sketch, the developer can download\u00a0it from the inspect page when inspecting an element.\nZeplin vs Invision: on both, developers can pick the file format they want from the available list\nYou can find the list of downloadable assets in a specific section.\nOn InVision, you can see the downloadable assets in the group panel.\nIcon inspection on Invision: on the left panel you can see that the Icon/User is downloadable.\nPrototyping\nWhy many choose InVision is because you can make clickable mockups\u00a0to navigate from a screen to another. You can test the\u00a0prototype with\u00a0the targeted users. Very handy to get feedback before development starts!\nThe downside of using InVision both for prototyping and integration comes when several version of the same mockup conflict. This can occur when mockups include\u00a0features under testing phase. It gets confusing for the developers.\nPricing (updated 27/11/18)\nZeplin\nFree tier: 1 project, no collaboration\nIf you are a service company with several clients,\u00a0your clients cannot create an account and invite you on their project, you\u2019ll need to disconnect and connect to\u00a0the same account.\nStarter: 17$/mo 3 projects \u2013 unlimited collaborators\nGrowing business: 26$/mo \u2013 17 projects \u2013 unlimited collaborators\nOrganization: from 122$/mo with only 16 collaborators, +7$/mo per extra collaborator\nInVision\nFree tier: 1 project \u2013 unlimited collaborators\nStarter: 15$/mo \u2013 3 prototypes\nGrowing business: 25$/mo \u2013 unlimited prototypes\nTeam: 99$/mo unlimited prototypes but only 5 members\nCustom: on demand\nTo go further into benchmarking\nSome other tools like Avocode would also need our attention, notably because of their powerful assets export feature. Framer\u2019s new FramerX tool is also an important player we should pay attention too. Their Beta version is not collaborative yet like Invision or Zeplin, but their prototyping tool based on React components is promising.\nWe at Theodo are building our custom Sketch plugin in order to make it even faster to integrate a component.\nWith Overlay, we can export components from Sketch and get prod ready React/VueJs code.\nGenerate React.js/Vue.js components with full design from Sketch files with Overlay plugin\nConclusion\nAll in all,\u00a0these tools will increase your mockup integration process. Nonetheless, the difference lies in\u00a0subtle details and better user experience.\u00a0That\u2019s what gives\u00a0Zeplin\u00a0the edge over\u00a0InVision.\nHaving developers and designers working together is not easy. Indeed, each profile\u00a0has their own stakes and think differently. We have a lot to learn from each other to deliver the best products. Integrating\u00a0the right tool in our process will make the collaboration much smoother.\nFrance and I have been working together to spread the DevUx culture (yes, DevOps is not the only one). This goes from understanding each other, defining mutual expectations and design collaboratively. More articles are coming!\nDon\u2019t hesitate to share your experience and tips using these tools and more generally on your mockup integration processes \nCredits and Resources\nCover pic\u00a0(more sumo battles there, thank you\u00a0Tomoshi Shiiba\u00a0for your art work)\nZeplin/Sketch gif\u00a0(from an article on how\u00a0The Create Labs\u00a0implemented Sketch & Zeplin in their workflow)\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYu Ling Cheng\r\n  \t\t\t\r\n  \t\t\t\tLead Developer at Theodo\r\nhttps://www.linkedin.com/in/yulingcheng  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\ttl:dr\nTo add a pre-commit git hook with Husky:\n\nInstall Husky with npm install husky --save-dev\nSet the pre-commit command in your package.json:\n\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nWhat are git hooks?\nGit hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created.\nThe scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.\n\nWhy do I need to install the Husky\u00a0package then?\nThe problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.\nHusky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:\n\nTheses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nTo add husky to your project, simply run npm install husky --save-dev.\nFor more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\".\nEnhancing your git flow\nGit hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.\n\nCheck for linting errors\n\nIf you have tools to check the code quality or formatting, you can run it on a pre-commit hook:\n\"scripts\": {\r\n    \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"\r\n},\r\n\nI advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.\n\nProtect important branches\n\nIn some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:\n\"scripts\": {\r\n    \"prepush\": \"./scripts/pre-push-check.sh\"\r\n},\r\n\n\n#!/bin/bash\r\nset -e\r\n\r\nbranch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')\r\nif [ \"$branch\" == \"master\" ]\r\nthen\r\n    npm test\r\nfi\r\n\n\nIf your tests fail, the code won\u2019t be pushed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy?\nAdding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.\nSetup Symfony and AWS\nFirst you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.\nAmazon AWS\nCreating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.\nFollow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.\nSymfony\nNow you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:\n\nVichUploader (a bundle that will ease files upload)\nKNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)\nAWS-SDK (A SDK provided by Amazon to communicate with AWS API)\n\nInstall the two bundles and the SDK with composer:\n\r\ncomposer require vich/uploader-bundle\r\ncomposer require aws/aws-sdk-php\r\ncomposer require knplabs/knp-gaufrette-bundle\r\n\r\n\nThen register the bundles in AppKernel.php\n\r\npublic function registerBundles()\r\n    {\r\n     return [\r\n            \tnew Vich\\UploaderBundle\\VichUploaderBundle(),\r\n            \tnew Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),\r\n            ];\r\n    }\r\n\r\n\nBucket parameters\nIt is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment.\nYou will need to define four parameters to get access to your AWS bucket:\n\nAWS_BUCKET_NAME\nAWS_BASE_URL\nAWS_KEY (only for and private buckets)\nAWS_SECRET_KEY (only for and private buckets)\n\nYou can find the values of these parameters in your AWS console.\nConfiguration\nYou will have to define a service extending Amazon AWS client and using your AWS credentials.\nAdd this service in services.yml:\n\r\nct_file_store.s3:\r\n        class: Aws\\S3\\S3Client\r\n        factory: [Aws\\S3\\S3Client, 'factory']\r\n        arguments:\r\n            -\r\n                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)\r\n                region: YOUR_AWS_S3_REGION\r\n                credentials:\r\n                    key: '%env(AWS_KEY)%'\r\n                    secret: '%env(AWS_SECRET_KEY)%'\r\n\r\n\nNow you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.\nHere is a basic example:\n\r\nknp_gaufrette:\r\n    stream_wrapper: ~\r\n    adapters:\r\n        document_adapter:\r\n            aws_s3:\r\n                service_id: ct_file_store.s3\r\n                bucket_name: '%env(AWS_BUCKET_NAME)%'\r\n                detect_content_type: true\r\n                options:\r\n                    create: true\r\n                    directory: document\r\n    filesystems:\r\n        document_fs:\r\n            adapter:    document_adapter\r\n\r\nvich_uploader:\r\n    db_driver: orm\r\n    storage: gaufrette\r\n    mappings:\r\n        document:\r\n            inject_on_load: true\r\n            uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET_NAME)%/document\"\r\n            upload_destination: document_fs\r\n            delete_on_update:   false\r\n            delete_on_remove:   false \r\n\r\n\nUpload files\nFirst step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).\nThe attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().\n\r\nnamespace MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\nuse Symfony\\Component\\HttpFoundation\\File\\File;\r\nuse Vich\\UploaderBundle\\Mapping\\Annotation as Vich;\r\n\r\n/**\r\n * Class Document\r\n *\r\n * @ORM\\Table(name=\"document\")\r\n * @ORM\\Entity()\r\n * @Vich\\Uploadable()\r\n */\r\nclass Document\r\n{\r\n    /**\r\n     * @var int\r\n     *\r\n     * @ORM\\Column(type=\"integer\", name=\"id\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(type=\"string\", length=255, nullable=true)\r\n     */\r\n    private $documentFileName;\r\n\r\n    /**\r\n     * @var File\r\n     * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")\r\n     */\r\n    private $documentFile;\r\n\r\n    /**\r\n     * @var \\DateTime\r\n     *\r\n     * @ORM\\Column(type=\"datetime\")\r\n     */\r\n    private $updatedAt;\r\n}\r\n\r\n\nThen you can add an uploaded document to any of your entities:\n\r\n     /**\r\n     * @var Document\r\n     *\r\n     * @ORM\\OneToOne(\r\n     *     targetEntity=\"\\MyBundle\\Entity\\Document\",\r\n     *     orphanRemoval=true,\r\n     *     cascade={\"persist\", \"remove\"},\r\n     * )\r\n     * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")\r\n     */\r\n    private $myDocument;\r\n\r\n\nCreate a form type to be able to upload a document:\n\r\nclass UploadDocumentType extends AbstractType\r\n{\r\n    public function buildForm(FormBuilderInterface $builder, array $options)\r\n    {\r\n        add('myDocument', VichFileType::class, [\r\n                'label'         => false,\r\n                'required'      => false,\r\n                'allow_delete'  => false,\r\n                'download_link' => true,\r\n            ]);\r\n    }\r\n...\r\n}\r\n\r\n\nUse this form type in your controller and pass the form to the twig:\n\r\n...\r\n$myEntity = new MyEntity();\r\n$form = $this->createForm(UploadDocumentType::class, $myEntity);\r\n...\r\nreturn [ 'form' => $form->createView()];\r\n\r\n\nFinally, add this form field in your twig and you should see an upload field in your form:\n\r\n<div class=\"row\">\r\n    <div class=\"col-xs-4\">\r\n        {{ form_label(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8\">\r\n        {{ form_widget(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8 col-xs-offset-4\">\r\n        {{ form_errors(form.myDocument) }}\r\n    </div>\r\n</div>\r\n\r\n\nNavigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.\nUsers are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.\nDisplay or download documents stored in private buckets\nIn most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.\nGet your document from private bucket\nYou will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.\ns3Client is the service (ct_file_store.s3) we defined previously extending AWS S3 client with credentials for private bucket. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.\n\r\n     /**\r\n     * @param string $documentName\r\n     *\r\n     * @return \\Aws\\Result|bool\r\n     */\r\n    public function getDocumentFromPrivateBucket($documentName)\r\n    {\r\n        try {\r\n            return $this->s3Client->getObject(\r\n                [\r\n                    'Bucket' => $this->privateBucketName,\r\n                    'Key'    => 'my-documents/'.$documentName,\r\n                ]\r\n            );\r\n        } catch (S3Exception $e) {\r\n            // Handle your exception here\r\n        }\r\n    }\r\n\r\n\nDefine an action with a custom route:\nYou will need to use the method previously defined to download the file from AWS and expose it on a custom route.\n\r\n     /**\r\n     * @param Document $document\r\n     * @Route(\"/{id}/download-document\", name=\"download_document\")\r\n     * @return RedirectResponse|Response\r\n     */\r\n    public function downloadDocumentAction(Document $document)\r\n    {\r\n        $awsS3Uploader  = $this->get('app.service.s3_uploader');\r\n\r\n        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());\r\n\r\n        if ($result) {\r\n            // Display the object in the browser\r\n            header(\"Content-Type: {$result['ContentType']}\");\r\n            echo $result['Body'];\r\n\r\n            return new Response();\r\n        }\r\n\r\n        return new Response('', 404);\r\n    }\r\n\r\n\nDownload document\nEventually add a download button to access a document stored in a private bucket directly in your Symfony application.\n\r\n<a href=\"{{ path('/download-document', {'id': document.id}) }}\" \r\n                   target=\"_blank\">\r\n   <i class=\"fa fa-print\">\r\n   {{ 'label_document_download'|trans }}\r\n</a>\r\n\r\n\nPublic assets\nYou may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.\n\r\n<img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />\r\n\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlan Rauzier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy\nWhat is a Virtual DOM ?\nThe virtual DOM (VDOM) is a programming concept where an ideal, or \u201cvirtual\u201d, representation of a UI is kept in memory.\nThen, it is\u00a0synced with the \u201creal\u201d DOM by a library such as ReactDOM. This process is called\u00a0reconciliation.\nPerformance\u00a0and\u00a0windowing\nYou might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.\nSometimes you might want to display a lot of html elements, like for grids,\u00a0lists, calendars, dropdowns etc and the user will often complain about performance.\n\nHence, a good way\u00a0to display a lot of information is to \u2018window\u2019\u00a0it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it \u2018renders\u2019 all the pages, the Kindle only display what the user can see.\nReact-Virtualized\nThat is how Brian Vaughn came up with the idea of creating React-Virtualized.\nIt is an open-source library which provides you many components in order to window some of your application List, Grid etc\nAs a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.\nFurthermore, it offers lots of functionalities and customization that you would not even think about.\nWe will discuss about it later, but before, let\u2019s see when to use React-virtualized.\nWhen\nWhen thinking about performance, lots of actions can be taken, but\u00a0React official website\u00a0already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).\nHow\nGetting into it\nOk, now that you\u2019re convinced, let\u2019s go throught the real part.\n\nYou can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github.\u00a0However,\u00a0I\u2019m going to show you a complex example so you can use React-Virtualized in an advanced way.\nReact-Virtualized 101\nTo render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.\nFirstly, you use the List component from the library, then, the few important props are the next one:\n\nwidth: the width of the List\nheight: the height of the List\nrowCount: the number of elements you will display\nrowHeight: the height of each row you will display\nrowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.\n\nThe example\n\r\nimport React from 'react';\r\nimport { List } from 'react-virtualized';\r\n\r\n// List data as an array of strings\r\nconst list = [\r\n 'Easy windowing1',\r\n 'Easy windowing2',\r\n // And so on...\r\n];\r\n\r\nfunction rowRenderer({\r\n key, // Unique key within array of rows\r\n index, // Index of row within collection\r\n isScrolling, // Used for performance\r\n isVisible, // Used for performancee\r\n style, // Style object to be applied to row (to position it)\r\n}) {\r\n return (\r\n\r\n<div key={key} style={style}>\r\n   {list[index]}\r\n </div>\r\n\r\n );\r\n}\r\n\r\n// Render your list\r\nconst ListExample = () => (\r\n <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />\r\n);\r\n\nClick here to see a demo\nA more complex virtualized list:\nDisplay a virtualized list might be easy, but you might have a complicated behaviour to implement.\n\nIn this advanced example, we will:\n\nUse the AutoSizer HOC to automatically calculate the size the List will fill\nBe able to display row with a dynamic height using the CellMeasurer\nBe able to use the CellMeasurer even if the data are not static\n\nThis advanced example goes through 4 steps:\n\nInstantiate the AutoSizer and List component\nSee how the CellMeasurer and the CellMeasurerCache work\nLearn how we use them in the rowRenderer\nGo further with using these on a list that does not contain a stable number of elements\n\nThe example\nLet\u2019s look first at how we render the list:\n\u00a0\n\r\nimport {\r\n AutoSizer,\r\n List,\r\n CellMeasurer,\r\n CellMeasurerCache,\r\n} from 'react-virtualized';\r\n...\r\n<AutoSizer>\r\n {({ height, width }) => (\r\n  <List\r\n    width={width}\r\n    height={height}\r\n    rowGetter={({ index }) => rows[index]}\r\n    rowCount={1000}\r\n    rowHeight={40}\r\n    rowRenderer={this.rowRenderer}\r\n    headerHeight={20}\r\n  />\r\n )}\r\n</AutoSizer>\r\n\nIt is very simple:\n\nWe wrap the list with the AutoSizer HOC\nIt uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.\n\nHow it works :\nFirst, you instantiate a new CellMeasurerCache that will contain all the calculated heights :\n\r\nconstructor(props) {\r\n super(props);\r\n this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best\r\n defaultHeight: 80,\r\n minHeight: 50,\r\n fixedWidth: true,\r\n });\r\n}\r\n\nThen, you use the CellMeasurer in the rowRenderer method:\n\r\nrowRenderer = ({\r\n   key, // Unique key within array of rows\r\n   index, // Index of row within collection\r\n   parent,\r\n   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment\r\n   isVisible, // This row is visible within the List (eg it is not an overscanned row)\r\n   style, // Style object to be applied to row (to position it)\r\n}) => (\r\n   <CellMeasurer\r\n     cache={this.cache}\r\n     columnIndex={0}\r\n     key={key}\r\n     parent={parent}\r\n     rowIndex={index}\r\n   >\r\n   <div\r\n     className=\"Row\"\r\n     key={key}\r\n     style={{\r\n       ...style,\r\n       display: 'flex',\r\n     }}\r\n   >\r\n     <span style={{ width: 400 }}>{rows[index].name}</span>\r\n     <span style={{ width: 100 }}>{rows[index].age}</span>\r\n   </div>\r\n   </CellMeasurer>\r\n);\r\n\n\u00a0\nPitfall:\nFinally, we obtain a nice windowed list, ready to be deployed and used\u2026\nUnless your application contain filters or some data added dynamically.\nActually, when I implemented this, after using some filters, some blank spaces were staying in the list.\nIt is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).\nConsequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:\n\r\ncomponentWillReceiveProps() { //Really important !!\r\n this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no \"blank spaces\" (some row are erased)\r\n this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights\r\n}\r\n\nA big thanks to\u00a0Brian Vaughn\u00a0for this amazing library\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyril Gaunet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don\u2019t need any huge tech background in order to follow this tutorial.\n1 \u2013 Your very first AR example\nFirst we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.\nLibrary import\nAs I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.\nFirst we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.\nYou can import A-Frame library using the folowing line :\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n\nThen we need to import ARjs, the web-framework for augmented reality we are going to use.\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"></script>\r\n\nInitialize the scene\nA-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :\n<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>\r\n  <!-- All our components goes here -->\r\n</a-scene>\r\n\nNote the 2 elements we have in our a-scene tag :\n\nstats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.\narjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.\n\nShape\nThen we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.\nIn this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :\n<a-box position=\"0 0 0\" rotation=\"0 0 0\"></a-box>\r\n\na-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :\n\nposition : the three coordinates that will be used to position our components\nrotation : that color of the shape\n\nMarker\nWe are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.\n\nDeployment\nAs announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.\nhttp-server\nThere are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.\nFirst you need to install the module by running the following command\nnpm install -g http-server\r\n\nThen to launch your server, you can do it with the command line :\nhttp-server\r\n\nYou can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:\npython -m SimpleHTTPServer 8000\r\n\nngrok\nNow that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.\nThen you use ngrok with the following command :\nngrok http 8080\r\n\nIt will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.\nSo with our index.html containing the following :\n<!doctype HTML>\r\n<html>\r\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\r\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\r\n  <body style='margin : 0px; overflow: hidden;'>\r\n    <a-scene stats embedded arjs='trackingMethod: best;'>\r\n      <a-marker preset=\"hiro\">\r\n      <a-box position='0 1 0' material='color: blue;'>\r\n      </a-box>\r\n      </a-marker>\r\n      <a-entity camera></a-entity>\r\n    </a-scene>\r\n  </body>\r\n</html>\r\n\nOr if you prefer on codepen.\nAnd the result should look like :\n\n2 \u2013 Animation\nNow that we were able to display our first A-frame component, we want to make it move.\nA-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :\n  <a-entity>\r\n    <a-animation></a-animation>\r\n  </a-entity>\r\n\na-animation can be used on various attributes of our entity such as position, rotation, scale or even color.\nWe will see in the next part what can be done with these attributes.\nBasics\nFirst we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.\n\ndur : duration of the animation\nfrom : start position or state of the animation\nto : end position or state of the animation\nrepeat : if and how the animation should be repeated\n\nPosition\nWe will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.\nOur code should look like this :\n<a-animation attribute=\"position\"\r\n    dur=\"1000\"\r\n    from=\"1 0 0\"\r\n    to=\"0 0 1\">\r\n</a-animation>\r\n\nRotation\nRotations work the same, except that instead of a vector you have to indicate three angles :\n<a-animation attribute=\"rotation\"\r\n    dur=\"2000\"\r\n    from=\"0 0 0\"\r\n    to=\"360 0 0\"\r\n    repeat=\"indefinite\">\r\n</a-animation>\r\n\nYou can either make your entity rotate on itself:\n\nYou can access the full code here.\nOr make it rotates around an object:\n\nAnd the full code is here.\nOthers properties\nThis animation pattern works on a lot of others properties, for example :\n\nScale :\n\n<a-animation\r\n    attribute=\"scale\"\r\n    to=\"2 2 3\">\r\n</a-animation>\r\n\n\nColor :\n\n<a-animation\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nTrigger\na-animation has a trigger : begin. That can either be used with a delay or an event. For example :\n<a-animation\r\n    begin=\"3000\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nThis animation will start in 3000ms.\nOtherwise with an event you can use :\n<a-entity id=\"entity\">\r\n<a-animation\r\n    begin=\"colorChange\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nWith the event emission :\ndocument.querySelector('#entity').emit('colorChange');\r\n\nCombining\nYou can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.\nFirst case is very simple you only have to add multiple entities with an animation for each.\nSecond one is more difficult because you must add an entity inside an entity, like in this example :\n<a-entity>\r\n    <a-animation attribute=\"rotation\"\r\n      dur=\"2000\"\r\n      easing=\"linear\"\r\n      from=\"0 0 0\"\r\n      to=\"0 360 0\"\r\n      repeat=\"indefinite\"></a-animation>\r\n          <a-entity rotation=\"0 0 25\">\r\n              <a-sphere position=\"2 0 2\"></a-sphere>\r\n          </a-entity>\r\n</a-entity>\r\n\nThe first entity is used as the axis for our rotating animation.\n3 \u2013 Model loading\nType of models\nYou can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.\nYou can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/\nYou can also use an online library to download models, some of them even allow you to directly use their models.\nLoading a model from the internet\nDuring the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models\nWe will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.\nTo load your 3d model, we will use the following piece of code :\n<a-assets>\r\n      <a-asset-item id=\"smiley\" src=\"https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf\"></a-asset-item>\r\n</a-assets>\r\n\nYou can of course load your very own model the same way.\na-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.\nThe full example look like this.\nDisplay your model\nNow that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :\n<a-entity gltf-model=\"#smiley\" rotation= \"180 0 0\">\r\n</a-entity>\r\n\nRemember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.\nAnd here is a demo :\n\n4 \u2013 Markers\nFor now we only have used Hiro marker, now we will see how we can use other kind of marker.\nBarcode\nIn ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :\n<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>\r\n\nAs you can guess, our value will be found in a 5\u00d75 matrix. And now the a-marker component will look like :\n<a-marker type='barcode' value='5'></a-marker>\r\n\nCustom marker\nYou can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.\nYou can then download the .patt file and you can use it in your application, like this:\n<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>\r\n\nMultiple markers\n<script src=\"https://aframe.io/releases/0.6.0/aframe.min.js\"></script>\r\n<script src=\"https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js\"></script>\r\n<body style='margin : 0px; overflow: hidden;'>\r\n  <a-scene embedded arjs='sourceType: webcam;'>\r\n\r\n    <a-marker type='pattern' url='path/to/pattern-marker.patt'>\r\n      <a-box position='0 0.5 0' material='color: red;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker preset='hiro'>\r\n      <a-box position='0 0.5 0' material='color: green;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker type='barcode' value='5'>\r\n      <a-box position='0 0.5 0' material='color: blue;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-entity camera></a-entity>\r\n  </a-scene>\r\n</body>\r\n\nWhat\u2019s next\nYou now have everything you need to start and deploy a basic AR web application using ARjs.\nIf you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBastien Teissier\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. \nThe principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.\nBasically, here is an example of my git history before and after I used this technique.\n\nStay focus, rebase and merge are no joke! \nWhat is the goal of a rebase or a merge ?\nRebase and merge both aim at integrating changes that happened on another branch into your branch.\nWhat happens during a merge ?\nFirst of all there are two types of merge:\n\nFast-forward merge\n3-way merge\n\nFast-forward merge\nA fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.\nThe following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nAs you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.\nNotes:\n\ngit checkout A, git rebase B you would have had the exact same result!\ngit checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.\n\n3-way merge\nA 3-way merge happens when both branches have had new commits since the last shared commit.\nThe following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nDuring a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:\n\nAll the modifications brought by the three commits from B (in purple)\nThe possible conflict resolutions\n\nGit will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.\nThe default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.\nWhat happens during a rebase?\nA rebase differ from a merge in the way in which it integrates the modifications.\nThe following drawings show what happens during a rebase and how it is shown in a graphical git software.\nA: the branch that you are rebasing\nB: the branch from which you get the new commits\n\ngit checkout A\ngit rebase B\n\n\n\nWhen you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.\nFor each commit to apply, if there are conflicts, they will be resolved inside of the commit.\nAfter a rebase, the new commits from A (in blue) are not exactly the same as they were:\n\nIf there were conflicts, those conflicts are integrated in each commit\nThey have a new hash\n\nBut they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.\nWhat is the best solution to integrate a new feature into a shared branch and keep your git tree clean?\nLet say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.\nYou have two main solutions: \nFirst solution: \n\ngit checkout feature\ngit rebase master\ngit checkout master\ngit merge feature\n\n\nNote : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only\nSecond solution:\n\ngit checkout master\ngit merge feature\n\n\nAs you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened.\nIn this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!\nUnfortunately, the first solution has a few drawbacks:\nHistory rewriting\nWhen you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.\nThis can potentially erase changes another developer made, or introduce conflicts resolution for him.\nTo avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.\nHowever you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.\nThe obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all.\nConflicts resolution\nWhen you merge or rebase, you might have to resolve conflicts.\nWhat I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch.\nThe only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.\nConclusion\nTo conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9mie Marniquet Fabre\r\n  \t\t\t\r\n  \t\t\t\tI am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tStop wasting your time on tasks your CI could do for you.\nFind 4 tips on how to better use your CI in order to focus on what matters \u2013 and what you love: code. Let\u2019s face it: as a developer, a huge part of the value you create is your code.\nNote: Some of these tips use the GitHub / CircleCI combo. Don\u2019t leave yet if you use BitBucket or Jenkins! I use GitHub and CircleCi on my personal and work-related projects, so they are the tools I know best. But most of those tips could be set up with every CI on the market.\nTip 1: Automatic Changelogs\nI used to work on a library of React reusable components, like Material UI. Several teams were using components from our library, and with our regular updates, we were wasting a lot of time writing changelogs. We decided to use Conventional Commits. Conventional Commits is a fancy name for commits with a standardized name:\nexample of Conventional Commits\nThe standard format is \u201cTYPE(SCOPE): DESCRIPTION OF THE CHANGES\u201d. \nTYPE can be\n\nfeat: a new feature on your project\nfix: a bugfix\ndocs: update documentation / Readme\nrefactor: a code change that neither fixes a bug nor adds a feature\nor others\u2026\n\nSCOPE (optional parameter) describes what part of your codebase is changed within the commit.\nDESCRIPTION OF THE CHANGES is pretty much what you would write in a \u201ctraditional\u201d commit message. However, you can use keywords in your commit message to add more information. For instance:\nfix(SomeButton): disable by default to fix IE7 behaviour\r\nBREAKING CHANGE: prop `isDisabled` is now mandatory\nWhy is this useful? Three main reasons:\n\nAllow scripts to parse the commit names, and generate changelogs with them\nHelp developers thinking about the impact of their changes (Does my feature add a Breaking Change?)\nAllow scripts to choose the correct version bump for your project, depending on \u201chow big\u201d the changes in a commit are (bugfix: x.y.Z, feature: x.Y.z, breaking change: X.y.z)\n\nThis standard version bump calculation is called Semantic Versioning. Depending of the version bump, you can anticipate the impact on your app and the amount of work needed.\nBe careful though! Not everyone follows this standard, and even those who do can miss a breaking change! You should never update your dependencies without testing everything is fine \ud83d\ude09\nHow to set up Conventional Commits\n\nInstall Commitizen\nInstall Semantic Releases\nAdd GITHUB_TOKEN and NPM_TOKEN to the environment variables of your CI\nAdd `npx semantic-release` after the bundle & tests steps on your CI master/production build\nUse `git cz` instead of `git commit` to get used to the commit message standard\nSquash & merge your feature branch on master/production branch\n\nWhen you get used to the commit message standard, you can go back to `git commit`, but remember the format! (e.g: `git commit -m \u201cfeat: add an awesome feature\u201d`)\nNow, every developer working on your codebase will create changelogs without even noticing it. Plus, if your project is used by others, they only need a glance at your package version/changelog to know what changes you\u2019ve made, and if they are Breaking.\nTip 2a: Run parallel tasks on your CI\nWhy do I say task instead of tests? Because a CI can do a lot more than run tests! You can:\n\nGenerate automatic changelogs \ud83d\ude09 and version your project\nBuild and push the bundle on a release branch\nDeploy your app\nDeploy your documentation site\n\nThere are several ways to use parallelism to run your tasks.\nThe blunt approach\nThis simply consists of using the built-in parallelism of your tasks, combined with a multi-thread CI container.\nWith Jest, you can choose the number of workers (threads) to use for your test with the `\u2013max-workers` flag.\nWith Pytest, try xdist and the `-n` flag to split your tests on multiple CPUs.\nAnother way of parallelizing tests is by splitting the test files between your CI containers, as React tries to do it. However, I won\u2019t write about this approach in this article since the correct way of doing it is nicely explained in the CircleCi docs.\n\u00a0\nTip 2b: CircleCI Workflows\nWith Workflows, we reduced our CI Build time by 25% on feature branches (from 11\u2033 to 8\u203330) and by 30% on our master branch (from 16\u203330 to 11\u203330). With an average of 7 features merged on master a day, this is 1 hour and 30 minutes less waiting every day for our team.\nWorkflow is a feature of CircleCI. Group your tasks in Jobs, then order your Jobs how it suits your project best. Let\u2019s imagine you are building a library of re-usable React Components (huh, I think I\u2019ve already read that somewhere\u2026). Your CI:\n\nSets up your project (maybe spawn a docker, install your dependencies, build your app)\nRuns unit/integration tests\nRuns E2E tests\nDeploys your Storybook\nPublishes your library\n\nEach of those bullet points can be a Job: it may have several tasks in it, but all serve the same purpose. But do you need to wait for your unit tests to pass before launching your E2E tests? Those two jobs are independent and could be running on two different machines.\nOur CircleCI workflow\nExtract of our config.yml\nAs you can see, it is pretty straight-forward to re-order or add dependencies between steps. For more info on how to setup Workflows, check out the documentation.\nThis is also useful for cross-platform testing (you can take a look at Yarn\u2019s workflows).\nNote: Having trouble setting up a workflow? You can SSH on the machine during the build.\n\u00a0\nParallelization drawbacks\nBut be careful with the parallelism: resources are not unlimited; if you share your CI plan with other teams in your organization, make sure using more resources for parallelism will not be counter-productive at a larger scale. You can easily understand why using 2 machines for 10 minutes can be worse than using 1 machine for 15 minutes:\n\u00a0\nProject #2 is queued on CI because there is no machine free when the build was triggered\n\u00a0\nPlus, sharing the Workspace (the current state) of one machine to others (e.g: after running `yarn`, to make your dependencies installed for every job) costs time (both when saving the state on the first machine and loading it on the other).\nSo, when should I parallelize my CI tasks?\nA good rule of thumb is always keeping jobDuration > (nb_containers * workspaceSharingDuration).\nWorkspace sharing can take up to a minute for a large codebase. You should try several workflow configurations to find what\u2019s best for you.\n\u00a0\nTip 3: Set up cron(tab)s\nCrontabs help make your CI more reliable without making builds longer.\n\nWant to run in-depth performance tests that need to send requests to your app? Schedule it for night time with a cron!\nWant to publish a new version of your app every week? Cron.\nWant to train your ML model but it takes hours? Your CI could trigger the training every night.\n\nSome of you may wonder: what is a cron/crontab? Cron(tab) is an abbreviation of ChronoTable, a job scheduler. A cron is a program that executes a series of instructions at a given time. It can be once an hour, once a day, once a year\u2026\nI worked on a project in finance linking several sources of data and API\u2019s. Regression was the biggest fear of our client. If you give a user outdated or incorrect info, global financial regulators could issue you a huge fine. Therefore, I built a tool to generate requests with randomized parameters (country, user profile\u2026), play them, and check for regressions. The whole process can take an hour. We run it via our CI, daily, at night, and it saved the client a lot of trouble.\nYou can easily set up crons on CircleCi if you\u2019ve already tried Jobs/Workflows. Check out the documentation.\nNote: Crons use the POSIX date-time notation, which can be a bit tricky at first. Check out this neat Crontab Tester tool to get used to it!\n\u00a0\nMisc tips:\n\nLearn Shell! All Continuous Integration / Continuous Delivery systems can run Shell scripts. Don\u2019t be afraid to sparkle some scripts in your build! Add quick checks between/during tasks to make debugging easier, or make your build fail faster: you don\u2019t want to wait for the full 10 minutes when you can check at 2\u201930 that your lockfile is not up-to-date!\nUse cache on your project dependencies!\nAdd extra short task to your CI to connect useful tools like Codecov.io or Danger\n\n\u00a0\nIf you have any other tip you would like to share, don\u2019t hesitate!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAur\u00e9lien Le Masson\r\n  \t\t\t\r\n  \t\t\t\tDeveloper @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a quick guide on how to set up the debugger in VS code server-side for use with Node.js in a Docker container. I recently worked on a project which uses the Koa.js framework as the API. Whilst trying to set up the debugger with VS code, a google search led to several articles that had conflicting information about how to set it up and the port number to expose, or was overly verbose and complicated.\nTo keep things simple, I have split this into 3 steps.\n1) Check version of Node.js on\u00a0server\nTo do this with docker-compose set up, use the following, replace [api] with the name of your\u00a0docker container.\ndocker-compose exec api\u00a0node --version\nInspector\u00a0Protocol\u00a0(Node V7+, since Oct 2016)\nRecent versions of Node.js now uses the inspector protocol. This is easier to set up and is the default setting for new Node.js applications, as most documentation will refer to this protocol. This means that:\n\nThe --inspect flag is required when starting the node process.\nBy default, the port 9229 is exposed, and is equivalent to --inspect:9229\nThe port can be changed, eg. --inspect-brk:1234 . Here, the \u2018-brk\u2019 flag adds a breakpoint on start.\n\nLegacy\u00a0Protocol (Node V6 and earlier)\nOlder versions of Node.js (prior to V7) uses the \u2018Legacy Debugger\u2019. The version of Node.js used on my project was 6.14. This means that:\n\nThe\u00a0--debug\u00a0flag is required when starting the node process.\nBy default, the port 5858 is exposed, and is equivalent to\u00a0--debug:5858\nThis port cannot be changed.\n\nFor more information goto:\nhttps://code.visualstudio.com/docs/nodejs/nodejs-debugging\nhttps://nodejs.org/en/docs/guides/debugging-getting-started/\n2) Expose port in Node and Docker\nIn \u2018package.json\u2019, add\u00a0--debug:5858\u00a0 (or\u00a0--inspect:9229\u00a0depending on Node version) when starting Node, so:\n\"dev\": \"nodemon index.js\",\u00a0becomes\n\"debug\": \"nodemon --debug:5858 index.js\",\nIn \u2018docker-compose.yml\u2019, run the debug node command\u00a0and expose the port. In my case:\napi:\nbuild: ./api\ncommand: yarn dev\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\nbecomes:\napi:\nbuild: ./api\ncommand: yarn debug\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\n- \"5858:5858\"\n3)\u00a0Set launch configuration of Debugger\nIn \u2018/.vscode/launch.json\u2019, my launch configuration is:\n{\n\"type\": \"node\",\n\"request\": \"attach\",\n\"name\": \"Docker: Attach to Node\",\n\"port\": 5858,\n\"address\": \"localhost\",\n\"localRoot\": \"${workspaceFolder}/api/\",\n \"remoteRoot\": \"/code/\",\n\"protocol\": \"legacy\"\n}\nThe port and protocol needs to correspond to the version of Node used as determine above. For newer versions of Node:\u00a0\"port\": \"9229\" and \"protocol\": \"inspector\" should be used.\n\u201clocalRoot\u201d and \u201cremoteRoot\u201d should be set to the folder corresponding to the entry point (eg. index.js) of your Node application in the local repository and the docker folder respectively.\n4) Attach debugger and go!\nIn VS code, set your breakpoints and press F5 or click the green triangle button to start debugging! By default VS code comes with a debug panel to the left and debug console to the bottom, and a moveable debug toolbar. Mousing over a variable shows its values if it has any.\n\n\u00a0\nI hope this article has been useful, and thanks for reading my tech blog!\u00a0 \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHo-Wan To\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUsually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.\nThe problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page!\nLooking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.\nWhy SSR (Server-Side Rendering)?\nThis is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:\n\nImprove your SEO\nSpeed up your first page load\nAvoid blank page flicker before rendering\n\nIf you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.\nBut let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here.\nWhat\u2019s the plan?\nFor this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings.\nThis article is split in 3 sections matching 3 server-side-rendering strategies:\n\nHow tomanually upgrade your React app to get\u00a0SSR\nHow to start with Next.js from scratch\nMigrate your existing React app to server-side with Next.js\n\nI won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon).\n1) Look how twisted manual SSR is\u2026\n\nIn this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:\n\npackage.json\u00a0for dependencies\nWebpack configuration included\nApp.js\u00a0\u2013 loads React and renders the Hello component\nindex.js\u00a0\u2013 puts all together into a root component\n\nChecking rendering type\nI just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window:\nconst isClientOrServer = () => {\r\n  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';\r\n};\r\n\nso that we display on the page what is rendering the application: server or client.\nTest it by yourself\n\nclone\u00a0this repository\ncheckout the initial commit\ninstall the dependencies with\u00a0yarn\nlaunch the dev server with\u00a0yarn start\nbrowse to\u00a0http://localhost:3000\u00a0to view the app\n\nI am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:\n\nImplementing\u00a0Server-side Rendering\nLet\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps.\nWe first need a node server using Express:\u00a0yarn add express.\nIn our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer.\n// use port 3001 because 3000 is used to serve our React app build\r\nconst PORT = 3001; const path = require('path');\r\n\r\n// initialize the application and create the routes\r\nconst app = express();\r\nconst router = express.Router();\r\n\r\n// root (/) should always serve our server rendered page\r\nrouter.use('^/$', serverRenderer);\r\n\nTo render our html, we use a server renderer that is replacing the root component with the built html:\n// index.html file created by create-react-app build tool\r\nconst filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');\r\n\r\nfs.readFile(filePath, 'utf8', (err, htmlData) => {\r\n  // render the app as a string\r\n  const html = ReactDOMServer.renderToString(<App />);\r\n\r\n  // inject the rendered app into our html\r\n  return res.send(\r\n    htmlData.replace(\r\n      '<div id=\"root\"></div>',\r\n      `<div id=\"root\">${html}</div>`\r\n    )\r\n  );\r\n}\r\n\nThis is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string.\nWe finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.\nrequire('babel-register')({\r\n  ignore: [ /(node_modules)/ ],\r\n  presets: ['es2015', 'react-app']\r\n});\r\n\nTest it by yourself\n\ncheckout last changes on master branch\ninstall the dependencies with\u00a0yarn\nbuild the application with\u00a0yarn build\ndeclare babel environment in your terminal:\u00a0export BABEL_ENV=development\nlaunch your node server with\u00a0node server/bootstrap.js\nbrowse to\u00a0http://localhost:3001\u00a0to view the app\n\nStill simulating the \u20183G network\u2019 in Chrome, here is the result:\n\nDo not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client.\nWe proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?!\nWe\u2019re missing many features\nOur example is a good proof of concept but very limited. We would like to see more features like:\n\nimport images in js files (logo problem)\nseveral routes usage or route management (check\u00a0this article)\ndeal with the\u00a0</head>\u00a0and the metatags (for SEO improvements)\ncode splitting (here is\u00a0an article\u00a0solving the problem)\nmanage the state of our app or use Redux (check this\u00a0great article\n\nand performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website.\nIt is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations.\nSeriously\u2026 Next.js can bring you all these features!\n2) Next.js helps you building server rendered React.js Application\n\nWhat is Next.js?\nNext.js is a minimalistic framework for server-rendered React applications with:\n\na very simple page based routing\nWebpack hot reloading\nautomatic transpilation (with babel)\ndeployment facilities\nautomatic code splitting (loads page faster)\nbuilt in css support\nability to run server-side actions\nsimple integration with Redux using next-redux-wrapper.\n\nGet started in 1 minute\nIn this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.\nFirst, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this:\n\"scripts\": {\r\n  \"dev\": \"next\",\r\n  \"build\": \"next build\",\r\n  \"start\": \"next start\"\r\n}\r\n\nCreate a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function):\nconst Index = ({ title = 'Hello from Next.js' }) => (\r\n  <div>\r\n    <h1>{title}</h1>\r\n    <p className=\"App-intro\">\r\n      Is my application rendered by server or client?\r\n    </p>\r\n    <h2><code>{isClientOrServer()}</code></h2>\r\n  </div>\r\n);\r\n\r\nexport default Index;\r\n\nNo need to import any library at the top of our index.js file, Next.js already knows that we are using React.\nNow enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!\n\nRepeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.\nYou\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository.\nUse create-next-app\nYou want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app:\nnpm install -g create-next-app\r\n\r\ncreate-next-app my-app\r\ncd my-app/\r\nnpm run dev\r\n\nThis is all you need to do to create a React app with server-side rendering thanks to Next.js.\nFinally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.\nVue.js and Nuxt\nYou\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:\n vue init nuxt-community/starter-template <project-name> \nWhat\u2019s Next? \nHope you liked this article which was mainly written in order to introduce server-side rendering with Next.\nIf you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBaptiste Jan\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @Theodo. I like Vue.js and all the ecosystem growing around.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHow to recode Big Brother in 15 min on your couch\nFace Recognition Explained\nIn this article, we will step by step implement a smart surveillance system, able to recognise people in a video stream and tell you who they are. \nMore seriously, we\u2019ll see how we can recognise in real-time known faces that appear in front of a camera, by having a database of portraits containing those faces.\nFirst, we\u2019ll start by identifying the different essential features that we\u2019ll need to implement. To do that,\u00a0we\u2019ll analyse the way we would to that, as human beings (to all the robots that are reading those words, I hope I don\u2019t hurt your feelings too much and I truly apologize for the methodology of this article).\nAsk yourself : if someone passes just in front of you, how would you recognise him ?\n\nYou\u2019ll need first to see the person\nYou then need to focus on the face itself\nThen there are actually two possibilities.\n\nEither I know this individual and I can recognise him by comparing his face with every face I know.\nOr I don\u2019t know him\n\n\n\nLet\u2019s see now how to the algo will do those different steps.\nFirst step of the process : seeing the person\nThis is quite a simple step. We\u2019ll simply need a computer and a webcam, to capture the video stream. \nWe\u2019ll use openCV Python. With a few lines of code, we\u2019ll be able to capture the video stream, and dispose of the frame one by one.\nimport cv2\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   frame = video_capture.read()\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nHow to detect a face in a picture ?\nTo be able to find a face in the picture, let\u2019s ask ourselves, what is a face and how can we discriminate a face from Gmail\u2019s logo for example ?\n\n\nWe actually do it all the time without even thinking about it. But how can we know that easily that all these pictures are faces ?\n\nWhen we look at those different pictures, photographs and drawings, we see that a face is actually made of certain common elements : \n\nA nose\nTwo eyes\nA mouth \nEars\n\u2026\n\nBut not only are the presence of these elements essential, but their positions is also paramount. \nIndeed, in the two pictures here, you\u2019ll find all the elements that you\u2019ll find in a face. Except one is a face, and one is not.\n\nSo, now that we\u2019ve seen that a face is characterised by certain criterias, we\u2019ll turn them into simple yes-no questions, which will be very useful to find a face in a square image.\nAs a matter of fact, the question \u201cIs there a face in a picture ?\u201d is very complex. However, we\u2019ll be able to approximate it quite well by asking one after the other a series of simple question : \u201cis there a nose ?\u201d ; \u201cIs there an eye ? If yes, is their two eyes ?\u201d ; \u201cAre there ears ?\u201d ; \u201cIs there some form of symmetry ?\u201d. \nAll these questions are both far more simple than the complex question \u201cIs there a face in the picture ?\u201d, while providing us with information to know if part of the image is or is not a face. \nFor each one of these questions, a no answer is very strong and will tell us that there is definitely no face in the picture. \nOn the contrary, a yes answer will not allow us to be sure that there is a human face, but it will slightly increase the probability of the presence of a face.\u00a0If the image is not a face, but it is tree, the answer to the first question \u201cis there a nose ?\u201d will certainly be negative. No need then to ask if there are eyes, or if there is some form of symmetry.\nHowever, if indeed there is a nose, we can go forward and ask \u201care there ears?\u201d. If the answer is still yes, this won\u2019t mean that there is a face, but will slightly increase the likeliness of this possibility, and we will keep digging until being sufficiently confident to say that there is a face indeed.\nThe interest is that the simplicity of the questions will reduce drastically the cost of face detection, and allow to do real-time face detection on a video stream. \nThis is the principle of a detection method called \u201cthe cascade of weak classifier\u201d. Every classifier is very weak considering that it gives only a very little degree of certitude. But if we do the checks one by one, and a region of the picture has them all, we\u2019ll be at the end almost sure that a face is present here. \nThat\u2019s why it is called a cascade classifier, because like a series of waterfalls, the algorithm will simply do a very simple and quick check, one by one, and will only move forward with another check if the first one is positive. \nTo do face detection on the whole picture, and considering that we don\u2019t know in advance the size of the face, we\u2019ll simply apply the cascade algorithm on a moving window for every frame.\n\nWhat we\u2019ve explained here is the principle of the algorithm. Lots of research has been made about how to use cascade for object detection. OpenCV has a built-in way to do face detection with a cascade classifier, by using a set of 6,000 weak classifiers especially developed to do face detection.\nimport cv2\r\n\r\nopencv_path = 'm33/lib/python3.7/site-packages/cv2/data/'\r\nface_cascade = cv2.CascadeClassifier(opencv_path + 'haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    # Capture frame-by-frame\r\n    ret, frame = video_capture.read()\r\n\r\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n \r\n    # Draw a rectangle around the faces\r\n    for (x, y, w, h) in faces:\r\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n    # Display the resulting frame\r\n    cv2.imshow('Video', frame)\r\n\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\nNow that we can detect the face, we need to recognise it among other known faces. Here is what we got :\u00a0\nFace recognition\nNow that we have a system able to detect a face, we need to make sense out of it and recognise the face.\nBy applying the same methodology as before, we\u2019ll find the criterias to recognise a face among others.\nTo do that, let\u2019s look at how we differentiate between two faces : Harry Potter on one side, and Aragorn on the second.\n\nLet\u2019s make a list of the things that can differentiate them : \n\nForm of their nose\nForm of their eyes\nTheir hair\nColor of their eyes\nDistance between the eyes\nSkin color\nBeard\nHeight of the face\nWidth of the face\nRatio of height to width\n\nOf course, this list is not exhaustive. However, are all those criterias good for face recognition ? \nSkin color is a bad one for example. We\u2019ve all seen Harry Potter or Aragorn after hard battles covered with dirt or mud, and we\u2019re still able to recognise them easily.\nSame goes for height and width of the face. Indeed, these measures change a lot with the distance of the person to the camera. Despite that we can easily recognise the faces even when their size changes. \nSo we can keep some good criterias that will really help recognise a face : \n\nForm of their nose\nForm of their eyes\nDistance between the eyes\nRatio of height to width \nPosition of the nose relative to the whole face\nForm of eyebrows\n\u2026\n\nLet\u2019s now measure all these elements. By doing this, we\u2019ll have a set of values that describe the face of an individual. These measures are a discrete description of what the face looks like.\n\nActually, what we have done, is that we reduced the face to a limited number of \u201cfeatures\u201d that will give us valuable and comparable information of the given face.\n\nMathematically speaking, we have simply created a vector space projection, that allowed us to reduce the number of dimensions in our problem. From a million-dimensions vector space problems (if the picture is 1MPixel RGB image, the vector space is of 1M * 3 dimensions) to a an approximately a-hundred-dimension vector space. The problem becomes far more simple ! \nNo need to consider all the pixels in the picture at all, we only need to extract from the image a limited set of features. These extracted features can be considered as vectors that we can then compare the way we do it with any vector by computing euclidean distances for example.\n\nAnd just like that, comparing faces becomes mathematically as simple as computing the distance between two points on a grid, with only a few more dimensions ! To be simple, it\u2019s as though, every portrait can then be described as a point in space. The closer points are, the more likely they describe the same face ! And that\u2019s all !\nWhen we find a face in a frame, we find its position in the feature-space and we look for the nearer known point. If the distance between the two is close, we\u2019ll consider that they\u2019re both linked to the same face. Otherwise, if the point representing the new face is too far from all the faces known, it means we don\u2019t know this face.\n\nTo implement that, we\u2019ll use the face_recognition Python library that allows us to use a deep learning algorithm that extracts from a face a 128-dimension vector of features. \nWe\u2019ll do it in two steps.\nWe first turn our portrait database into a set of computed feature-vectors (reference points like the Frodo point in the example above). \nimport face_recognition\r\nimport os\r\nimport pandas as pd\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n\t// happens when no face is detected\r\n       Continue\r\n\r\nfeatures_df = pd.DataFrame(face_features, names)\r\nfeatures_df.to_csv('database.csv')\r\n\r\n\nThen, we load the database and launch the real-time face recognition:\nimport cv2\r\nimport pandas as pd\r\nfrom helpers import load_database\r\nimport PIL\r\nimport numpy as np\r\nimport face_recognition\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n       # happens when no face is detected\r\n       Continue\r\n\r\nface_cascade= cv2.CascadeClassifier('m33/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   ret, frame = video_capture.read()\r\n   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n   faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n  \r\n   # Draw a rectangle around the faces\r\n   for (x, y, w, h) in faces:\r\n       cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n       pil_im = PIL.Image.fromarray(frame[y:y+h, x:x+w])\r\n       face = np.array(pil_im.convert('RGB'))\r\n       try:\r\n           face_descriptor = face_recognition.face_encodings(face)[0]\r\n       except Exception:\r\n           continue\r\n       distances = np.linalg.norm(face_descriptors - face_descriptor, axis=1)\r\n       if(np.min(distances) < 0.7): found_name = names[np.argmin(distances)] print(found_name) print(found_name) #y = top - 15 if top - 15 > 15 else top + 15\r\n       cv2.putText(frame, found_name, (y, y-15), cv2.FONT_HERSHEY_SIMPLEX,\r\n                   0.75, (0, 255, 0), 2)\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nAnd here it comes ! \n\nHere is a github repo with the code working : https://github.com/oussj/big_brother_for_dummies\nExternal links I used : \n\nhttps://github.com/ageitgey/face_recognition\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78\nhttps://realpython.com/face-recognition-with-python/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tOussamah Jaber\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from MINES ParisTech, I joined Theodo as an agile web developer to use cutting-edge technology and build awesome products !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I started web development, the developer tools were so new to me I thought I would save time not using them in the first place. I quickly realized how wrong I was as I started using them. No more console.log required, debugging became a piece of cake in a lot of cases!\nThe Network tab is used to track all the interactions between your front end and your back end, thus the network.\nIn this article, I will show usages of the developer tools Network tab on Google Chrome web browser.\nLet\u2019s start exploring the network tab!\nRecord the network logs\nThe first thing to do is to record the network logs by making sure the record button is activated before going on the page you want to record: \n\nCheck the response of a request sent to your server\nYou can keep only the requests sent to your server by clicking on the filter icon and then selecting \u201cXHR\u201d:\n\nIn that section, you can see some information about your requests:\n\nHTTP status code\nType of request\nSize of the request\nEtc.\n\nTo get more details about a request, you can simply click on it.\nLet\u2019s look at the my-shortcuts endpoint that retrieves the shortcuts of an user connected on the application I am using. You can look at the formatted response by clicking on the \u201cPreview\u201d tab: \n\nIf the response of an XHR is an image, you will be able to preview the image instead.\nOn this tab, it becomes easy to determine if the format of the response corresponds to what your front end expected.\nGreat! Now you know how to check the response of a request sent to your server without writing any console.log in your code!\nTest your requests with various Internet connection speeds\nIf the users of the application you are developing have a lower Internet speed than yours, it can be interesting to test your application with custom Internet speed.\nIt is possible to do so by using bandwidth throttling by clicking on the following dropdown menu: \u00a0\n\n\nReplay a request\nReplaying a request can be useful if you want to see how the front end interacts with the response of a request again or if you need to test your request with different parameters. It can be long and painful to reload the page and reproduce exactly the same actions over and over again. Here are some better ways to replay a request:\n\n When right-clicking on a request, you can copy the cURL format of your request and paste it in the terminal of your computer to send the request to your back end:\n\n\n\n When right-clicking on a request, you can copy the request headers and paste them in your favorite API development environment (e.g. Postman):\n\n\nIn Postman, click on \u201cHeaders tab\u201d > \u201cBulk Edit\u201d to edit the headers:\n\nNow all you need to do is paste your headers. Don\u2019t forget to comment the path of the request which is not a header: \u00a0\n\n\n\n If you are using \u201cXHR\u201d requests, you can simply right-click on the request you want to replay and click on \u201cReplay XHR: \n\n\nI hope that I could help you debug your network interactions with this article!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJordan Lao\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"}
][
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA few weeks ago, Nicolas and I launched https://jamstack.paris, a JAMstack website powered by GatsbyJS and hosted on Netlify.\nJAMstack applications\u00a0deliver static websites to end-users with the benefit of high performance and CDN ease and still allow dynamism by sourcing content or data during build time.\n\nOn https://jamstack.paris for example, we collect the number of attendees for our events using Gatsby Source Meetup plugin and this happens on build time.\nThe drawback of this dynamism-on-build approach is that our website may be not synced with real count of attendees until we build the site again.\n\nSo, we wanted a quick way to build the site on Netlify with a simple button on our smartphones, and\u00a0you can have the same setup in less than five minutes, let\u2019s go \ud83d\ude80.\nClaim your build hook on Netlify\nHead on Settings > Build & Deploy > Build hooks\u00a0section on your Netlify dashboard and hit the \u201cAdd build hook\u201d button.\n\nGive an explicit name to your hook, for example\u00a0Smartphone Deploy and copy the curl request Netlify provides to you.\n\nSend\u00a0the build request from your phone\nNow we want to sent this build request from our phone.\nYou are on android \ud83d\udc7e\nHTTP Request Shortcuts\u00a0application is doing exactly that\u00a0and requires no permissions on device, which is good in terms of privacy.\n\n\nYou are on iOS \ud83c\udf4f\nThe native Shortcuts application does the job. Head to the app and create a new shortcut. You will need two actions:\n\nURL\nGet Contents of URL\n\nPaste the URL from Netlify (without curl) in the field of the first action.\nThe second action is also really easy to setup. Just open the Advanced options and change the Method to POST.\nHurray! Now we can deploy from our\u00a0smartphone\u2019s homescreen \ud83d\ude80\nIf you have any suggestions or questions, feel free to add them in the comments. And if you want to speak about JAMstack in Paris, we\u2019d love to get in touch with you. First meetup gathering in Paris on December 18th\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMatthieu Auger\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tKotlin is an object-oriented programming language for making Android apps that uses Java-like syntax with functional programming features. It was created by Jetbrains, the makers of hugely popular IDEs like IntelliJ and PyCharm, and is used by big companies such as Pinterest, Uber, and Atlassian.\nAt Theodo, React Native is the language of choice for building apps. And for good reason. It has a rich ecosystem of libraries to use, hot reloading out of the box, is very similar to React which is a hugely popular web framework, and vitally it allows you to compile your apps to both Android and iOS from just one codebase. But this doesn\u2019t mean that we should necessarily use React Native in every case. I will talk through some examples of why Kotlin might be a better choice in some cases.\nWhy use Kotlin/Java over React Native?\nThere are many reasons why one may choose to write native code for an app rather than React Native. One of the most common I have seen is API support. Many services that have APIs for apps cannot be used fully within React Native. This leads to developers having to write native code to get the functionality they need. This can be tricky to developers who may not have ever written native code. Kotlin, on the other hand, is strongly supported within the Android ecosystem and is interoperable with Java so it has easier access to these APIs. Any API that supports Java 8 can be easily used with Kotlin.\nAnother big reason to prefer native code over React Native is performance. For advanced functionality or heavy computation, the overhead of using React Native can considerably slow down an app. Kotlin, on the other hand, appears and acts totally natively without lag.\nMany of us have suffered through the pain of having to update React or React Native. It can be a long and arduous task. However, with writing Kotlin, each new version is backwards compatible with the last. That means that even legacy code written by previous teams doesn\u2019t have to be rewritten when updating to a new version of Kotlin.\nA big problem with large javascript projects is the errors that can come up due to the lack of typing. This is why many developers choose to integrate Flow or TypeScript to help with this. However, this can be a difficult task, especially if the codebase is already large before you start. Kotlin however is statically typed meaning that there is no need for tools like these to check for type errors.\nAnother benefit of using Kotlin over React Native is that it is fully compatible with Android Studio, and Intellij. This means you can make use of loads of cool features like advanced code completion, the built in debugger, and gradle integration, all while benefitting from a UI that was purpose built for making Android apps.\nWhy use Kotlin instead of Java?\nKotlin was created by JetBrains as an attempt to be a more concise and feature-rich version of Java, whilst being designed primarily for Android app development. I understand that writing native code can feel intimidating to many people who have only written Javascript or Python before, but I see Kotlin as a good stepping stone; it has the benefits of writing native code but it will feel more familiar in its functional aspects such as spread, deconstruction, and closures.\nKotlin can also be used to write iOS applications. This is a great benefit when writing applications for both iOS and Android as the application logic can be shared between the two. However, the rendering code is platform specific and thus must be rewritten for each.\nA great thing about Kotlin is that it\u2019s fully compatible with Java. This means that Java code can be inserted into it, or it can be inserted into a Java codebase. This can be helpful if you have a large codebase of legacy code written in Java which you need to add new features to.\nKotlin has all the features of Java 8, plus more. This means you get lambda functions, static functions, streams and nested classes from Java 8, on top of immutable variables, method references, methods without classes, extension methods to classes, closures, spread and deconstruction. As well as all this, built in @Nullable and @NonNull mean that there are no Null-Pointer Exceptions which comes as a great relief to those who are apprehensive to get started writing native code.\nWriting Kotlin, you can use any library that is compatible with Java 8. As well as this, Kotlin has a lot of libraries written for it such as kotlinx-coroutines-android and rxkotlin for writing asynchronous and event-based programs, just to name a few. Whilst still perhaps not as many libraries as React Native might have, there are still a huge number of libraries to take advantage of when writing Kotlin.\nDrawbacks of using Kotlin/Java over React Native\nSadly no tool is perfect and there are many reasons why you may choose to go for React Native over native code. The main reason is iOS compatibility. Yes, Kotlin code can be compiled to objective C, and is bi-directionally interoperable with Objective-C and Swift. But you still have to write separate render code for iOS and Android. This could take a long time and is the main reason why one would choose React Native.\nAnother big reason is that writing React allows hot reloading straight out of the box. While Kotlin does allow for hot reloading, it can be a pain to set up and work unreliably. Hot reloading can really speed up development and if you\u2019re used to having it you may sorely miss it if you can\u2019t get it working for Kotlin development.\nConclusion\nIf you want to get the best of both worlds, it is possible to use React Native to implement the frontend code and Kotlin to implement the backend of your application. This means that you get the speed improvements and Java-compatible API integration of Kotlin, but only have to write one codebase for both Android and iOS. As well as this, you can make use of the front-end libraries available to React Native.\nJetBrains have created a library called kotlin-wrappers which contains wrappers to use Kotlin with React, Redux, styled-components, and React Router DOM, amongst others. There are also a few other libraries online which do similar things.\nThis is a very new field and there are not many people using it just yet, so there may be some teething issues. One possible problem is that passing the store via props to the component won\u2019t work because the state will become immutable. There may be libraries to help with this, and if not then I\u2019m sure there soon will be, but this could be a big pain point.\nThis emerging combination of languages looks like a really new, interesting addition to our use of React Native here at Theodo. I hope I will on the next applicable project to test out this exciting new tech.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAbbie Howell\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tFor one of Theodo\u2019s clients, we built a complex website including a catalog, account management and the availability to order products.\nAs a result, the project was complex, with several languages (symfony, vue, javascript), utilities (docker, aws, webpack) and microservices (for the search of products, the management of accounts, the orders).\nThe impact of this complexity for the development teams was the numerous commands they had to use every day, and particularly to install the project.\nThus, and following Symfony 4 best practices, we decided to use make\u00a0on the project to solve these problems.\nAnd it worked!\nWhat is make\nMake is a build automation tool created in 1976, designed to solve dependency problems of the build process.\nIt was originally used to get compiled files from source code, for instance for C language.\nIn website development, an equivalent could be the installation of a project and its dependencies from the source code.\nLet me introduce a few concepts about make that we will need after.\nMake reads a descriptive file called Makefile, to build a target, executing some commands, and depending on a prerequisite.\nThe architecture of the Makefile is the following:\n\r\ntarget: [prerequisite]\r\n    command1\r\n    [command2]\r\n\nAnd you run make target in your terminal to execute the target\u00a0commands. Simple, right?\nUse it for project installation\nWhat is mainly used to help project installation is a README describing the several commands you need to run to get your project installed.\nWhat if all these commands were executed by running make install?\nYou would have your project working with one command, and no documentation to maintain anymore.\nI will only describe a simple way to build dependencies from your composer.json file\n\r\nvendor: composer.json\r\n    composer install\r\n\nThis snippet will build the vendor directory, running composer install, only if the vendor directory does not exist. Or if composer.json file has changed since the last time you built the vendor directory.\nNote that if you don\u2019t want to check the existency of a directory or a file named as your target, you can use a Phony Target. It means adding the line .PHONY: target to your Makefile.\nThere is much more you can do, and I won\u2019t talk about it here.\nBut if you want a nice example to convert an installation README\u00a0into a Makefile, you can have a look at these slides,\u00a0that are coming from a talk at Paris Symfony Live 2018.\nUse it for the commonly used commands you need on your project\nAfter the project installation, a complexity for the developer is to find the commands needed to develop features locally.\nWe decided to create a Makefile\u00a0to gather all the useful commands we use in the project on a daily basis.\nWhat are the advantages of this:\n\nThe commands are committed and versioned\nAll developers of the team are using the same, reviewed commands. -> there is no risk to forget one thing before executing a command line and break something\nIt is language agnostic -> which means you can start php jobs, javascript builds, docker commands, \u2026\nIt\u2019s well integrated with the OS -> for instance there is autocompletion for targets and even for options\nYou can use it for continuous improvement -> when a command fails for one dev, modify that command and commit the new version. It will never fail anymore!\n\nAuto-generate a help target\nBut after a long time, we started having a lot of commands.\nIt was painful to find the one you wanted in the file, and even to know the one that existed\nSo we added a new target help, in order to automatically generate a list of available commands from the comments in the Makefile.\nThe initial snippet we used is:\n\r\n.DEFAULT_GOAL := help\r\nhelp:\r\n    @grep -E '(^[a-zA-Z_-]+:.*?##.*$$)|(^##)' $(MAKEFILE_LIST) | awk 'BEGIN {FS = \":.*?## \"}{printf \"\\033[32m%-30s\\033[0m %s\\n\", $$1, $$2}' | sed -e 's/\\[32m##/[33m/'\r\n\nIf you add the following target and comments in your Makefile:\n\r\n## Example section\r\nexample_target: ## Description for example target\r\n        @does something\r\n\nIt would give this help message:\n\nThis generic, reusable snippet has another advantage: the documentation it generates is always up to date!\nAnd you can customize it to your need, for instance to display options associated to your commands.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tMartin Guillier\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIf you already know what scraping is, you can directly jump to how I did it\nWhat is scraping?\nScraping is the process of data mining. Also known as web data extraction, web harvesting, spying.. It is software that simulates human interaction with a web page to retrieve any wanted information (eg images, text, videos). This is done by a scraper.\nThis scraper involves making a GET request to a website and parsing the html response. The scraper then searches for the data required within the html and repeats the process until we have collected all the data we want.\nIt is useful for quickly accessing and analysing large amounts of data, which can be stored in a CSV file, or in a database depending on our needs!\nThere are many reasons to do web scraping such as lead generation and market analysis. However, when scraping websites, you must always be careful not to violate the terms and conditions of websites you are scraping, or to violate anyone\u2019s privacy. This is why it is thought to be a little controversial but can save many hours of searching through sites, and logging the data manually. All of those hours saved mean a great deal of money saved.\nThere are many different libraries that can be used for web scraping, e.g. selenium, phantomjs.\u00a0In Ruby you can also use the nokogiri gem to write your own ruby based scraper. Another popular library is is beautiful soup which is popular among python devs.\nAt Theodo, we needed to use a web scraping tool with the ability to follow links and as python developers the solution we opted for was using theDjango framework with an open source web scraping framework called\u00a0Scrapy.\nScrapy and Django\nScrapy allows us to define data structures, write data extractors, and comes with built in CSS and xpath selectors that we can use to extract the data, the scrapy shell, and built in JSON, CSV, and XML output. There is also a built in FormRequest class which allows you to mock login and is easy to use out of the box.\nWebsites tend to have countermeasures to prevent excessive requests, so Scrapy randomises the time between each request by default which can help avoid getting banned from them. Scrapy can also be used for automated testing and monitoring.\nDjango has an integrated admin which makes it easy to access the db. That along with the ease of filtering and sorting data and import/export library to allow us to export data.\nScrapy also used to have a built in class called DjangoItem which is now an easy to use external library. The DjangoItem library provides us with a class of item that uses the field defined in a Django model just by specifying which model it is related to. The class also provides a method to create and populate the Django model instance with the item data from the pipeline. This library allows us to integrate Scrapy and Django easily and means we can also have access to all the data directly in the admin!\nSo what happens?\n\u00a0\nSpiders\nLet\u2019s start from the spider. Spiders are the core of the scraper. It makes the request to our defined URLs, parses the responses, and extracts information from them to be processed in the items.\nScrapy has a start_requests method which generates a request with the URL. When Scrapy fetches a website according to the request, it will parse the response to a callback method specified in the request object. The callback method can generate an item from the response data or generate another request.\nWhat happens behind the scenes? Everytime we start a Scrapy task, we start a crawler to do it. The Spider defines how to perform the crawl (ie following links). The crawler has an engine to drive it\u2019s flow. When a crawler starts, it will get the spider from its queue, which means the crawler can have more than one spider. The next spider will then be started by the crawler and scheduled to crawl the webpage by the engine. The engine middlewares drive the flow of the crawler. The middlewares are organised in chains to process requests and responses.\nSelectors\nSelectors can be use to parse a web page to generate an item. They select parts of the html document specified either by xpath or css expressions. Xpath selects nodes in XML docs (that can also be used in HTML docs) and CSS is a language for applying styles to HTML documents. CSS selectors use the HTML classes and id tag names to select the data within the tags. Scrapy in the background using the cssselect library transforms these CSS selectors into xpath selectors.\nCSS vs Xpath\n\r\n        data = response.css(\"div.st-about-employee-pop-up\") \r\n        data = response.xpath(\"//div[@class='team-popup-wrap st-about-employee-pop-up']\")\r\n\nShort but sweet: when dealing with classes, ids and tag names, use CSS selectors. If you have no class name and just know the content of the tag use xpath selectors. Either way chrome dev tools can help: copy selector for the element\u2019s unique css selector or you can copy its xpath selector. This is to give a basis, may have to tweak it! Two more helper tools are XPath helper and this cheatsheet. Selectors are also chainable.\nItems and Pipeline\nItems produce the output. They are used to structure the data parsed by the spider. The Item Pipeline is where the data is processed once the items have been extracted from the spiders. Here we can run tasks such as validation and storing items in a database.\nHow I did it\nHere\u2019s an example of how we can integrate Scrapy and Django. Let\u2019s scrape the data off the Theodo UK Team Page and integrate it into a Django Admin Panel:\n\nGenerate Django project with integrated admin + db\nCreate a django project, with admin and database\nCreate app and add to installed apps\nDefine the data structure, so the item, so our django model.\n## models.py\r\n      from django.db import model\r\n\r\n      class TheodoTeam(models.Model):\r\n        name = models.CharField(max_length=150)\r\n        image = models.CharField(max_length=150)\r\n        fun_fact = models.TextField(blank=True)\r\n\r\n        class Meta:\r\n            verbose_name = \"theodo UK team\"\r\n      \n\nInstall Scrapy\nRun\nscrapy startproject scraper\n\nConnect using DjangoItem\n    ## items.py\r\n      from scrapy_djangoitem import DjangoItem\r\n      from theodo_team.models import TheodoTeam\r\n\r\n      class TheodoTeamItem(DjangoItem):\r\n        django_model = TheodoTeam\r\n    \n\nThe Spider \u2013 Spiders have a starturls class which takes a list of URLs. The URLs will then be used by the startrequests method to create the initial requests for your spider. Then using the response and selectors, select the data required.\nimport scrapy\r\n    from scraper.items import TheodoTeamItem\r\n\r\n    class TheodoSpider(scrapy.Spider):\r\n      name = \"theodo\"\r\n      start_urls = [\"https://www.theodo.co.uk/team\"]\r\n\r\n      # this is what start_urls does\r\n      # def start_requests(self):\r\n      #     urls = ['https://www.theodo.co.uk/team',]\r\n      #     for url in urls:\r\n      #       yield scrapy.Request(url=url, callback=self.parse)\r\n\r\n      def parse(self, response):\r\n          data = response.css(\"div.st-about-employee-pop-up\")\r\n\r\n          for line in data:\r\n              item = TheodoTeamItem()\r\n              item[\"name\"] = line.css(\"div.h3 h3::text\").extract_first()\r\n              item[\"image\"] = line.css(\"img.img-team-popup::attr(src)\").extract_first()\r\n              item[\"fun_fact\"] = line.css(\"div.p-small p::text\").extract().pop()\r\n              yield item\r\n    \n\nPipeline \u2013 use it to save the items to the database\n## pipelines.py\r\n    class TheodoTeamPipeline(object):\r\n      def process_item(self, item, spider):\r\n          item.save()\r\n          return item\r\n    \n\nCreate a Django command to run Scrapy crawl \u2013 This initialises django in the scraper and is needed to be able to access django in the spider.\n## commands/crawl.py\r\n\r\n    from django.core.management.base import BaseCommand\r\n    from scraper.spiders import TheodoSpider\r\n    from scrapy.crawler import CrawlerProcess\r\n    from scrapy.utils.project import get_project_settings\r\n\r\n    class Command(BaseCommand):\r\n      help = \"Release the spiders\"\r\n\r\n      def handle(self, *args, **options):\r\n          process = CrawlerProcess(get_project_settings())\r\n\r\n          process.crawl(TheodoSpider)\r\n          process.start()\r\n    \n\nRun manage.py crawl to save the items to the database\n\nProject Structure:\n scraper\r\n     management\r\n         commands\r\n             crawl.py\r\n     spiders\r\n         theodo_team_spider.py\r\n         apps.py\r\n         items.py\r\n         middlewares.py\r\n         pipelines.py\r\n         settings.py\r\n theodo_team\r\n     admin\r\n     migrations\r\n     models\r\n\nChallenges and problems encountered:\nSelectors!! Selectors are not one size fits all. Different selectors are needed for every website and if there is constant layout changes, they require upkeep. It can also be difficult to find all the data required without manipulating it. This occurs when tags may not have a class name or if data is not consistently stored in the same tag.\nAn example of how complicated selectors can get:\nsegments = response.css(\"tr td[rowspan]\")\r\nrowspan = int(segment.css(\"::attr(rowspan)\").extract_first())\r\n           all_td_after_segment = segment.xpath(\"./../following-sibling::tr[position()<={}]/td\".format(rowspan- 1))\r\n\r\nline = all_td_after_segment.extract_first()\r\ndata = line.xpath(\"./descendant-or-self::a/text()\")\r\nmore_data = line.xpath(\"substring-after(substring-before(./strong/text()[preceding-sibling::a], '%'), '\\xa0')\").extract_first()\r\n\nAs you can see, setting up the scraper is not the hard part! I think integrating Scrapy and Django is a desirable, efficient and speedy solution to be able to store data from a website into a database.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHenriette Brand\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tLate in 2018 AWS released Lambda Layers and custom runtime support. This means that to run unsupported runtimes you no longer need to \u2018hack\u2019 around with VMs, docker or using node.js to `exec` your binary.\nRecently I needed to\u00a0setup\u00a0a 100% serverless PHP infrastructure for a client. PHP is one option, but similar steps can allow you to run any language.\nLambda layers provide shared code between different lambda functions. For instance, if you wanted to share your vendor code between lambdas (e.g. node_modules for node.js).\nWe will create a Lambda layer to provide the PHP binary for our custom runtime API. You could also create a second to provide the vendor folder for composer dependencies.\n# Step 1: Compiling the\u00a0PHP\u00a0binary\nWe will need a `/bin` directory containing the PHP binary. Because we are compiling a binary this step needs to happen on the same OS and architecture that our Lambda will use.\n\nThis page\u00a0lists the AWS Execution environment, but last time I checked their version was out of date.\nTo find the correct AMI I used the latest version for the region I was deploying in, found here\u00a0by a quick regex for AMIs containing `amzn-ami-hvm-.*-gp2`.\n\nOnce you have the correct AMI, spin up a large EC2 instance and `ssh` in.\nRun the following commands as listed in AWS\u2019s docs.\n\r\nsudo yum update -y\r\nsudo yum install autoconf bison gcc gcc-c++ libcurl-devel libxml2-devel -y\r\n\r\ncurl -sL http://www.openssl.org/source/openssl-1.0.1k.tar.gz | tar -xvz\r\ncd openssl-1.0.1k\r\n./config && make && sudo make install\r\ncd ~\r\n\r\nmkdir ~/php-7-bin\r\ncurl -sL https://github.com/php/php-src/archive/php-7.3.0.tar.gz | tar -xvz\r\ncd php-src-php-7.3.0\r\n\r\n./buildconf --force\r\n./configure --prefix=/home/ec2-user/php-7-bin/ --with-openssl=/usr/local/ssl --with-curl --with-zlib\r\nmake install\r\n\nCheckpoint:\u00a0The following should give\u00a0you the version number of PHP you wanted\n /home/ec2-user/php-7-bin/bin/php -v \nMove this into a `/bin` directory.\n`mkdir './bin && mv php ./bin\nNow we can zip up the code for our Lambda layer that will provide our custom runtime API.\nzip -r runtime.zip bin bootstrap\n# Vendor files\nOn the same EC2, we need to use composer to get our vendor code.\ncurl -sS https://getcomposer.org/installer | ./bin/php`\nAdd some vendor code:\n./bin/php composer.phar require guzzlehttp/guzzle\n\u00a0\nzip -r vendor.zip vendor/\n\u00a0# Bring down to local\nNow use `scp` to copy the PHP binary down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/runtime.zip .\nNow use `scp` to copy the vendor zip down to your local machine:\n\u2013 From local:\nscp {YOUR EC2}:/home/ec2-user/php-7-bin/bin/vendor.zip .\nDon\u2019t forget to terminate your large EC2 instance\n# Creating a custom runtime API\nTo use a custom runtime AWS requires you specify a bootstrap file which will provide the interface for lambda events. As we will be writing PHP support we can write it in PHP (very meta).\nCreate a `bootstrap` executable:\ntouch ./bootstrap && chmod +x ./bootstrap\nExample adapted from AWS docs\n\r\n#!/opt/bin/php\r\n<?php\r\n\r\n// This invokes Composer's autoloader so that we'll be able to use Guzzle and any other 3rd party libraries we need.\r\nrequire __DIR__ . '/vendor/autoload.php';\r\n\r\n// amzn-ami-hvm-2017.03.1.20170812-x86_64-gp2\r\n\r\nfunction getNextRequest()\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $response = $client->get('http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/next');\r\n\r\n return [\r\n 'invocationId' => $response->getHeader('Lambda-Runtime-Aws-Request-Id')[0],\r\n 'payload' => json_decode((string) $response->;getBody(), true)\r\n ];\r\n}\r\n\r\nfunction sendResponse($invocationId, $response)\r\n{\r\n $client = new \\GuzzleHttp\\Client();\r\n $client->post(\r\n 'http://' . $_ENV['AWS_LAMBDA_RUNTIME_API'] . '/2018-06-01/runtime/invocation/' . $invocationId . '/response',\r\n ['body' => $response]\r\n );\r\n}\r\n\r\n// This is the request processing loop. Barring unrecoverable failure, this loop runs until the environment shuts down.\r\ndo {\r\n // Ask the runtime API for a request to handle.\r\n $request = getNextRequest();\r\n\r\n // Obtain the function name from the _HANDLER environment variable and ensure the function's code is available.\r\n $handlerFunction = array_slice(explode('.', $_ENV['_HANDLER']), -1)[0];\r\n require_once $_ENV['LAMBDA_TASK_ROOT'] . '/src/' . $handlerFunction . '.php';\r\n\r\n // Execute the desired function and obtain the response.\r\n $response = $handlerFunction($request['payload']);\r\n\r\n // Submit the response back to the runtime API.\r\n sendResponse($request['invocationId'], $response);\r\n} while (true);\r\n\r\n?>\r\n\n\u2013 Note: #!/opt/bin/php links to our /bin/php created earlier.\nManual Deployment\n\nGo to AWS lambda page.\nCreate a function selecting to use a custom runtime.\nCreate a layer called `php` and upload `runtime.zip`.\nCreate a layer called `vendor` and upload `vendor.zip`.\nApply the layers to the function you created in the merge order: 1) runtime, 2) vendor\n\nWriting a handler function\n\r\n\r\nmkdir src\r\n\r\ntouch src/hello.php\r\n\r\n\nAdd some basic function called `hello`:\n\r\n<?php\r\n\r\nfunction hello($data)\r\n{\r\n return \"Hello, {$data['name']}!\";\r\n}\r\n\r\n?>\r\n\nThen zip this up to be uploaded:.\nzip hello.zip src/hello.php\nUpload the function handler zip to the function and change the handler name to the name of the php file without the extension. e.g. hello.php => hello\n\u00a0\n# Automation\nMany of the steps here can be automated once you have compiled your binary. Either by using the AWS API, cloudformation or the `serverless`\u00a0library which supports layers.\n# Other languages\nThese steps should allow any language that can compile on the AWS AMI used by Lambda to be used as your runtime, e.g. Rust.\n\u00a0\nResources:\n\nhttps://aws.amazon.com/amazon-linux-ami/2018.03-release-notes/\nhttps://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html\nhttps://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html\nhttps://aws.amazon.com/blogs/apn/aws-lambda-custom-runtime-for-php-a-practical-example/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBen Ellerby\r\n  \t\t\t\r\n  \t\t\t\tI'm an Architect Developer, working with startups to launch MVP's and large corporates to deliver in startup speed. \r\n\r\nI'm tech at heart, loving to code, participating in hackathons, guest lecturing as part of the University of Southampton CompSci course and acting as a tech advisor to multiple startups.\r\n\r\nhttps://www.linkedin.com/in/benjaminellerby/  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tA year ago I was a young developer starting his journey on React. My client came to see my team and told us: \u201cWe need to make reusable components\u201d, I asked him: \u201cWhat is a reusable component?\u201d and the answer was \u201cThis project is a pilot on the subject\u201d.\n2 months later another developer tried to use our components and the disillusion started: despite our efforts, our component were not reusable\u00a0\ud83d\ude31\nAt the time we managed to work with him to improve our code so that he could use it, but how could we have avoided the problem?\nThe answer was given to me by Florian Rival, a former developer at Bam, now working for Facebook: Storybook !\n Storybook, what is that?\nIt is an open source visual documentation software (here is the repo). It allows you to display the different states of your component. The cluster of all the different cases for your component are called the component stories.\nThis allows you to visually describe your components : anyone who wants to use your components can just look at your stories and see how to use it. No need to dig in the code to find all the use cases, they are all there!\nA picture is worth a thousand words,\u00a0so just check the best example I know, the open Storybook of Airbnb.\nOne interesting thing to\u00a0note is that it\u2019s working with Vue, Angular and React!\nUsage example\nLet\u2019s make an example to explain this better to you. I will use a react todo list, I started with the one on this repo.\nThen I added Storybook to the project, I won\u2019t detail this part as the\u00a0Storybook doc\u00a0is very good. I would say it takes approximately 20 minutes to add storybook to your project, but might take longer to properly setup your asset builder.\nNow I\u2019ll focus on the component FilteredList that display the todos, first it looked like this:\n\r\nimport React from 'react';\r\nimport styled from 'styled-components';\r\nimport TodoItem from './TodoItem';\r\n\r\nconst StyledUl = styled.ul`\r\n  list-style: none;\r\n`;\r\n\r\nconst StyledP = styled.p`\r\n  margin: 10px 0;\r\n  padding: 10px;\r\n  border-radius: 0;\r\n  background: #f2f2f2;\r\n  border: 1px solid rgba(229, 229, 229, 0.5);\r\n  color: #888;\r\n`;\r\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus} = props;\r\n\r\n    if (items.length === 0) {\r\n        return (\r\n            <StyledP>There are no items.</StyledP>\r\n        );\r\n    }\r\n\r\n    return (\r\n        <StyledUl>\r\n            {items.map(item => (\r\n                <TodoItem key={item.id} data={item} changeStatus={changeStatus}/>\r\n            ))}\r\n        </StyledUl>\r\n    );\r\n}\r\n\n(It is not exactly the same as the one on the repo, I\u00a0used styled-component instead of plain css)\nTodoItem is the component that displays an element of the list.\nHere we can see there are two different branches in the render: the nominal case and the empty state.\nLet\u2019s write some stories, I created a file FilteredList.stories.js\u00a0and added this inside:\n\r\nimport React from 'react';\r\nimport { storiesOf } from '@storybook/react';\r\nimport FilteredList from \"./FilteredList\";\r\n\r\nconst data = [{\r\n    id: 1,\r\n    completed: true,\r\n    text: 'Jean-Claude Van Damme'\r\n}];\r\n\r\nstoriesOf('FilteredList')\r\n    .add('Nominal usage', () => (\r\n        <FilteredList items={data} changeMode={() => void 0}/>\r\n    ))\r\n    .add('Empty state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0}/>\r\n    ));\r\n\nSo what did I do here?\nI defined placeholder data in a variable for the component props.\nWe use the function storiesOf\u00a0from storybook, this function takes the name we want to give to the group of stories as entry param.\nThen we add some stories with .add. It\u00a0works pretty much\u00a0like jest or mocha\u2019s\u00a0describe or\u00a0it\u00a0in tests, it takes the name of the story and a function that returns the component to render.\nHere\u2019s what it looks like:\n\n\nIt\u2019s rather simple but it\u2019s working, we see the two different cases.\nWhat if we add other branches? Let\u2019s say the parent component of FilteredList\u00a0is calling an API to get the list and so we have to add a loading and error state.\n\r\nexport default function FilteredList(props) {\r\n    const {items, changeStatus, isLoading, error} = props;\r\n\r\n    if (isLoading) {\r\n        return (\r\n            <StyledP>Loading...</StyledP>\r\n        )\r\n    }\r\n\r\n    if (error) {\r\n        return (\r\n            <StyledP>Sorry an error occurred with the following message: {error}</StyledP>\r\n        )\r\n    }\r\n    \r\n    //...\r\n}\r\n\nNow we need to add the corresponding stories.\n\r\n.add('Loading state', () => (\r\n        <FilteredList items={[]} changeMode={() => void 0} isLoading />\r\n))\r\n.add('Error state', () => (\r\n    <FilteredList items={[]} changeMode={() => void 0} error=\"Internal server error\"/>\r\n));\r\n\nAnd now our Storybook looks like:\n\n\nThis example is rather simple but it shows pretty well how we worked with Storybook. Each time you create new component behaviors you then create the corresponding stories.\nAin\u2019t nobody got time for that?\nIn fact it takes time when you are coding but I feel like it is more like an investment.\nWhen you develop your app\u00a0aren\u2019t you trying to make it easy to use? Then why not make it easy for developers to use your code?\nAnd that\u2019s where Storybook is helping you, now your components are easier to share, this leads to a better collaboration between developers and therefore to better component development practices shared inside the team.\nThis is very important because you are not the only user of your code, you might be working with a team or someone else will\u00a0take over\u00a0your project afterwards.\nWe all had this part in our project code that have been here for ages and no one really know how to deal with it, how to avoid that? Make it good the first time you code it! Seems obvious but still is right, and for that you can use Storybook to share your front end components and make perfect APIs! (or at least very good ones)\nFinal thoughts\nIn a way we are all trying to do reusable components \u2013 whether you are trying to do a library or not, you want other members of your team to be able to update/fix your code. It\u2019s not easy to make perfect APIs for your components if you can not have a lot of user feedback and that\u2019s why Storybook or any visual doc are so efficient. They improve greatly the vision others have of your component and help them modify it without breaking anything.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tL\u00e9o Anesi\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tZeplin\u00a0vs\u00a0InVision: I work for a service company as a lead developer and we have been using\u00a0these tools\u00a0on different projects for mockup integration. I teamed up with France Wang, lead designer at BAM, to list the pros and cons and give you our combined point of view on these\u00a0design handoff tools.\n\nTo clarify the need we have on our projects and what we use these platforms for, here is our development workflow:\n\nDesigners make the mockups on\u00a0Sketch\nThey\u00a0import the mockup on Zeplin or InVision along with the assets\n(They add comments for developers to\u00a0explain\u00a0specific behaviours)\nDevs use the platform to inspect mockups and integrate them\n\nUsually there are back-and forth between devs and designers during the mockup integration\u00a0phase (4). Choosing the best tool helps limiting these wastes and frustrations. We focus on these goals for the benchmark. We\u2019ll also share tips on how we use these tools for better\u00a0collaboration.\nDisclaimer: this content is not sponsored by either mentioned parties\nAnd the best tool for mockup integration is\u2026\n\nAfter benchmarking the two solutions, our recommendation would be to use Zeplin,\u00a0as it is more advanced and more convenient for design handoff.\nIf you have the budget (17$-26$/month) do not hesitate! The time you\u2019ll save is worth it. The platform eases knowledge sharing between designers and developer. Consequently the production workflow will be faster and less painful. Happier collaboration yay \\o/! #DevUx\nIf you need prototyping and do user testing, use InVision in addition. Designers can use the same Sketch file for both platforms. Separate tools for separate purposes!\nDetailed Benchmark of Zeplin vs Invision\nHere is a recap of the comparison we made:\n\n\n\n\n\n\n\n\nZeplin\u2019s Pros:\n\nKiller\u00a0features:\n\nAutomatically-generated styleguide linked to the mockups\nCommenting tool\nPixel perfect comparison\n\n\nMost\u00a0complete tool for mockup integration\n\n\n\nInVision\u2019s Pros:\n\nInteresting free plan (as many collaborators as you want)\nMore complete offer if you need prototyping\nWill get the job done\n\n\n\n\n\nZeplin\u2019s Cons:\n\nPlans are more expensive and free plan less interesting if your want several collaborators\n\n\n\nInVision\u2019s Cons\n\nOverall\u00a0less efficient because the platform is multipurpose and the UX is not focused on\u00a0mockup integration.\n\n\n\n\n\nKiller feature: Semi-automatically generated styleguide\nWhy should you use a styleguide?\nUsing a styleguide helps us save time and limit rework. To\u00a0both designers and developers, the styleguide defines a design reference\u00a0(for both UI and UX).\nFor the designers, it helps with\n\nkeeping the product consistent\nsummarising\u00a0all colors, font styles, components used throughout the app\u00a0along as their states (idle, disabled, active, loading, success, error\u2026) and variations\u00a0(primary, secondary, icon only etc\u2026)\nexplaining to the devs each\u00a0component behaviour without having to repeat it on each mock up\n\nFor developers, it serves as\n\na reference of standard color codes, font sizes, font weights to use in the code\na library of components\u00a0with the variations\u00a0and states they can have\na compilation of\u00a0components behaviour so as\u00a0not to forget any cases\n\nStyleguides on Zeplin vs Invision: why Zeplin wins\nZeplin integrates an interface to create a styleguide from imported mockups. The platform detects font properties and colors so the designers can add them to the styleguide easily.\nDesigners can add colors to the styleguide in one click from the mockup and define a name for each\nDesigners can also export individual Sketch Symbols, which will then appear in the styleguide Components section.\nThese components are reused by the designer on several mockups. You can see how the navigation looks like with 2 and 3 tabs.\nComponents and mockup are linked: on the styleguide, the developer can see which mockups use which components. Reciprocally, the developer can also see on\u00a0each mockup which components are used and can click the link to see different component states.\nA link to the component styleguide from a mockup on Zeplin\nDesigners can also share their styleguide publicly to get visibility and reactions.\nOn InVision, \u00a0if you want a styleguide you will need to create one from scratch in Sketch.\u00a0Consequently, there are no links between the styleguide and the mockups, so it is less maintainable (or takes too much time to maintain) and is less visible for the team.\nCommenting the mockup to clarify integration\nComments are an essential feature to hand over extra informations not visible on the mockup or the inspector. They also allow to cover edge cases. Is it scrollable? Vertically centered? Is the size proportional? These pieces of information\u00a0should be left by the designers for the developer to use on integration.\nComments on Zeplin are visible when the developer inspects elements\nWhat makes it best on Zeplin is that comments are visible by default on the inspector\u2019s page. On InVision they are on a separate page, and the users need\u00a0to switch between modes. So they\u00a0can forget they exist. What a pity \nComments on InVision are on a separate page as the Inspector\nSmall plus, Zeplin\u00a0lets you to categorise your comments. However\u00a0the links you attach are not always clickable (is it a bug?)\nAnother\u00a0killer feature for pixel perfect integration\n(Only on Mac) With Zeplin\u00a0desktop\u00a0app, you can generate a transparent overlay of the design to compare to the actual development. Of course if you don\u2019t need this level of accuracy it\u2019s just a wow feature \ud83d\ude09\nThis helps the dev checking that their integration matches exactly the mockup, for the desired screen sizes.\nPop out the mockup in a transparent window and move it over your app to check for differences\nCss properties inspection\nThis is a basic feature that\u00a0both platforms allow, but here is what makes Zeplin\u00a0slightly better:\n\nThe inspector\u2019s panel is more condensed\u00a0because null properties are hidden. So you can check properties more easily without having to scroll down\nCSS has syntaxic coloration\n\n\n\n\n\nZeplin inspector\n\nInVision inspector: opacity and left alignment are not useful to have here as these are default values\n\n\n\nPadding/margin inspect\nIn this battle of Zeplin vs Invision, this is the only\u00a0round where\u00a0Zeplin looses!\nDesigners create groups of elements in Sketch to have blocks containing a label and its value for instance, or an input with its submit button \u2013 just like developers might build their app. Designers use these groups to place blocks on the mockup and build a screen.\nWe tried a little experiment on both platforms. The designer made a table with header and values on Sketch.\nHere is\u00a0what you can see on InVision\u2019s inspector tab\nWe uploaded the Sketch file on InVision. Above is what we could see in the inspector: we were able to select the \u2018Group 5\u2032 containing the invoice number and its value\u00a0\u201901_000001\u2019. So by hovering the adjacent block, we could see the margin in between (48px).\nZeplin\u2019s\u00a0mockup inspection panel\nThen we uploaded the same sketch file on Zeplin: the\u00a0Sketch groups are not replicated on the inspector platform: you cannot select the\u00a0container blocks, only the text elements. Therefore, devs cannot see spacing between blocks easily. They loose the designer\u2019s previous reflection on block cutting and spacing. It\u2019s too bad to have two people do the same work twice!\nThe downside of using groups on the other hand is that it makes small details inspection harder. By experience, InVision\u2019s inspector cursor is less accurate because you hover the groups before the element. You need to zoom in for more accuracy.\nManaging Assets\nManaging assets is equivalent on both platforms. Once the designer has set the export options in Sketch, the developer can download\u00a0it from the inspect page when inspecting an element.\nZeplin vs Invision: on both, developers can pick the file format they want from the available list\nYou can find the list of downloadable assets in a specific section.\nOn InVision, you can see the downloadable assets in the group panel.\nIcon inspection on Invision: on the left panel you can see that the Icon/User is downloadable.\nPrototyping\nWhy many choose InVision is because you can make clickable mockups\u00a0to navigate from a screen to another. You can test the\u00a0prototype with\u00a0the targeted users. Very handy to get feedback before development starts!\nThe downside of using InVision both for prototyping and integration comes when several version of the same mockup conflict. This can occur when mockups include\u00a0features under testing phase. It gets confusing for the developers.\nPricing (updated 27/11/18)\nZeplin\nFree tier: 1 project, no collaboration\nIf you are a service company with several clients,\u00a0your clients cannot create an account and invite you on their project, you\u2019ll need to disconnect and connect to\u00a0the same account.\nStarter: 17$/mo 3 projects \u2013 unlimited collaborators\nGrowing business: 26$/mo \u2013 17 projects \u2013 unlimited collaborators\nOrganization: from 122$/mo with only 16 collaborators, +7$/mo per extra collaborator\nInVision\nFree tier: 1 project \u2013 unlimited collaborators\nStarter: 15$/mo \u2013 3 prototypes\nGrowing business: 25$/mo \u2013 unlimited prototypes\nTeam: 99$/mo unlimited prototypes but only 5 members\nCustom: on demand\nTo go further into benchmarking\nSome other tools like Avocode would also need our attention, notably because of their powerful assets export feature. Framer\u2019s new FramerX tool is also an important player we should pay attention too. Their Beta version is not collaborative yet like Invision or Zeplin, but their prototyping tool based on React components is promising.\nWe at Theodo are building our custom Sketch plugin in order to make it even faster to integrate a component.\nWith Overlay, we can export components from Sketch and get prod ready React/VueJs code.\nGenerate React.js/Vue.js components with full design from Sketch files with Overlay plugin\nConclusion\nAll in all,\u00a0these tools will increase your mockup integration process. Nonetheless, the difference lies in\u00a0subtle details and better user experience.\u00a0That\u2019s what gives\u00a0Zeplin\u00a0the edge over\u00a0InVision.\nHaving developers and designers working together is not easy. Indeed, each profile\u00a0has their own stakes and think differently. We have a lot to learn from each other to deliver the best products. Integrating\u00a0the right tool in our process will make the collaboration much smoother.\nFrance and I have been working together to spread the DevUx culture (yes, DevOps is not the only one). This goes from understanding each other, defining mutual expectations and design collaboratively. More articles are coming!\nDon\u2019t hesitate to share your experience and tips using these tools and more generally on your mockup integration processes \nCredits and Resources\nCover pic\u00a0(more sumo battles there, thank you\u00a0Tomoshi Shiiba\u00a0for your art work)\nZeplin/Sketch gif\u00a0(from an article on how\u00a0The Create Labs\u00a0implemented Sketch & Zeplin in their workflow)\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tYu Ling Cheng\r\n  \t\t\t\r\n  \t\t\t\tLead Developer at Theodo\r\nhttps://www.linkedin.com/in/yulingcheng  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tUsually, we are completely running React.js on client-side: Javascript is interpreted by your browser. The initial html returned by the server contains a placeholder, e.g.\u00a0<div id=\"root\"></div>, and then, once all your scripts are loaded, the entire UI is rendered in the browser. We call it client-side rendering.\nThe problem is that, in the meantime, your visitor sees\u2026 nothing, a blank page!\nLooking for how to get rid of this crappy blank page for a personal project, I discovered Next.js: in my opinion the current best framework for making server-side rendering and production ready React applications.\nWhy SSR (Server-Side Rendering)?\nThis is not the point of this article, but here is a quick sum-up of what server-side rendering can bring to your application:\n\nImprove your SEO\nSpeed up your first page load\nAvoid blank page flicker before rendering\n\nIf you want to know more about it, please read this great article: Benefits of Server-Side Over Client Side Rendering.\nBut let\u2019s focus on the \u201chow\u201d rather than the \u201cwhy\u201d here.\nWhat\u2019s the plan?\nFor this article, I start with a basic app made\u00a0with\u00a0create-react-app. Your own React application is probably using similar settings.\nThis article is split in 3 sections matching 3 server-side-rendering strategies:\n\nHow tomanually upgrade your React app to get\u00a0SSR\nHow to start with Next.js from scratch\nMigrate your existing React app to server-side with Next.js\n\nI won\u2019t go through all the steps, but I will bring your attention on the main points of interesting. I also provide a repository for each of the 3 strategies. As the article is a bit long, I\u2019ve split it in 2 articles, this one will only deal with the first 2 sections. If your main concern is to migrate your app to Next.js, you can go directly to the second article\u00a0(coming soon).\n1) Look how twisted manual SSR is\u2026\n\nIn this part, we will see how to implement Server-side Rendering\u00a0manually on an existing React app. Let\u2019s take the\u00a0create-react-app\u00a0starter code:\n\npackage.json\u00a0for dependencies\nWebpack configuration included\nApp.js\u00a0\u2013 loads React and renders the Hello component\nindex.js\u00a0\u2013 puts all together into a root component\n\nChecking rendering type\nI just added to the code base a simple function\u00a0isClientOrServer\u00a0based on the availability of\u00a0the Javascript object window representing the browser\u2019s window:\nconst isClientOrServer = () => {\r\n  return (typeof window !== 'undefined' && window.document) ? 'client' : 'server';\r\n};\r\n\nso that we display on the page what is rendering the application: server or client.\nTest it by yourself\n\nclone\u00a0this repository\ncheckout the initial commit\ninstall the dependencies with\u00a0yarn\nlaunch the dev server with\u00a0yarn start\nbrowse to\u00a0http://localhost:3000\u00a0to view the app\n\nI am now simulating a \u20183G network\u2019 in Chrome so that we really understand what is going on:\n\nImplementing\u00a0Server-side Rendering\nLet\u2019s fix that crappy flickering with server-side rendering! I won\u2019t show all the code (check the repo to see it in details) but here are the main steps.\nWe first need a node server using Express:\u00a0yarn add express.\nIn our React app, Webpack only loads the src/ folder, we can thus create a new folder named server/ next to it. Inside, create a file\u00a0index.js\u00a0where we use express and a server renderer.\n// use port 3001 because 3000 is used to serve our React app build\r\nconst PORT = 3001; const path = require('path');\r\n\r\n// initialize the application and create the routes\r\nconst app = express();\r\nconst router = express.Router();\r\n\r\n// root (/) should always serve our server rendered page\r\nrouter.use('^/$', serverRenderer);\r\n\nTo render our html, we use a server renderer that is replacing the root component with the built html:\n// index.html file created by create-react-app build tool\r\nconst filePath = path.resolve(__dirname, '..', '..', 'build', 'index.html');\r\n\r\nfs.readFile(filePath, 'utf8', (err, htmlData) => {\r\n  // render the app as a string\r\n  const html = ReactDOMServer.renderToString(<App />);\r\n\r\n  // inject the rendered app into our html\r\n  return res.send(\r\n    htmlData.replace(\r\n      '<div id=\"root\"></div>',\r\n      `<div id=\"root\">${html}</div>`\r\n    )\r\n  );\r\n}\r\n\nThis is possible thanks to\u00a0ReactDOMServer.renderToString\u00a0which fully renders the HTML markup of a page to a string.\nWe finally need an entry point that will tell Node how to interpret our React JSX code. We achieve this with Babel.\nrequire('babel-register')({\r\n  ignore: [ /(node_modules)/ ],\r\n  presets: ['es2015', 'react-app']\r\n});\r\n\nTest it by yourself\n\ncheckout last changes on master branch\ninstall the dependencies with\u00a0yarn\nbuild the application with\u00a0yarn build\ndeclare babel environment in your terminal:\u00a0export BABEL_ENV=development\nlaunch your node server with\u00a0node server/bootstrap.js\nbrowse to\u00a0http://localhost:3001\u00a0to view the app\n\nStill simulating the \u20183G network\u2019 in Chrome, here is the result:\n\nDo not be mistaken, the page is rendered by server. But as soon as the javascript is fully loaded, window.document is available and the\u00a0isClientOrServer()\u00a0function returns\u00a0client.\nWe proved that we can do Server-side Rendering, but what\u2019s going on\u00a0with that React logo?!\nWe\u2019re missing many features\nOur example is a good proof of concept but very limited. We would like to see more features like:\n\nimport images in js files (logo problem)\nseveral routes usage or route management (check\u00a0this article)\ndeal with the\u00a0</head>\u00a0and the metatags (for SEO improvements)\ncode splitting (here is\u00a0an article\u00a0solving the problem)\nmanage the state of our app or use Redux (check this\u00a0great article\n\nand performance is bad on large pages:\u00a0ReactDOMServer.renderToString()\u00a0is a synchronous CPU bound call and can starve out incoming requests to the server. Walmart worked on\u00a0an optimization\u00a0for their e-commerce website.\nIt is possible to make Server-side Rendering\u00a0work perfectly on top of create-react-app, we won\u2019t go through all the painful work in this article. Still, if you\u2019re interested in it, I attached just above some great articles giving detailed explanations.\nSeriously\u2026 Next.js can bring you all these features!\n2) Next.js helps you building server rendered React.js Application\n\nWhat is Next.js?\nNext.js is a minimalistic framework for server-rendered React applications with:\n\na very simple page based routing\nWebpack hot reloading\nautomatic transpilation (with babel)\ndeployment facilities\nautomatic code splitting (loads page faster)\nbuilt in css support\nability to run server-side actions\nsimple integration with Redux using next-redux-wrapper.\n\nGet started in 1 minute\nIn this short example, we are going to see how crazy simple it is to have a server-side rendering app ready with Next.js.\nFirst, generate your package.json with\u00a0npm init\u00a0and install Next.js with\u00a0npm install --save next react react-dom. Then, add a script to your package.json like this:\n\"scripts\": {\r\n  \"dev\": \"next\",\r\n  \"build\": \"next build\",\r\n  \"start\": \"next start\"\r\n}\r\n\nCreate a pages/ folder. Every .js file becomes a route that gets automatically processed and rendered. Add a index.js file in that pages/ folder (with the execution of our\u00a0isClientOrServer\u00a0function):\nconst Index = ({ title = 'Hello from Next.js' }) => (\r\n  <div>\r\n    <h1>{title}</h1>\r\n    <p className=\"App-intro\">\r\n      Is my application rendered by server or client?\r\n    </p>\r\n    <h2><code>{isClientOrServer()}</code></h2>\r\n  </div>\r\n);\r\n\r\nexport default Index;\r\n\nNo need to import any library at the top of our index.js file, Next.js already knows that we are using React.\nNow enter\u00a0npm run dev\u00a0into your terminal and go to\u00a0http://localhost:3000: Tadaaaaa!\n\nRepeat the same operation inside your pages/ folder to create a new page. The url to access it will directly match the name you give to the file.\nYou\u2019re ready to go! You\u2019re already doing SSR. Check the\u00a0documentation on Next.js\u00a0official repository.\nUse create-next-app\nYou want to start a server-side rendered React app, you can now stop using create-react-app, and start using\u00a0create-next-app:\nnpm install -g create-next-app\r\n\r\ncreate-next-app my-app\r\ncd my-app/\r\nnpm run dev\r\n\nThis is all you need to do to create a React app with server-side rendering thanks to Next.js.\nFinally, better than a simple Hello World app, check this\u00a0Hacker News clone\u00a0implementing Next.js. It is fully server-rendered, queries the data over Firebase and updates in realtime as new votes come in.\nVue.js and Nuxt\nYou\u2019re maybe a Vue.js developer. Just after Next.js first release, two french brothers made the same for Vue.js: Nuxt was born! Like Vue, the Nuxt documentation is very clear and you can use the same starter template\u00a0vue-cli\u00a0for you app:\n vue init nuxt-community/starter-template <project-name> \nWhat\u2019s Next? \nHope you liked this article which was mainly written in order to introduce server-side rendering with Next.\nIf you are interested in server-side rendering for your existing React application, in the following, I am going to demonstrate how to migrate your existing create-react-app to Next.js. Coming soon\u2026\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBaptiste Jan\r\n  \t\t\t\r\n  \t\t\t\tWeb Developer @Theodo. I like Vue.js and all the ecosystem growing around.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\ttl:dr\nTo add a pre-commit git hook with Husky:\n\nInstall Husky with npm install husky --save-dev\nSet the pre-commit command in your package.json:\n\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nWhat are git hooks?\nGit hooks are scripts launched when carrying out some git actions. The most common one is the pre-commit hook that\u00a0runs when performing git commit, before the commit is actually created.\nThe scripts are located in the .git/hooks folder. Check out the\u00a0.sample file examples in your local git repository.\n\nWhy do I need to install the Husky\u00a0package then?\nThe problem with git hooks is that they are in the .git directory, which means that they are not committed hence not shared between developers.\nHusky takes care of this: when a developer runs npm install, it will automatically create the scripts in .git/hooks:\n\nTheses scripts will parse your package.json and run the associated command. For example, the pre-commit script will run the npm run precommit command\n\"scripts\": {\r\n    \"precommit\": \"npm test\"\r\n},\r\n\nTo add husky to your project, simply run npm install husky --save-dev.\nFor more complex commands, I recommend to use a separate bash script :\u00a0\"precommit\": \"bash ./scripts/check-lint.sh\".\nEnhancing your git flow\nGit hooks are a convenient way to automate tasks during your git flow and protect you from pushing unwanted code by mistake.\n\nCheck for linting errors\n\nIf you have tools to check the code quality or formatting, you can run it on a pre-commit hook:\n\"scripts\": {\r\n    \"precommit\": \"prettier-check \\\"**/*.js\\\" && eslint . --quiet\"\r\n},\r\n\nI advise to run those tests on your CI tool as well, but checking it on a precommit hook can make you\u00a0save a lot of time as you won\u2019t have to wait for your CI to set up your whole project and fail only because you forgot a\u00a0semicolon.\n\nProtect important branches\n\nIn some rare situations, you have to push code directly on a branch that is deployed. One way to protect\u00a0it from developers in a hurry who forget to run the tests locally is to launch them on a pre-push hook:\n\"scripts\": {\r\n    \"prepush\": \"./scripts/pre-push-check.sh\"\r\n},\r\n\n\n#!/bin/bash\r\nset -e\r\n\r\nbranch=$(git branch | sed -n -e 's/^\\* \\(.*\\)/\\1/p')\r\nif [ \"$branch\" == \"master\" ]\r\nthen\r\n    npm test\r\nfi\r\n\n\nIf your tests fail, the code won\u2019t be pushed.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHugo Lime\r\n  \t\t\t\r\n  \t\t\t\tAgile Web Developer at Theodo.\r\n\r\nPassionate about new technologies to make web apps stronger and faster.  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy\nWhat is a Virtual DOM ?\nThe virtual DOM (VDOM) is a programming concept where an ideal, or \u201cvirtual\u201d, representation of a UI is kept in memory.\nThen, it is\u00a0synced with the \u201creal\u201d DOM by a library such as ReactDOM. This process is called\u00a0reconciliation.\nPerformance\u00a0and\u00a0windowing\nYou might know that React uses this virtual DOM. Thus, it is only when React renders elements that the user will have them into his/her HTML DOM.\nSometimes you might want to display a lot of html elements, like for grids,\u00a0lists, calendars, dropdowns etc and the user will often complain about performance.\n\nHence, a good way\u00a0to display a lot of information is to \u2018window\u2019\u00a0it. The idea is to create only elements the user can see. An example is the Kindle vs Book. While the book is a heavy object because it \u2018renders\u2019 all the pages, the Kindle only display what the user can see.\nReact-Virtualized\nThat is how Brian Vaughn came up with the idea of creating React-Virtualized.\nIt is an open-source library which provides you many components in order to window some of your application List, Grid etc\nAs a developer, you do not want to reinvent the wheel. React-virtualized is a stable and maintained library. Its community is large and as it is open-source, many modules and extensions are already available in order to window a maximum of elements.\nFurthermore, it offers lots of functionalities and customization that you would not even think about.\nWe will discuss about it later, but before, let\u2019s see when to use React-virtualized.\nWhen\nWhen thinking about performance, lots of actions can be taken, but\u00a0React official website\u00a0already got a complete article to be read. In consequence, if you face a performance problem, be sure you have already done all of these before to start to window your application (but stay pragmatic).\nHow\nGetting into it\nOk, now that you\u2019re convinced, let\u2019s go throught the real part.\n\nYou can begin by following instructions for installing the right package via npm and look at simple examples here : React virtualized github.\u00a0However,\u00a0I\u2019m going to show you a complex example so you can use React-Virtualized in an advanced way.\nReact-Virtualized 101\nTo render a windowed list, no need for digging one hour a complex documentation, React-Virtualized is very simple to use.\nFirstly, you use the List component from the library, then, the few important props are the next one:\n\nwidth: the width of the List\nheight: the height of the List\nrowCount: the number of elements you will display\nrowHeight: the height of each row you will display\nrowRenderer: a callback method to define yourself depending on what you want to do with your data. This method is the core of your list, it is here that you define what will be rendered thanks to your data.\n\nThe example\n\r\nimport React from 'react';\r\nimport { List } from 'react-virtualized';\r\n\r\n// List data as an array of strings\r\nconst list = [\r\n 'Easy windowing1',\r\n 'Easy windowing2',\r\n // And so on...\r\n];\r\n\r\nfunction rowRenderer({\r\n key, // Unique key within array of rows\r\n index, // Index of row within collection\r\n isScrolling, // Used for performance\r\n isVisible, // Used for performancee\r\n style, // Style object to be applied to row (to position it)\r\n}) {\r\n return (\r\n\r\n<div key={key} style={style}>\r\n   {list[index]}\r\n </div>\r\n\r\n );\r\n}\r\n\r\n// Render your list\r\nconst ListExample = () => (\r\n <List width={300} height={300} rowCount={list.length} rowHeight={20} rowRenderer={rowRenderer} />\r\n);\r\n\nClick here to see a demo\nA more complex virtualized list:\nDisplay a virtualized list might be easy, but you might have a complicated behaviour to implement.\n\nIn this advanced example, we will:\n\nUse the AutoSizer HOC to automatically calculate the size the List will fill\nBe able to display row with a dynamic height using the CellMeasurer\nBe able to use the CellMeasurer even if the data are not static\n\nThis advanced example goes through 4 steps:\n\nInstantiate the AutoSizer and List component\nSee how the CellMeasurer and the CellMeasurerCache work\nLearn how we use them in the rowRenderer\nGo further with using these on a list that does not contain a stable number of elements\n\nThe example\nLet\u2019s look first at how we render the list:\n\u00a0\n\r\nimport {\r\n AutoSizer,\r\n List,\r\n CellMeasurer,\r\n CellMeasurerCache,\r\n} from 'react-virtualized';\r\n...\r\n<AutoSizer>\r\n {({ height, width }) => (\r\n  <List\r\n    width={width}\r\n    height={height}\r\n    rowGetter={({ index }) => rows[index]}\r\n    rowCount={1000}\r\n    rowHeight={40}\r\n    rowRenderer={this.rowRenderer}\r\n    headerHeight={20}\r\n  />\r\n )}\r\n</AutoSizer>\r\n\nIt is very simple:\n\nWe wrap the list with the AutoSizer HOC\nIt uses the CellMeasurerCache to know the height of each row and the rowRenderer to render the elements.\n\nHow it works :\nFirst, you instantiate a new CellMeasurerCache that will contain all the calculated heights :\n\r\nconstructor(props) {\r\n super(props);\r\n this.cache = new CellMeasurerCache({ //Define a CellMeasurerCache --> Put the height and width you think are the best\r\n defaultHeight: 80,\r\n minHeight: 50,\r\n fixedWidth: true,\r\n });\r\n}\r\n\nThen, you use the CellMeasurer in the rowRenderer method:\n\r\nrowRenderer = ({\r\n   key, // Unique key within array of rows\r\n   index, // Index of row within collection\r\n   parent,\r\n   isScrolling, // The List is currently being scrolled --> Important if you need some perf adjustment\r\n   isVisible, // This row is visible within the List (eg it is not an overscanned row)\r\n   style, // Style object to be applied to row (to position it)\r\n}) => (\r\n   <CellMeasurer\r\n     cache={this.cache}\r\n     columnIndex={0}\r\n     key={key}\r\n     parent={parent}\r\n     rowIndex={index}\r\n   >\r\n   <div\r\n     className=\"Row\"\r\n     key={key}\r\n     style={{\r\n       ...style,\r\n       display: 'flex',\r\n     }}\r\n   >\r\n     <span style={{ width: 400 }}>{rows[index].name}</span>\r\n     <span style={{ width: 100 }}>{rows[index].age}</span>\r\n   </div>\r\n   </CellMeasurer>\r\n);\r\n\n\u00a0\nPitfall:\nFinally, we obtain a nice windowed list, ready to be deployed and used\u2026\nUnless your application contain filters or some data added dynamically.\nActually, when I implemented this, after using some filters, some blank spaces were staying in the list.\nIt is a performance consideration due to the fact we use a cache, but it is a good compromise unless you have many rows and many columns in a Grid (as we display a list, we only have 1 column).\nConsequently, I managed to fix this issue by clearing the cache every time my list had its data reloaded:\n\r\ncomponentWillReceiveProps() { //Really important !!\r\n this.cache.clearAll(); //Clear the cache if row heights are recompute to be sure there are no \"blank spaces\" (some row are erased)\r\n this.virtualizedList && this.virtualizedList.recomputeRowHeights(); //We need to recompute the heights\r\n}\r\n\nA big thanks to\u00a0Brian Vaughn\u00a0for this amazing library\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tCyril Gaunet\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhy?\nAdding upload fields in Symfony application eases the way of managing assets. It makes it possible to upload public assets as well as sensitive documents instantly without any devops knowledge. Hence, I\u2019ll show you a way of implementing a Symfony / Amazon AWS architecture to store your documents in the cloud.\nSetup Symfony and AWS\nFirst you need to setup both Symfony and AWS to start storing some files from Symfony in AWS buckets.\nAmazon AWS\nCreating a bucket on Amazon AWS is really straight forward. First you need to sign up on Amazon S3 (http://aws.amazon.com/s3). Go to the AWS console and search S3. Then click on Create a bucket.\nFollow bucket creation process choosing default values (unless you purposely want to give public access to your documents, you should keep your bucket private). Eventually create a directory for each of your environments. Your AWS S3 bucket is now ready to store your documents.\nSymfony\nNow you need to setup Symfony to be able to store files and to communicate with Amazon AWS. You will need 2 bundles and 1 SDK to set it up:\n\nVichUploader (a bundle that will ease files upload)\nKNP/Gauffrette (a bundle that will provide an abstraction layer to use uploaded files in your Symfony application without requiring to know where those files are actually stored)\nAWS-SDK (A SDK provided by Amazon to communicate with AWS API)\n\nInstall the two bundles and the SDK with composer:\n\r\ncomposer require vich/uploader-bundle\r\ncomposer require aws/aws-sdk-php\r\ncomposer require knplabs/knp-gaufrette-bundle\r\n\r\n\nThen register the bundles in AppKernel.php\n\r\npublic function registerBundles()\r\n    {\r\n     return [\r\n            \tnew Vich\\UploaderBundle\\VichUploaderBundle(),\r\n            \tnew Knp\\Bundle\\GaufretteBundle\\KnpGaufretteBundle(),\r\n            ];\r\n    }\r\n\r\n\nBucket parameters\nIt is highly recommended to use environment variables to store your buckets parameters and credentials. It will make it possible to use different buckets depending on your environment and will prevent credentials from being stored in version control system. Hence, you won\u2019t pollute your production buckets with test files generated in development environment.\nYou will need to define four parameters to get access to your AWS bucket:\n\nAWS_BUCKET_NAME\nAWS_BASE_URL\nAWS_KEY (only for and private buckets)\nAWS_SECRET_KEY (only for and private buckets)\n\nYou can find the values of these parameters in your AWS console.\nConfiguration\nYou will have to define a service extending Amazon AWS client and using your AWS credentials.\nAdd this service in services.yml:\n\r\nct_file_store.s3:\r\n        class: Aws\\S3\\S3Client\r\n        factory: [Aws\\S3\\S3Client, 'factory']\r\n        arguments:\r\n            -\r\n                version: YOUR_AWS_S3_VERSION (to be found in AWS console depending on your bucket region and version)\r\n                region: YOUR_AWS_S3_REGION\r\n                credentials:\r\n                    key: '%env(AWS_KEY)%'\r\n                    secret: '%env(AWS_SECRET_KEY)%'\r\n\r\n\nNow you need to configure VichUploader and KNP_Gaufrette in Symfony/app/config/config.yml. Use the parameters previously stored in your environment variables.\nHere is a basic example:\n\r\nknp_gaufrette:\r\n    stream_wrapper: ~\r\n    adapters:\r\n        document_adapter:\r\n            aws_s3:\r\n                service_id: ct_file_store.s3\r\n                bucket_name: '%env(AWS_BUCKET_NAME)%'\r\n                detect_content_type: true\r\n                options:\r\n                    create: true\r\n                    directory: document\r\n    filesystems:\r\n        document_fs:\r\n            adapter:    document_adapter\r\n\r\nvich_uploader:\r\n    db_driver: orm\r\n    storage: gaufrette\r\n    mappings:\r\n        document:\r\n            inject_on_load: true\r\n            uri_prefix: \"%env(AWS_BASE_URL)%/%env(AWS_BUCKET_NAME)%/document\"\r\n            upload_destination: document_fs\r\n            delete_on_update:   false\r\n            delete_on_remove:   false \r\n\r\n\nUpload files\nFirst step in our workflow is to upload a file from Symfony to AWS. You should create an entity to store your uploaded document (getters and setters are omitted for clarity, you will need to generate them).\nThe attribute mapping in $documentFile property annotation corresponds to the mapping defined in config.yml. Don\u2019t forget the class attribute @Vich\\Uploadable().\n\r\nnamespace MyBundle\\Entity;\r\n\r\nuse Doctrine\\ORM\\Mapping as ORM;\r\nuse Symfony\\Component\\HttpFoundation\\File\\File;\r\nuse Vich\\UploaderBundle\\Mapping\\Annotation as Vich;\r\n\r\n/**\r\n * Class Document\r\n *\r\n * @ORM\\Table(name=\"document\")\r\n * @ORM\\Entity()\r\n * @Vich\\Uploadable()\r\n */\r\nclass Document\r\n{\r\n    /**\r\n     * @var int\r\n     *\r\n     * @ORM\\Column(type=\"integer\", name=\"id\")\r\n     * @ORM\\Id\r\n     * @ORM\\GeneratedValue(strategy=\"AUTO\")\r\n     */\r\n    private $id;\r\n\r\n    /**\r\n     * @var string\r\n     *\r\n     * @ORM\\Column(type=\"string\", length=255, nullable=true)\r\n     */\r\n    private $documentFileName;\r\n\r\n    /**\r\n     * @var File\r\n     * @Vich\\UploadableField(mapping=\"document\", fileNameProperty=\"documentFileName\")\r\n     */\r\n    private $documentFile;\r\n\r\n    /**\r\n     * @var \\DateTime\r\n     *\r\n     * @ORM\\Column(type=\"datetime\")\r\n     */\r\n    private $updatedAt;\r\n}\r\n\r\n\nThen you can add an uploaded document to any of your entities:\n\r\n     /**\r\n     * @var Document\r\n     *\r\n     * @ORM\\OneToOne(\r\n     *     targetEntity=\"\\MyBundle\\Entity\\Document\",\r\n     *     orphanRemoval=true,\r\n     *     cascade={\"persist\", \"remove\"},\r\n     * )\r\n     * @ORM\\JoinColumn(name=\"document_file_id\", referencedColumnName=\"id\", onDelete=\"SET NULL\")\r\n     */\r\n    private $myDocument;\r\n\r\n\nCreate a form type to be able to upload a document:\n\r\nclass UploadDocumentType extends AbstractType\r\n{\r\n    public function buildForm(FormBuilderInterface $builder, array $options)\r\n    {\r\n        add('myDocument', VichFileType::class, [\r\n                'label'         => false,\r\n                'required'      => false,\r\n                'allow_delete'  => false,\r\n                'download_link' => true,\r\n            ]);\r\n    }\r\n...\r\n}\r\n\r\n\nUse this form type in your controller and pass the form to the twig:\n\r\n...\r\n$myEntity = new MyEntity();\r\n$form = $this->createForm(UploadDocumentType::class, $myEntity);\r\n...\r\nreturn [ 'form' => $form->createView()];\r\n\r\n\nFinally, add this form field in your twig and you should see an upload field in your form:\n\r\n<div class=\"row\">\r\n    <div class=\"col-xs-4\">\r\n        {{ form_label(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8\">\r\n        {{ form_widget(form.myDocument) }}\r\n    </div>\r\n    <div class=\"col-xs-8 col-xs-offset-4\">\r\n        {{ form_errors(form.myDocument) }}\r\n    </div>\r\n</div>\r\n\r\n\nNavigate to your page, upload a file and submit your form. You should now be able to see this document in your AWS bucket.\nUsers are now able to upload files on your Symfony application and these documents are safely stored on Amazon AWS S3 bucket. The next step is to provide a way to download and display these documents from AWS in your Symfony application.\nDisplay or download documents stored in private buckets\nIn most cases, your files are stored in private buckets. Here is a step by step way to safely give access to these documents to your users.\nGet your document from private bucket\nYou will need a method to retrieve your files from your private bucket and display it on a custom route. As a result, users will never see the actual route used to download the file. You should define this method in a separate service and use it in the controller.\ns3Client is the service (ct_file_store.s3) we defined previously extending AWS S3 client with credentials for private bucket. You will need to inject your bucket name from your environment variables in this service. my-documents/ is the folder you created to store your documents.\n\r\n     /**\r\n     * @param string $documentName\r\n     *\r\n     * @return \\Aws\\Result|bool\r\n     */\r\n    public function getDocumentFromPrivateBucket($documentName)\r\n    {\r\n        try {\r\n            return $this->s3Client->getObject(\r\n                [\r\n                    'Bucket' => $this->privateBucketName,\r\n                    'Key'    => 'my-documents/'.$documentName,\r\n                ]\r\n            );\r\n        } catch (S3Exception $e) {\r\n            // Handle your exception here\r\n        }\r\n    }\r\n\r\n\nDefine an action with a custom route:\nYou will need to use the method previously defined to download the file from AWS and expose it on a custom route.\n\r\n     /**\r\n     * @param Document $document\r\n     * @Route(\"/{id}/download-document\", name=\"download_document\")\r\n     * @return RedirectResponse|Response\r\n     */\r\n    public function downloadDocumentAction(Document $document)\r\n    {\r\n        $awsS3Uploader  = $this->get('app.service.s3_uploader');\r\n\r\n        $result = $awsS3Uploader->getDocumentOnPrivateBucket($document->getDocumentFileName());\r\n\r\n        if ($result) {\r\n            // Display the object in the browser\r\n            header(\"Content-Type: {$result['ContentType']}\");\r\n            echo $result['Body'];\r\n\r\n            return new Response();\r\n        }\r\n\r\n        return new Response('', 404);\r\n    }\r\n\r\n\nDownload document\nEventually add a download button to access a document stored in a private bucket directly in your Symfony application.\n\r\n<a href=\"{{ path('/download-document', {'id': document.id}) }}\" \r\n                   target=\"_blank\">\r\n   <i class=\"fa fa-print\">\r\n   {{ 'label_document_download'|trans }}\r\n</a>\r\n\r\n\nPublic assets\nYou may want to display some assets from Amazon AWS in public pages of your application. To do so, use a public bucket to upload these assets. It is quite straight forward to access it to display them. Be conscious that anyone will be able to access these files outside your application.\n\r\n<img src=\"{{ vich_uploader_asset(myEntity.myDocument, 'documentFile') }}\" alt=\"\" />\r\n\r\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAlan Rauzier\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThanks to a new colleague of mine, I have learned how to make my git history cleaner and more understandable. \nThe principle is simple: rebase your branch before you merge it. But this technique also has weaknesses. In this article, I will explain what a rebase and a merge really do and what are the implications of this technique.\nBasically, here is an example of my git history before and after I used this technique.\n\nStay focus, rebase and merge are no joke! \nWhat is the goal of a rebase or a merge ?\nRebase and merge both aim at integrating changes that happened on another branch into your branch.\nWhat happens during a merge ?\nFirst of all there are two types of merge:\n\nFast-forward merge\n3-way merge\n\nFast-forward merge\nA fast-forward merge happens when the most recent shared commit between the two branches is also the tip of the branch in which you are merging.\nThe following drawing shows what happens during a fast-forward merge and how it is shown on a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nAs you can see, git simply brings the new commits on top of branch A. After a fast-forward merge, branches A and B are exactly the same.\nNotes:\n\ngit checkout A, git rebase B you would have had the exact same result!\ngit checkout B, git merge A would have left the branches in the \u201cbefore\u201d situation, since branch A has no new commits for branch B.\n\n3-way merge\nA 3-way merge happens when both branches have had new commits since the last shared commit.\nThe following drawing shows what happens during a 3-way merge and how it is shown in a graphical git software.\nA: the branch in which you are merging\nB: the branch from which you get the modifications\n\ngit checkout A\ngit merge B\n\n\nDuring a 3-way merge, git creates a new commit named \u201cmerge commit\u201d (in orange) that contains:\n\nAll the modifications brought by the three commits from B (in purple)\nThe possible conflict resolutions\n\nGit will keep all information about the commits of the merged branch B even if you delete it. On a graphical git software, git will also keep a small loop to represent the merge.\nThe default behaviour of git is to try a fast-forward merge first. If it\u2019s not possible, that is to say if both branch have had changes since the last shared commit, it will be a 3-way merge.\nWhat happens during a rebase?\nA rebase differ from a merge in the way in which it integrates the modifications.\nThe following drawings show what happens during a rebase and how it is shown in a graphical git software.\nA: the branch that you are rebasing\nB: the branch from which you get the new commits\n\ngit checkout A\ngit rebase B\n\n\n\nWhen you rebase A on B, git creates a temporary branch that is a copy of branch B, and tries to apply the new commits of A on it one by one.\nFor each commit to apply, if there are conflicts, they will be resolved inside of the commit.\nAfter a rebase, the new commits from A (in blue) are not exactly the same as they were:\n\nIf there were conflicts, those conflicts are integrated in each commit\nThey have a new hash\n\nBut they keep their original date which might be confusing since in the final branch, commits in blue were created before the two last commits in purple.\nWhat is the best solution to integrate a new feature into a shared branch and keep your git tree clean?\nLet say that you have a new feature made of three new commits on a branch named `feature`. You want to merge this branch into a shared branch, for exemple `master` that has received two new commits since you started from it.\nYou have two main solutions: \nFirst solution: \n\ngit checkout feature\ngit rebase master\ngit checkout master\ngit merge feature\n\n\nNote : Be careful, git merge feature should do a fast-forward merge, but some hosting services for version control do a 3-way merge anyway. To prevent this, you can use git merge feature \u2013ff-only\nSecond solution:\n\ngit checkout master\ngit merge feature\n\n\nAs you can see, the final tree is more simple with the first solution. You simply have a linear git history. On the opposite, the second solution creates a new \u201cmerge commit\u201d and a loop to show that a merge happened.\nIn this situation, the git tree is still readable, so the advantage of the first solution is not so obvious. The complexity emerges when you have several developers in your team, and several feature branches developed at the same time. If everyone uses the second solution, your git tree ends up complex with several loop, and it can even be difficult to see which commits belong to which branch!\nUnfortunately, the first solution has a few drawbacks:\nHistory rewriting\nWhen you use a rebase, like in the first solution, you \u201crewrite history\u201d because you change the order of past commits on your branch. This can be problematic if several developers work on the same branch: when you rewrite history, you have to use git push \u2013 \u2013 force in order to erase the old branch on the remote repository and put your new branch (with the new history) in its place.\nThis can potentially erase changes another developer made, or introduce conflicts resolution for him.\nTo avoid this problem, you should only rebase branches on which you are the only one working. For example in our case, if you are the only one working on the feature branch.\nHowever you might sometime have to rewrite history of shared branches. In this case, make sure that the other developers working on the branch are aware of it, and are available to help you if you have conflicts to resolve.\nThe obvious advantage of the 3-way merge here, is that you don\u2019t rewrite history at all.\nConflicts resolution\nWhen you merge or rebase, you might have to resolve conflicts.\nWhat I like about the rebase, is that the conflicts added by one commit will be resolved in this same commit. On the opposite, the 3-way merge will resolve all the conflicts into the new \u201cmerge commit\u201d, mixing all together the conflicts added by the different commits of your feature branch.\nThe only problem with the rebase is that you may have to resolve more conflicts, due to the fact that the rebase applies the commits of your branch one by one.\nConclusion\nTo conclude, I hope I have convinced you that rebasing your branch before merging it, can clear your git history a lot! Here is a recap of the advantages and disadvantages of the rebase and merge method versus the 3-way merge method:\n\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJ\u00e9r\u00e9mie Marniquet Fabre\r\n  \t\t\t\r\n  \t\t\t\tI am an agile web developer at Theodo, I enjoy outdoor sports (mountain biking, skiing...) and new technologies !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tIn this tutorial, you will see how to use ARjs on simple examples in order to discover this technology. You don\u2019t need any huge tech background in order to follow this tutorial.\n1 \u2013 Your very first AR example\nFirst we will start with a simple example that will help us understand all the elements we need to start an ARjs prototype.\nLibrary import\nAs I wished to keep it as simple as possible we will only use html static files and javascript libraries. Two external librairies are enough to start with ARjs.\nFirst we need A-Frame library, a web-framework for virtual reality. It is based on components that you can use as tag once defined.\nYou can import A-Frame library using the folowing line :\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n\nThen we need to import ARjs, the web-framework for augmented reality we are going to use.\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"></script>\r\n\nInitialize the scene\nA-Frame works using a scene that contains the elements the user wants to display. To create a new scene, we use the a-scene tag :\n<a-scene stats embedded arjs='trackingMethod: best; debugUIEnabled: false'>\r\n  <!-- All our components goes here -->\r\n</a-scene>\r\n\nNote the 2 elements we have in our a-scene tag :\n\nstats : it displays stats about your application performance. You are free to remove it, but to start it will give us some useful information.\narjs : Here you can find some basic ARjs configuration. trackingMethod is the type of camera tracking you use, here we have choosen which is an auto configuration that will be great for our example. And debugUIEnabled is set at false in order to remove debugging tools from the camera view.\n\nShape\nThen we will use our first a-frame component. A-frame is built around a generic component a-entity that you use and edit to have the behaviour you want.\nIn this demo, we want to display a cube. Thankfully a components exists in A-frame, already implemented, that can be used to do that :\n<a-box position=\"0 0 0\" rotation=\"0 0 0\"></a-box>\r\n\na-box has a lot of attributes on which you can learn more in the official documentation, here we will focus on two attributes :\n\nposition : the three coordinates that will be used to position our components\nrotation : that color of the shape\n\nMarker\nWe are going to use a Hiro marker to start. It is a special kind of marker designed for augmented reality. We will dig deeper into other markers in one of the following part.\n\nDeployment\nAs announced we want to make our application accessible from a mobile device, so we will need to deploy our web page and make it accessible from our smartphone.\nhttp-server\nThere are a lot of node modules that you can use to start a development web server, I choose to use http-server which is very simple and can be used with a single command line.\nFirst you need to install the module by running the following command\nnpm install -g http-server\r\n\nThen to launch your server, you can do it with the command line :\nhttp-server\r\n\nYou can use other tools, such as the python based SimpleHTTPServer which is natively available on macOS with the following command:\npython -m SimpleHTTPServer 8000\r\n\nngrok\nNow that your server is running, you need to make your webpage accessible for your mobile device. We are going to use ngrok. It is a very simple command line tool that you can download from here : https://ngrok.com/download.\nThen you use ngrok with the following command :\nngrok http 8080\r\n\nIt will redirect all connections to the address : http://xxxxxx.ngrok.io, directly toward your server and your html page.\nSo with our index.html containing the following :\n<!doctype HTML>\r\n<html>\r\n<script src=\"https://aframe.io/releases/0.6.1/aframe.min.js\"></script>\r\n<script src=\"https://rawgit.com/donmccurdy/aframe-extras/master/dist/aframe-extras.loaders.min.js\"></script>\r\n<script src=\"https://cdn.rawgit.com/jeromeetienne/AR.js/1.5.0/aframe/build/aframe-ar.js\"> </script>\r\n  <body style='margin : 0px; overflow: hidden;'>\r\n    <a-scene stats embedded arjs='trackingMethod: best;'>\r\n      <a-marker preset=\"hiro\">\r\n      <a-box position='0 1 0' material='color: blue;'>\r\n      </a-box>\r\n      </a-marker>\r\n      <a-entity camera></a-entity>\r\n    </a-scene>\r\n  </body>\r\n</html>\r\n\nOr if you prefer on codepen.\nAnd the result should look like :\n\n2 \u2013 Animation\nNow that we were able to display our first A-frame component, we want to make it move.\nA-frame contains a component a-animation that has been designed to animate an entity. As A-frame uses tag components, the a-animation has to be inside the entity tag that you want to animate :\n  <a-entity>\r\n    <a-animation></a-animation>\r\n  </a-entity>\r\n\na-animation can be used on various attributes of our entity such as position, rotation, scale or even color.\nWe will see in the next part what can be done with these attributes.\nBasics\nFirst we will focus on some basic elements that we will need but you will see that it is enough to do a lot of things.\n\ndur : duration of the animation\nfrom : start position or state of the animation\nto : end position or state of the animation\nrepeat : if and how the animation should be repeated\n\nPosition\nWe will begin with the position, we want our object to move from one position to another. To start with this animation, we only need a 3 dimension vector indicating the initial position and the final position of our object.\nOur code should look like this :\n<a-animation attribute=\"position\"\r\n    dur=\"1000\"\r\n    from=\"1 0 0\"\r\n    to=\"0 0 1\">\r\n</a-animation>\r\n\nRotation\nRotations work the same, except that instead of a vector you have to indicate three angles :\n<a-animation attribute=\"rotation\"\r\n    dur=\"2000\"\r\n    from=\"0 0 0\"\r\n    to=\"360 0 0\"\r\n    repeat=\"indefinite\">\r\n</a-animation>\r\n\nYou can either make your entity rotate on itself:\n\nYou can access the full code here.\nOr make it rotates around an object:\n\nAnd the full code is here.\nOthers properties\nThis animation pattern works on a lot of others properties, for example :\n\nScale :\n\n<a-animation\r\n    attribute=\"scale\"\r\n    to=\"2 2 3\">\r\n</a-animation>\r\n\n\nColor :\n\n<a-animation\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nTrigger\na-animation has a trigger : begin. That can either be used with a delay or an event. For example :\n<a-animation\r\n    begin=\"3000\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nThis animation will start in 3000ms.\nOtherwise with an event you can use :\n<a-entity id=\"entity\">\r\n<a-animation\r\n    begin=\"colorChange\"\r\n    attribute=\"color\"\r\n    to=\"red\">\r\n</a-animation>\r\n\nWith the event emission :\ndocument.querySelector('#entity').emit('colorChange');\r\n\nCombining\nYou can of course, use multiple animations either by having multiple entities with their own animations or by having imbricated entities that share animation.\nFirst case is very simple you only have to add multiple entities with an animation for each.\nSecond one is more difficult because you must add an entity inside an entity, like in this example :\n<a-entity>\r\n    <a-animation attribute=\"rotation\"\r\n      dur=\"2000\"\r\n      easing=\"linear\"\r\n      from=\"0 0 0\"\r\n      to=\"0 360 0\"\r\n      repeat=\"indefinite\"></a-animation>\r\n          <a-entity rotation=\"0 0 25\">\r\n              <a-sphere position=\"2 0 2\"></a-sphere>\r\n          </a-entity>\r\n</a-entity>\r\n\nThe first entity is used as the axis for our rotating animation.\n3 \u2013 Model loading\nType of models\nYou can also load a 3D model inside ARjs and project it on a marker. A-frame works with various models type but glTF is privilegied so you should use it.\nYou can generate your model from software like Blender. I have not so much experience in this area, but if you are interested you should give a look at this list of tutorials: https://www.blender.org/support/tutorials/\nYou can also use an online library to download models, some of them even allow you to directly use their models.\nLoading a model from the internet\nDuring the building of this tutorial, I found KronosGroup github that allow me to made my very first examples with 3d model. You can find all their work here : https://github.com/KhronosGroup/glTF-Sample-Models\nWe will now discover a new component from A-frame : a-asset. It is used to load your assets inside A-frame, and as you can guess our 3d model is one of our asset.\nTo load your 3d model, we will use the following piece of code :\n<a-assets>\r\n      <a-asset-item id=\"smiley\" src=\"https://cdn.rawgit.com/KhronosGroup/glTF-Sample-Models/9176d098/1.0/SmilingFace/glTF/SmilingFace.gltf\"></a-asset-item>\r\n</a-assets>\r\n\nYou can of course load your very own model the same way.\na-asset works as a container that will contain the a-asset-item you will need in your page. Those a-asset-item are used to load 3d model.\nThe full example look like this.\nDisplay your model\nNow that we have loaded our model, we can add it into our scene and start manipulating it. To do that we will need to declare an a-entity inside our a-scene that will be binded to the id of our a-asset-item. The code look like :\n<a-entity gltf-model=\"#smiley\" rotation= \"180 0 0\">\r\n</a-entity>\r\n\nRemember what we did in the previous part with animation. We are still working with a-entity so we can do the same here. The full code for our moving smiley is here.\nAnd here is a demo :\n\n4 \u2013 Markers\nFor now we only have used Hiro marker, now we will see how we can use other kind of marker.\nBarcode\nIn ARjs barcode are also implemented in case you need to have a lot of different marker without waiting to create them from scratch. You first need to declare in the a-scene component what kind of marker you are looking for :\n<a-scene arjs='detectionMode: mono_and_matrix; matrixCodeType: 5x5;'></a-scene>\r\n\nAs you can guess, our value will be found in a 5\u00d75 matrix. And now the a-marker component will look like :\n<a-marker type='barcode' value='5'></a-marker>\r\n\nCustom marker\nYou can also use your very own custom marker. Select the image you want, and use this tool developed by ARjs developer.\nYou can then download the .patt file and you can use it in your application, like this:\n<a-marker-camera type='pattern' url='path/to/pattern-marker.patt'></a-marker-camera>\r\n\nMultiple markers\n<script src=\"https://aframe.io/releases/0.6.0/aframe.min.js\"></script>\r\n<script src=\"https://jeromeetienne.github.io/AR.js/aframe/build/aframe-ar.js\"></script>\r\n<body style='margin : 0px; overflow: hidden;'>\r\n  <a-scene embedded arjs='sourceType: webcam;'>\r\n\r\n    <a-marker type='pattern' url='path/to/pattern-marker.patt'>\r\n      <a-box position='0 0.5 0' material='color: red;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker preset='hiro'>\r\n      <a-box position='0 0.5 0' material='color: green;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-marker type='barcode' value='5'>\r\n      <a-box position='0 0.5 0' material='color: blue;'></a-box>\r\n    </a-marker>\r\n\r\n\r\n    <a-entity camera></a-entity>\r\n  </a-scene>\r\n</body>\r\n\nWhat\u2019s next\nYou now have everything you need to start and deploy a basic AR web application using ARjs.\nIf you are interested in web augmented or virtual reality you can give a deeper look at the A-Frame library so that you will see you can build more custom component or even component you can interact with.\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tBastien Teissier\r\n  \t\t\t\r\n  \t\t\t\tWeb developer at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tStop wasting your time on tasks your CI could do for you.\nFind 4 tips on how to better use your CI in order to focus on what matters \u2013 and what you love: code. Let\u2019s face it: as a developer, a huge part of the value you create is your code.\nNote: Some of these tips use the GitHub / CircleCI combo. Don\u2019t leave yet if you use BitBucket or Jenkins! I use GitHub and CircleCi on my personal and work-related projects, so they are the tools I know best. But most of those tips could be set up with every CI on the market.\nTip 1: Automatic Changelogs\nI used to work on a library of React reusable components, like Material UI. Several teams were using components from our library, and with our regular updates, we were wasting a lot of time writing changelogs. We decided to use Conventional Commits. Conventional Commits is a fancy name for commits with a standardized name:\nexample of Conventional Commits\nThe standard format is \u201cTYPE(SCOPE): DESCRIPTION OF THE CHANGES\u201d. \nTYPE can be\n\nfeat: a new feature on your project\nfix: a bugfix\ndocs: update documentation / Readme\nrefactor: a code change that neither fixes a bug nor adds a feature\nor others\u2026\n\nSCOPE (optional parameter) describes what part of your codebase is changed within the commit.\nDESCRIPTION OF THE CHANGES is pretty much what you would write in a \u201ctraditional\u201d commit message. However, you can use keywords in your commit message to add more information. For instance:\nfix(SomeButton): disable by default to fix IE7 behaviour\r\nBREAKING CHANGE: prop `isDisabled` is now mandatory\nWhy is this useful? Three main reasons:\n\nAllow scripts to parse the commit names, and generate changelogs with them\nHelp developers thinking about the impact of their changes (Does my feature add a Breaking Change?)\nAllow scripts to choose the correct version bump for your project, depending on \u201chow big\u201d the changes in a commit are (bugfix: x.y.Z, feature: x.Y.z, breaking change: X.y.z)\n\nThis standard version bump calculation is called Semantic Versioning. Depending of the version bump, you can anticipate the impact on your app and the amount of work needed.\nBe careful though! Not everyone follows this standard, and even those who do can miss a breaking change! You should never update your dependencies without testing everything is fine \ud83d\ude09\nHow to set up Conventional Commits\n\nInstall Commitizen\nInstall Semantic Releases\nAdd GITHUB_TOKEN and NPM_TOKEN to the environment variables of your CI\nAdd `npx semantic-release` after the bundle & tests steps on your CI master/production build\nUse `git cz` instead of `git commit` to get used to the commit message standard\nSquash & merge your feature branch on master/production branch\n\nWhen you get used to the commit message standard, you can go back to `git commit`, but remember the format! (e.g: `git commit -m \u201cfeat: add an awesome feature\u201d`)\nNow, every developer working on your codebase will create changelogs without even noticing it. Plus, if your project is used by others, they only need a glance at your package version/changelog to know what changes you\u2019ve made, and if they are Breaking.\nTip 2a: Run parallel tasks on your CI\nWhy do I say task instead of tests? Because a CI can do a lot more than run tests! You can:\n\nGenerate automatic changelogs \ud83d\ude09 and version your project\nBuild and push the bundle on a release branch\nDeploy your app\nDeploy your documentation site\n\nThere are several ways to use parallelism to run your tasks.\nThe blunt approach\nThis simply consists of using the built-in parallelism of your tasks, combined with a multi-thread CI container.\nWith Jest, you can choose the number of workers (threads) to use for your test with the `\u2013max-workers` flag.\nWith Pytest, try xdist and the `-n` flag to split your tests on multiple CPUs.\nAnother way of parallelizing tests is by splitting the test files between your CI containers, as React tries to do it. However, I won\u2019t write about this approach in this article since the correct way of doing it is nicely explained in the CircleCi docs.\n\u00a0\nTip 2b: CircleCI Workflows\nWith Workflows, we reduced our CI Build time by 25% on feature branches (from 11\u2033 to 8\u203330) and by 30% on our master branch (from 16\u203330 to 11\u203330). With an average of 7 features merged on master a day, this is 1 hour and 30 minutes less waiting every day for our team.\nWorkflow is a feature of CircleCI. Group your tasks in Jobs, then order your Jobs how it suits your project best. Let\u2019s imagine you are building a library of re-usable React Components (huh, I think I\u2019ve already read that somewhere\u2026). Your CI:\n\nSets up your project (maybe spawn a docker, install your dependencies, build your app)\nRuns unit/integration tests\nRuns E2E tests\nDeploys your Storybook\nPublishes your library\n\nEach of those bullet points can be a Job: it may have several tasks in it, but all serve the same purpose. But do you need to wait for your unit tests to pass before launching your E2E tests? Those two jobs are independent and could be running on two different machines.\nOur CircleCI workflow\nExtract of our config.yml\nAs you can see, it is pretty straight-forward to re-order or add dependencies between steps. For more info on how to setup Workflows, check out the documentation.\nThis is also useful for cross-platform testing (you can take a look at Yarn\u2019s workflows).\nNote: Having trouble setting up a workflow? You can SSH on the machine during the build.\n\u00a0\nParallelization drawbacks\nBut be careful with the parallelism: resources are not unlimited; if you share your CI plan with other teams in your organization, make sure using more resources for parallelism will not be counter-productive at a larger scale. You can easily understand why using 2 machines for 10 minutes can be worse than using 1 machine for 15 minutes:\n\u00a0\nProject #2 is queued on CI because there is no machine free when the build was triggered\n\u00a0\nPlus, sharing the Workspace (the current state) of one machine to others (e.g: after running `yarn`, to make your dependencies installed for every job) costs time (both when saving the state on the first machine and loading it on the other).\nSo, when should I parallelize my CI tasks?\nA good rule of thumb is always keeping jobDuration > (nb_containers * workspaceSharingDuration).\nWorkspace sharing can take up to a minute for a large codebase. You should try several workflow configurations to find what\u2019s best for you.\n\u00a0\nTip 3: Set up cron(tab)s\nCrontabs help make your CI more reliable without making builds longer.\n\nWant to run in-depth performance tests that need to send requests to your app? Schedule it for night time with a cron!\nWant to publish a new version of your app every week? Cron.\nWant to train your ML model but it takes hours? Your CI could trigger the training every night.\n\nSome of you may wonder: what is a cron/crontab? Cron(tab) is an abbreviation of ChronoTable, a job scheduler. A cron is a program that executes a series of instructions at a given time. It can be once an hour, once a day, once a year\u2026\nI worked on a project in finance linking several sources of data and API\u2019s. Regression was the biggest fear of our client. If you give a user outdated or incorrect info, global financial regulators could issue you a huge fine. Therefore, I built a tool to generate requests with randomized parameters (country, user profile\u2026), play them, and check for regressions. The whole process can take an hour. We run it via our CI, daily, at night, and it saved the client a lot of trouble.\nYou can easily set up crons on CircleCi if you\u2019ve already tried Jobs/Workflows. Check out the documentation.\nNote: Crons use the POSIX date-time notation, which can be a bit tricky at first. Check out this neat Crontab Tester tool to get used to it!\n\u00a0\nMisc tips:\n\nLearn Shell! All Continuous Integration / Continuous Delivery systems can run Shell scripts. Don\u2019t be afraid to sparkle some scripts in your build! Add quick checks between/during tasks to make debugging easier, or make your build fail faster: you don\u2019t want to wait for the full 10 minutes when you can check at 2\u201930 that your lockfile is not up-to-date!\nUse cache on your project dependencies!\nAdd extra short task to your CI to connect useful tools like Codecov.io or Danger\n\n\u00a0\nIf you have any other tip you would like to share, don\u2019t hesitate!\n\u00a0\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tAur\u00e9lien Le Masson\r\n  \t\t\t\r\n  \t\t\t\tDeveloper @ Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tThis is a quick guide on how to set up the debugger in VS code server-side for use with Node.js in a Docker container. I recently worked on a project which uses the Koa.js framework as the API. Whilst trying to set up the debugger with VS code, a google search led to several articles that had conflicting information about how to set it up and the port number to expose, or was overly verbose and complicated.\nTo keep things simple, I have split this into 3 steps.\n1) Check version of Node.js on\u00a0server\nTo do this with docker-compose set up, use the following, replace [api] with the name of your\u00a0docker container.\ndocker-compose exec api\u00a0node --version\nInspector\u00a0Protocol\u00a0(Node V7+, since Oct 2016)\nRecent versions of Node.js now uses the inspector protocol. This is easier to set up and is the default setting for new Node.js applications, as most documentation will refer to this protocol. This means that:\n\nThe --inspect flag is required when starting the node process.\nBy default, the port 9229 is exposed, and is equivalent to --inspect:9229\nThe port can be changed, eg. --inspect-brk:1234 . Here, the \u2018-brk\u2019 flag adds a breakpoint on start.\n\nLegacy\u00a0Protocol (Node V6 and earlier)\nOlder versions of Node.js (prior to V7) uses the \u2018Legacy Debugger\u2019. The version of Node.js used on my project was 6.14. This means that:\n\nThe\u00a0--debug\u00a0flag is required when starting the node process.\nBy default, the port 5858 is exposed, and is equivalent to\u00a0--debug:5858\nThis port cannot be changed.\n\nFor more information goto:\nhttps://code.visualstudio.com/docs/nodejs/nodejs-debugging\nhttps://nodejs.org/en/docs/guides/debugging-getting-started/\n2) Expose port in Node and Docker\nIn \u2018package.json\u2019, add\u00a0--debug:5858\u00a0 (or\u00a0--inspect:9229\u00a0depending on Node version) when starting Node, so:\n\"dev\": \"nodemon index.js\",\u00a0becomes\n\"debug\": \"nodemon --debug:5858 index.js\",\nIn \u2018docker-compose.yml\u2019, run the debug node command\u00a0and expose the port. In my case:\napi:\nbuild: ./api\ncommand: yarn dev\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\nbecomes:\napi:\nbuild: ./api\ncommand: yarn debug\nvolumes:\n- ./api:/code\nports:\n- \"4000:4000\"\n- \"5858:5858\"\n3)\u00a0Set launch configuration of Debugger\nIn \u2018/.vscode/launch.json\u2019, my launch configuration is:\n{\n\"type\": \"node\",\n\"request\": \"attach\",\n\"name\": \"Docker: Attach to Node\",\n\"port\": 5858,\n\"address\": \"localhost\",\n\"localRoot\": \"${workspaceFolder}/api/\",\n \"remoteRoot\": \"/code/\",\n\"protocol\": \"legacy\"\n}\nThe port and protocol needs to correspond to the version of Node used as determine above. For newer versions of Node:\u00a0\"port\": \"9229\" and \"protocol\": \"inspector\" should be used.\n\u201clocalRoot\u201d and \u201cremoteRoot\u201d should be set to the folder corresponding to the entry point (eg. index.js) of your Node application in the local repository and the docker folder respectively.\n4) Attach debugger and go!\nIn VS code, set your breakpoints and press F5 or click the green triangle button to start debugging! By default VS code comes with a debug panel to the left and debug console to the bottom, and a moveable debug toolbar. Mousing over a variable shows its values if it has any.\n\n\u00a0\nI hope this article has been useful, and thanks for reading my tech blog!\u00a0 \n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tHo-Wan To\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tWhen I started web development, the developer tools were so new to me I thought I would save time not using them in the first place. I quickly realized how wrong I was as I started using them. No more console.log required, debugging became a piece of cake in a lot of cases!\nThe Network tab is used to track all the interactions between your front end and your back end, thus the network.\nIn this article, I will show usages of the developer tools Network tab on Google Chrome web browser.\nLet\u2019s start exploring the network tab!\nRecord the network logs\nThe first thing to do is to record the network logs by making sure the record button is activated before going on the page you want to record: \n\nCheck the response of a request sent to your server\nYou can keep only the requests sent to your server by clicking on the filter icon and then selecting \u201cXHR\u201d:\n\nIn that section, you can see some information about your requests:\n\nHTTP status code\nType of request\nSize of the request\nEtc.\n\nTo get more details about a request, you can simply click on it.\nLet\u2019s look at the my-shortcuts endpoint that retrieves the shortcuts of an user connected on the application I am using. You can look at the formatted response by clicking on the \u201cPreview\u201d tab: \n\nIf the response of an XHR is an image, you will be able to preview the image instead.\nOn this tab, it becomes easy to determine if the format of the response corresponds to what your front end expected.\nGreat! Now you know how to check the response of a request sent to your server without writing any console.log in your code!\nTest your requests with various Internet connection speeds\nIf the users of the application you are developing have a lower Internet speed than yours, it can be interesting to test your application with custom Internet speed.\nIt is possible to do so by using bandwidth throttling by clicking on the following dropdown menu: \u00a0\n\n\nReplay a request\nReplaying a request can be useful if you want to see how the front end interacts with the response of a request again or if you need to test your request with different parameters. It can be long and painful to reload the page and reproduce exactly the same actions over and over again. Here are some better ways to replay a request:\n\n When right-clicking on a request, you can copy the cURL format of your request and paste it in the terminal of your computer to send the request to your back end:\n\n\n\n When right-clicking on a request, you can copy the request headers and paste them in your favorite API development environment (e.g. Postman):\n\n\nIn Postman, click on \u201cHeaders tab\u201d > \u201cBulk Edit\u201d to edit the headers:\n\nNow all you need to do is paste your headers. Don\u2019t forget to comment the path of the request which is not a header: \u00a0\n\n\n\n If you are using \u201cXHR\u201d requests, you can simply right-click on the request you want to replay and click on \u201cReplay XHR: \n\n\nI hope that I could help you debug your network interactions with this article!\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tJordan Lao\r\n  \t\t\t\r\n  \t\t\t\tDeveloper at Theodo  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"},
{"article": "\r\n\t\t\t\t\t\t\t\t\t\tHow to recode Big Brother in 15 min on your couch\nFace Recognition Explained\nIn this article, we will step by step implement a smart surveillance system, able to recognise people in a video stream and tell you who they are. \nMore seriously, we\u2019ll see how we can recognise in real-time known faces that appear in front of a camera, by having a database of portraits containing those faces.\nFirst, we\u2019ll start by identifying the different essential features that we\u2019ll need to implement. To do that,\u00a0we\u2019ll analyse the way we would to that, as human beings (to all the robots that are reading those words, I hope I don\u2019t hurt your feelings too much and I truly apologize for the methodology of this article).\nAsk yourself : if someone passes just in front of you, how would you recognise him ?\n\nYou\u2019ll need first to see the person\nYou then need to focus on the face itself\nThen there are actually two possibilities.\n\nEither I know this individual and I can recognise him by comparing his face with every face I know.\nOr I don\u2019t know him\n\n\n\nLet\u2019s see now how to the algo will do those different steps.\nFirst step of the process : seeing the person\nThis is quite a simple step. We\u2019ll simply need a computer and a webcam, to capture the video stream. \nWe\u2019ll use openCV Python. With a few lines of code, we\u2019ll be able to capture the video stream, and dispose of the frame one by one.\nimport cv2\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   frame = video_capture.read()\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nHow to detect a face in a picture ?\nTo be able to find a face in the picture, let\u2019s ask ourselves, what is a face and how can we discriminate a face from Gmail\u2019s logo for example ?\n\n\nWe actually do it all the time without even thinking about it. But how can we know that easily that all these pictures are faces ?\n\nWhen we look at those different pictures, photographs and drawings, we see that a face is actually made of certain common elements : \n\nA nose\nTwo eyes\nA mouth \nEars\n\u2026\n\nBut not only are the presence of these elements essential, but their positions is also paramount. \nIndeed, in the two pictures here, you\u2019ll find all the elements that you\u2019ll find in a face. Except one is a face, and one is not.\n\nSo, now that we\u2019ve seen that a face is characterised by certain criterias, we\u2019ll turn them into simple yes-no questions, which will be very useful to find a face in a square image.\nAs a matter of fact, the question \u201cIs there a face in a picture ?\u201d is very complex. However, we\u2019ll be able to approximate it quite well by asking one after the other a series of simple question : \u201cis there a nose ?\u201d ; \u201cIs there an eye ? If yes, is their two eyes ?\u201d ; \u201cAre there ears ?\u201d ; \u201cIs there some form of symmetry ?\u201d. \nAll these questions are both far more simple than the complex question \u201cIs there a face in the picture ?\u201d, while providing us with information to know if part of the image is or is not a face. \nFor each one of these questions, a no answer is very strong and will tell us that there is definitely no face in the picture. \nOn the contrary, a yes answer will not allow us to be sure that there is a human face, but it will slightly increase the probability of the presence of a face.\u00a0If the image is not a face, but it is tree, the answer to the first question \u201cis there a nose ?\u201d will certainly be negative. No need then to ask if there are eyes, or if there is some form of symmetry.\nHowever, if indeed there is a nose, we can go forward and ask \u201care there ears?\u201d. If the answer is still yes, this won\u2019t mean that there is a face, but will slightly increase the likeliness of this possibility, and we will keep digging until being sufficiently confident to say that there is a face indeed.\nThe interest is that the simplicity of the questions will reduce drastically the cost of face detection, and allow to do real-time face detection on a video stream. \nThis is the principle of a detection method called \u201cthe cascade of weak classifier\u201d. Every classifier is very weak considering that it gives only a very little degree of certitude. But if we do the checks one by one, and a region of the picture has them all, we\u2019ll be at the end almost sure that a face is present here. \nThat\u2019s why it is called a cascade classifier, because like a series of waterfalls, the algorithm will simply do a very simple and quick check, one by one, and will only move forward with another check if the first one is positive. \nTo do face detection on the whole picture, and considering that we don\u2019t know in advance the size of the face, we\u2019ll simply apply the cascade algorithm on a moving window for every frame.\n\nWhat we\u2019ve explained here is the principle of the algorithm. Lots of research has been made about how to use cascade for object detection. OpenCV has a built-in way to do face detection with a cascade classifier, by using a set of 6,000 weak classifiers especially developed to do face detection.\nimport cv2\r\n\r\nopencv_path = 'm33/lib/python3.7/site-packages/cv2/data/'\r\nface_cascade = cv2.CascadeClassifier(opencv_path + 'haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n    # Capture frame-by-frame\r\n    ret, frame = video_capture.read()\r\n\r\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n\r\n    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n \r\n    # Draw a rectangle around the faces\r\n    for (x, y, w, h) in faces:\r\n        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n    # Display the resulting frame\r\n    cv2.imshow('Video', frame)\r\n\r\n    if cv2.waitKey(1) & 0xFF == ord('q'):\r\n        break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\nNow that we can detect the face, we need to recognise it among other known faces. Here is what we got :\u00a0\nFace recognition\nNow that we have a system able to detect a face, we need to make sense out of it and recognise the face.\nBy applying the same methodology as before, we\u2019ll find the criterias to recognise a face among others.\nTo do that, let\u2019s look at how we differentiate between two faces : Harry Potter on one side, and Aragorn on the second.\n\nLet\u2019s make a list of the things that can differentiate them : \n\nForm of their nose\nForm of their eyes\nTheir hair\nColor of their eyes\nDistance between the eyes\nSkin color\nBeard\nHeight of the face\nWidth of the face\nRatio of height to width\n\nOf course, this list is not exhaustive. However, are all those criterias good for face recognition ? \nSkin color is a bad one for example. We\u2019ve all seen Harry Potter or Aragorn after hard battles covered with dirt or mud, and we\u2019re still able to recognise them easily.\nSame goes for height and width of the face. Indeed, these measures change a lot with the distance of the person to the camera. Despite that we can easily recognise the faces even when their size changes. \nSo we can keep some good criterias that will really help recognise a face : \n\nForm of their nose\nForm of their eyes\nDistance between the eyes\nRatio of height to width \nPosition of the nose relative to the whole face\nForm of eyebrows\n\u2026\n\nLet\u2019s now measure all these elements. By doing this, we\u2019ll have a set of values that describe the face of an individual. These measures are a discrete description of what the face looks like.\n\nActually, what we have done, is that we reduced the face to a limited number of \u201cfeatures\u201d that will give us valuable and comparable information of the given face.\n\nMathematically speaking, we have simply created a vector space projection, that allowed us to reduce the number of dimensions in our problem. From a million-dimensions vector space problems (if the picture is 1MPixel RGB image, the vector space is of 1M * 3 dimensions) to a an approximately a-hundred-dimension vector space. The problem becomes far more simple ! \nNo need to consider all the pixels in the picture at all, we only need to extract from the image a limited set of features. These extracted features can be considered as vectors that we can then compare the way we do it with any vector by computing euclidean distances for example.\n\nAnd just like that, comparing faces becomes mathematically as simple as computing the distance between two points on a grid, with only a few more dimensions ! To be simple, it\u2019s as though, every portrait can then be described as a point in space. The closer points are, the more likely they describe the same face ! And that\u2019s all !\nWhen we find a face in a frame, we find its position in the feature-space and we look for the nearer known point. If the distance between the two is close, we\u2019ll consider that they\u2019re both linked to the same face. Otherwise, if the point representing the new face is too far from all the faces known, it means we don\u2019t know this face.\n\nTo implement that, we\u2019ll use the face_recognition Python library that allows us to use a deep learning algorithm that extracts from a face a 128-dimension vector of features. \nWe\u2019ll do it in two steps.\nWe first turn our portrait database into a set of computed feature-vectors (reference points like the Frodo point in the example above). \nimport face_recognition\r\nimport os\r\nimport pandas as pd\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n\t// happens when no face is detected\r\n       Continue\r\n\r\nfeatures_df = pd.DataFrame(face_features, names)\r\nfeatures_df.to_csv('database.csv')\r\n\r\n\nThen, we load the database and launch the real-time face recognition:\nimport cv2\r\nimport pandas as pd\r\nfrom helpers import load_database\r\nimport PIL\r\nimport numpy as np\r\nimport face_recognition\r\n\r\ndef load_image_file(file):\r\n    im = PIL.Image.open(file)\r\n    im = im.convert('RGB')\r\n    return np.array(im)\r\n\r\nface_features = []\r\nnames = []\r\n\r\nfor counter, name in enumerate(os.listdir('photos/')):\r\n   if '.jpg' not in name:\r\n       continue\r\n   image = load_image_file(pictures_dir + name)\r\n   try:\r\n       face_features.append(face_recognition.face_encodings(image)[0])\r\n       names.append(name.replace('.jpg', ''))\r\n   except IndexError:\r\n       # happens when no face is detected\r\n       Continue\r\n\r\nface_cascade= cv2.CascadeClassifier('m33/lib/python3.7/site-packages/cv2/data/haarcascade_frontalface_default.xml')\r\n\r\nvideo_capture = cv2.VideoCapture(0)\r\n\r\nwhile True:\r\n   # Capture frame-by-frame\r\n   ret, frame = video_capture.read()\r\n   gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\r\n   faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\r\n  \r\n   # Draw a rectangle around the faces\r\n   for (x, y, w, h) in faces:\r\n       cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\r\n\r\n       pil_im = PIL.Image.fromarray(frame[y:y+h, x:x+w])\r\n       face = np.array(pil_im.convert('RGB'))\r\n       try:\r\n           face_descriptor = face_recognition.face_encodings(face)[0]\r\n       except Exception:\r\n           continue\r\n       distances = np.linalg.norm(face_descriptors - face_descriptor, axis=1)\r\n       if(np.min(distances) < 0.7): found_name = names[np.argmin(distances)] print(found_name) print(found_name) #y = top - 15 if top - 15 > 15 else top + 15\r\n       cv2.putText(frame, found_name, (y, y-15), cv2.FONT_HERSHEY_SIMPLEX,\r\n                   0.75, (0, 255, 0), 2)\r\n\r\n   # Display the resulting frame\r\n   cv2.imshow('Video', frame)\r\n\r\n   if cv2.waitKey(1) & 0xFF == ord('q'):\r\n       break\r\n\r\n# When everything is done, release the capture\r\nvideo_capture.release()\r\ncv2.destroyAllWindows()\r\n\r\n\nAnd here it comes ! \n\nHere is a github repo with the code working : https://github.com/oussj/big_brother_for_dummies\nExternal links I used : \n\nhttps://github.com/ageitgey/face_recognition\nhttps://medium.com/@ageitgey/machine-learning-is-fun-part-4-modern-face-recognition-with-deep-learning-c3cffc121d78\nhttps://realpython.com/face-recognition-with-python/\n\n\r\n\t\t\t\t\t\t\t\t\t\tYou liked this article? You'd probably be a good match for our ever-growing tech team at Theodo. Join Us\r\n\r\n\t\t\t\t\t\t\t\t\t\t\r\n\tWRITTEN BY\r\n\r\n\t\r\n\t\t    \r\n  \t\t\r\n  \t\t\t\r\n  \t\t\t\t  \t\t\t\r\n  \t\t\r\n\r\n  \t\t\r\n\t\t\t\tOussamah Jaber\r\n  \t\t\t\r\n  \t\t\t\tAfter graduating from MINES ParisTech, I joined Theodo as an agile web developer to use cutting-edge technology and build awesome products !  \t\t\t\r\n  \t\t\r\n    \r\n\t\t\t\r\n\r\n\t\t\t\t\t\t\t\t\t"}
]